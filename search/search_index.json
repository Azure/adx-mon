{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Overview","text":"<p>ADX-Mon is a comprehensive observability platform that seamlessly integrates metrics, logs, traces, continuous profile and any telemetry into a unified platform. It addresses the traditional challenges of data being siloed and difficult to correlate, as well as the cardinality and scale issues found in existing metrics solutions, streamlining the collection and analysis of observability data.</p>"},{"location":"#features","title":"Features","text":"<ul> <li>Unlimited Cardinality, Retention, and Granularity: No restrictions on data dimensions, storage duration, or detail level.</li> <li>Low Latency Ingestion and Lightweight Collection: Ensures rapid data processing with minimal resource overhead.</li> <li>Unified Alerting: Simplified alerting mechanisms for metrics, logs, and traces.</li> <li>Powered by Azure Data Explorer (ADX): Built on the robust and scalable ADX platform with the powerful Kusto Query Language (KQL) unifying access to all telemetry.</li> <li>Flexible and Standards-Based Interfaces: Supports OTEL, Prometheus, and other industry standards ingestion protocols.</li> <li>Optimized for Kubernetes and Cloud-Native Environments: Designed to thrive in modern, dynamic infrastructures.</li> <li>Pluggable Alerting Provider API: Customizable alerting integrations.</li> <li>Broad Compatibility: Works seamlessly with Grafana, ADX Dashboards, PowerBI, and any ADX-compatible products.</li> <li>Turnkey, Scalable, and Reliable: A ready-to-use observability platform that scales from hobby projects to enterprise-level deployments.</li> </ul>"},{"location":"#learn-more","title":"Learn More","text":"<ul> <li>Installation</li> <li>Quick Start</li> <li>Concepts</li> <li>Guides</li> <li>CRD Reference</li> <li>Cook Book</li> </ul>"},{"location":"concepts/","title":"Concepts","text":""},{"location":"concepts/#overview","title":"Overview","text":"<p>ADX-Mon is a fully managed observability solution that supports metrics, logs and traces in a unified stack. The entrypoint to ADX-Mon is the <code>collector</code> which is deployed as a daemonset in your Kubernetes cluster.</p> <p>The collector is responsible for collecting metrics, logs and traces from your Kubernetes cluster and sending them to the <code>ingestor</code> endpoint which handles the ingestion of data into Azure Data Explorer (ADX).</p> <p>All collected data is translated to ADX tables.  Each table has a consistent schema that can be extended through <code>update policies</code> to pull commonly used labels and attributes up to top level columns.</p> <p>These tables are all queried with KQL.  KQL queries are used for analysis, alerting and visualization.</p>"},{"location":"concepts/#components","title":"Components","text":""},{"location":"concepts/#collector","title":"Collector","text":"<p>The Collector is the entrypoint for telemetry collection in ADX-Mon. It is typically deployed as a DaemonSet on every node in your Kubernetes cluster, where it collects metrics, logs, and traces from the local node and forwards them to the Ingestor for aggregation and storage in Azure Data Explorer (ADX).</p>"},{"location":"concepts/#key-features","title":"Key Features","text":"<ul> <li>Multi-Source Telemetry Collection: Collects from Prometheus endpoints, logs (including OTLP and host logs), and traces.</li> <li>Kubernetes Native: Discovers pods and services to scrape via annotations (e.g., <code>adx-mon/scrape: \"true\"</code>).</li> <li>Configurable via TOML: Uses a TOML config file (see <code>docs/config.md</code>) for flexible setup of scrape targets, filters, exporters, and storage.</li> <li>Efficient Buffering: Uses a Write-Ahead Log (WAL) to buffer data locally for reliability and performance.</li> <li>Label/Attribute Lifting: Supports lifting selected labels/attributes to top-level columns for easier querying in ADX.</li> <li>Filtering: Supports regex-based keep/drop rules for metrics and labels, and attribute-based log filtering.</li> <li>Exporters: Can forward telemetry to additional endpoints (e.g., OTLP, Prometheus remote write) in parallel with ADX.</li> <li>Health &amp; Metrics: Exposes Prometheus metrics for collector health, WAL status, and scrape/export stats.</li> </ul>"},{"location":"concepts/#configuration-usage","title":"Configuration &amp; Usage","text":"<ul> <li>Deployment: Usually managed by the adx-mon Operator via the <code>Collector</code> CRD, or deployed as a DaemonSet.</li> <li>Config File: Main configuration is via a TOML file (see <code>docs/config.md</code> for all options and examples). Key fields include:</li> <li><code>endpoint</code>: Ingestor URL to send telemetry to.</li> <li><code>storage-dir</code>: Directory for WAL and log cursors.</li> <li><code>prometheus-scrape</code>: Prometheus scrape settings (interval, targets, filters).</li> <li><code>prometheus-remote-write</code>: Accepts Prometheus remote write protocol.</li> <li><code>otel-log</code>/<code>otel-metric</code>: Accepts OTLP logs/metrics.</li> <li><code>host-log</code>: Collects host and kernel logs.</li> <li><code>exporters</code>: Additional telemetry destinations.</li> <li>Kubernetes Annotations:</li> <li><code>adx-mon/scrape</code>: Enables scraping for a pod/service.</li> <li><code>adx-mon/port</code>, <code>adx-mon/path</code>: Configure scrape port/path. Port can be numeric (e.g., \"8080\") or named (e.g., \"metrics\").</li> <li><code>adx-mon/targets</code>: Comma-separated list of path:port combinations. Supports named ports.</li> <li><code>adx-mon/log-destination</code>: Sets log destination table.</li> <li><code>adx-mon/log-parsers</code>: Comma-separated list of log parsers (e.g., <code>json</code>).</li> </ul>"},{"location":"concepts/#example-collector-crd","title":"Example Collector CRD","text":"<pre><code>apiVersion: adx-mon.azure.com/v1\nkind: Collector\nmetadata:\n  name: prod-collector\nspec:\n  image: \"ghcr.io/azure/adx-mon/collector:v1.0.0\"\n  ingestorEndpoint: \"http://prod-ingestor.monitoring.svc.cluster.local:8080\"\n</code></pre>"},{"location":"concepts/#example-config-snippet","title":"Example Config Snippet","text":"<pre><code>endpoint = 'https://ingestor.adx-mon.svc.cluster.local'\nstorage-dir = '/var/lib/adx-mon'\nregion = 'eastus'\n\n[prometheus-scrape]\n  database = 'Metrics'\n  scrape-interval = 10\n  scrape-timeout = 5\n  drop-metrics = ['^kube_pod_ips$', 'etcd_grpc.*']\n  keep-metrics = ['nginx.*']\n\n[[prometheus-scrape.static-scrape-target]]\n  host-regex = '.*'\n  url = 'http://localhost:9090/metrics'\n  namespace = 'monitoring'\n  pod = 'host-monitor'\n  container = 'host-monitor'\n</code></pre>"},{"location":"concepts/#how-it-works","title":"How It Works","text":"<ol> <li>Discovery: Finds pods/services to scrape based on annotations and static config.</li> <li>Scraping/Collection: Collects metrics, logs, and traces from configured sources.</li> <li>Buffering: Writes data to local WAL for reliability and batching.</li> <li>Forwarding: Sends batches to the Ingestor, which aggregates and uploads to ADX.</li> <li>Exporting: Optionally forwards telemetry to additional endpoints (e.g., OTLP, remote write).</li> <li>Health Monitoring: Exposes <code>/metrics</code> for Prometheus scraping and tracks WAL/export status.</li> </ol>"},{"location":"concepts/#development-testing","title":"Development &amp; Testing","text":"<ul> <li>Configurable for local or in-cluster testing.</li> <li>See <code>docs/config.md</code> for all config options and advanced usage.</li> </ul>"},{"location":"concepts/#ingestor","title":"Ingestor","text":"<p>The Ingestor is the aggregation and buffering point for all telemetry collected by ADX-Mon. It receives metrics, logs, and traces from Collectors, batches and stores them using a Write-Ahead Log (WAL), and uploads them to Azure Data Explorer (ADX) in optimized batches. The ingestor is designed for high-throughput, reliability, and efficient use of ADX resources.</p>"},{"location":"concepts/#key-features_1","title":"Key Features","text":"<ul> <li>WAL-Based Buffering: All incoming telemetry is written to disk in an append-only WAL for durability and efficient batching.</li> <li>Batching &amp; Coalescing: Segments are batched by table/schema and uploaded to ADX in large, compressed batches (100MB\u20131GB recommended) to optimize ingestion cost and performance.</li> <li>Peer Transfer: Small segments are transferred to peer ingestors for coalescing, reducing the number of small files ingested by ADX.</li> <li>Direct Upload Fallback: If a segment is too old or too large, it is uploaded directly to ADX, bypassing peer transfer.</li> <li>Multi-Database Support: Can upload to multiple ADX databases (e.g., metrics, logs) simultaneously.</li> <li>Kubernetes Native: Deployed as a StatefulSet, supports scaling and partitioning.</li> <li>CRD-Driven: Managed via the <code>Ingestor</code> CRD for declarative configuration.</li> <li>Health &amp; Metrics: Exposes Prometheus metrics for WAL, batching, upload, and transfer status.</li> </ul>"},{"location":"concepts/#configuration-usage_1","title":"Configuration &amp; Usage","text":"<ul> <li>Deployment: Usually managed by the adx-mon Operator via the <code>Ingestor</code> CRD, or deployed as a StatefulSet.</li> <li>Config File/CLI: Main configuration is via CLI flags or environment variables (see <code>cmd/ingestor/main.go</code>). Key options include:</li> <li><code>--storage-dir</code>: Directory for WAL segments.</li> <li><code>--metrics-kusto-endpoints</code>, <code>--logs-kusto-endpoints</code>: ADX endpoints for metrics/logs (format: <code>&lt;db&gt;=&lt;endpoint&gt;</code>).</li> <li><code>--cluster-labels</code>: Labels used to identify and distinguish ingestor clusters (format: <code>&lt;key&gt;=&lt;value&gt;</code>). Used for <code>&lt;key&gt;</code> substitutions in SummaryRules.</li> <li><code>--uploads</code>: Number of concurrent uploads.</li> <li><code>--max-segment-size</code>, <code>--max-segment-age</code>: Segment batching thresholds.</li> <li><code>--max-transfer-size</code>, <code>--max-transfer-age</code>: Peer transfer thresholds.</li> <li><code>--disable-peer-transfer</code>: Disable peer transfer (upload all segments directly).</li> <li><code>--partition-size</code>: Number of nodes in a partition for sharding.</li> <li><code>--max-disk-usage</code>, <code>--max-segment-count</code>: Backpressure controls.</li> <li><code>--enable-wal-fsync</code>: Enable fsync for WAL durability.</li> <li>See <code>docs/config.md</code> for all options.</li> </ul>"},{"location":"concepts/#example-ingestor-crd","title":"Example Ingestor CRD","text":"<pre><code>apiVersion: adx-mon.azure.com/v1\nkind: Ingestor\nmetadata:\n  name: prod-ingestor\nspec:\n  image: \"ghcr.io/azure/adx-mon/ingestor:v1.0.0\"\n  replicas: 3\n  endpoint: \"http://prod-ingestor.monitoring.svc.cluster.local:8080\"\n  exposeExternally: false\n  adxClusterSelector:\n    matchLabels:\n      app: adx-mon\n</code></pre>"},{"location":"concepts/#how-it-works_1","title":"How It Works","text":"<ol> <li>Receive Telemetry: Collectors send metrics/logs/traces to the ingestor via HTTP (Prometheus remote write, OTLP, etc.).</li> <li>WAL Write: Data is written to a WAL segment file per table/schema.</li> <li>Batching: When a segment reaches the max size or age, it is either:</li> <li>Transferred to a peer (if small enough and within age threshold), or</li> <li>Uploaded directly to ADX (if too large/old or transfer fails).</li> <li>Peer Transfer: Segments are transferred to the peer responsible for the table (partitioned by hash). Peers coalesce segments for larger uploads.</li> <li>ADX Upload: Batches are compressed and uploaded to ADX using the Kusto ingestion API. Table and mapping are auto-managed.</li> <li>Cleanup: Uploaded/expired segments are removed from disk. Backpressure is applied if disk usage/segment count exceeds limits.</li> </ol>"},{"location":"concepts/#example-data-flow-diagram","title":"Example Data Flow Diagram","text":"<pre><code>flowchart TD\n    Collector1 --&gt;|metrics/logs| IngestorA\n    Collector2 --&gt;|metrics/logs| IngestorB\n    IngestorA -- Peer Transfer --&gt; IngestorB\n    IngestorA -- Upload --&gt; ADX[(Azure Data Explorer)]\n    IngestorB -- Upload --&gt; ADX</code></pre>"},{"location":"concepts/#health-metrics","title":"Health &amp; Metrics","text":"<ul> <li>Exposes <code>/metrics</code> endpoint for Prometheus scraping.</li> <li>Tracks WAL segment counts, disk usage, upload/transfer queue sizes, upload/transfer rates, and error counts.</li> <li>Provides metrics for backpressure, slow requests, and dropped segments.</li> </ul>"},{"location":"concepts/#development-testing_1","title":"Development &amp; Testing","text":"<ul> <li>Can be run locally with test ADX clusters or in \"direct upload\" mode.</li> <li>Includes integration tests for WAL, batching, and upload logic.</li> <li>See <code>docs/ingestor.md</code> and <code>README.md</code> for advanced usage and troubleshooting.</li> </ul>"},{"location":"concepts/#summaryrules","title":"SummaryRules","text":"<p>SummaryRules provide automated data aggregation and ETL (Extract, Transform, Load) capabilities within ADX-Mon. They execute scheduled KQL queries against ADX to create rollups, downsampled data, or import data from external sources, helping manage data retention costs and improve query performance.</p>"},{"location":"concepts/#key-features_2","title":"Key Features","text":"<ul> <li>Automated Scheduling: Execute KQL queries at defined intervals (e.g., hourly, daily) with precise time window management.</li> <li>Async Operation Tracking: Submit queries as ADX async operations and monitor them through completion, handling retries and failures automatically.</li> <li>Time Window Management: Calculate exact execution windows to ensure no data gaps or overlaps between runs.</li> <li>Cluster Label Substitution: Support environment-agnostic rules using cluster labels for multi-environment deployments.</li> <li>State Persistence: Track execution history and operation status using Kubernetes conditions.</li> <li>Resilient Operation: Handle ADX cluster restarts, network issues, and operation cleanup automatically.</li> </ul>"},{"location":"concepts/#configuration-usage_2","title":"Configuration &amp; Usage","text":"<ul> <li>Deployment: SummaryRules are managed by the Ingestor's <code>SummaryRuleTask</code> which runs periodically to process all rules.</li> <li>CRD Definition: Rules are defined as <code>SummaryRule</code> CRDs with fields:</li> <li><code>database</code>: Target ADX database</li> <li><code>table</code>: Destination table for aggregated results</li> <li><code>body</code>: KQL query with <code>_startTime</code> and <code>_endTime</code> placeholders</li> <li><code>interval</code>: Execution frequency (e.g., <code>1h</code>, <code>15m</code>, <code>1d</code>)</li> <li>Placeholders:</li> <li><code>_startTime</code>/<code>_endTime</code>: Automatically replaced with execution window times</li> <li><code>_&lt;label&gt;</code>: Replaced with cluster label values from ingestor configuration</li> </ul>"},{"location":"concepts/#example-summaryrule-crd","title":"Example SummaryRule CRD","text":"<pre><code>apiVersion: adx-mon.azure.com/v1\nkind: SummaryRule\nmetadata:\n  name: hourly-cpu-usage\nspec:\n  database: Metrics\n  table: CPUUsageHourly\n  interval: 1h\n  body: |\n    Metrics\n    | where Timestamp between (_startTime .. _endTime)\n    | where Name == \"cpu_usage_percent\"\n    | summarize avg(Value), max(Value), min(Value)\n      by bin(Timestamp, 1h), Pod, Namespace\n</code></pre> <p>For detailed examples and best practices, see the SummaryRules Cookbook.</p>"},{"location":"concepts/#wal-segment-file-format","title":"WAL Segment File Format","text":"<p>Note: The WAL binary format is fully documented below and matches the implementation in <code>pkg/wal/segment.go</code>. This includes segment and block headers, field layout, encoding, versioning, and repair logic. For advanced integrations or troubleshooting, see also Ingestor Overview.</p> <p>WAL (Write-Ahead Log) segment files are the durable, append-only storage format used for buffering telemetry data before upload to Azure Data Explorer (ADX). Understanding the binary format is essential for troubleshooting, recovery, and advanced integrations.</p>"},{"location":"concepts/#segment-file-structure","title":"Segment File Structure","text":"<p>Each WAL segment file consists of:</p> <ol> <li>Segment Header (8 bytes):</li> <li>Bytes 0-5: ASCII <code>\"ADXWAL\"</code> (magic number)</li> <li> <p>Bytes 6-7: Reserved for future use (e.g., versioning)</p> </li> <li> <p>Block Sequence:</p> </li> <li>The remainder of the file is a sequence of blocks, each with its own header and compressed payload.</li> </ol>"},{"location":"concepts/#block-layout","title":"Block Layout","text":"<p>Each block is encoded as:</p> Field Size Description Length 4 bytes Big-endian uint32: length of the compressed block CRC32 4 bytes Big-endian uint32: CRC32 checksum of the compressed block Block Data variable S2 (Snappy-compatible) compressed block (see below)"},{"location":"concepts/#block-data-after-decompression","title":"Block Data (after decompression)","text":"Field Size Description Magic 2 bytes <code>0xAA 0xAA</code> (block header magic number) Version 1 byte Block version (currently <code>1</code>) Type 1 byte Sample type (e.g., metric, log, trace) Count 4 bytes Big-endian uint32: number of samples in the block Value N bytes Actual uncompressed data payload <ul> <li>The block data is compressed using S2 (Snappy-compatible) before being written to disk.</li> <li>The block header (magic, version, type, count) is always present at the start of the decompressed block.</li> </ul>"},{"location":"concepts/#block-write-example","title":"Block Write Example","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502    Len    \u2502   CRC   \u2502   Magic   \u2502  Version  \u2502   Type    \u2502   Count   \u2502   Value   \u2502\n\u2502  4 bytes  \u2502 4 bytes \u2502  2 bytes  \u2502  1 byte   \u2502  1 byte   \u2502  4 bytes  \u2502  N bytes  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"concepts/#versioning-compatibility","title":"Versioning &amp; Compatibility","text":"<ul> <li>The segment version is stored in the block header (currently <code>1</code>).</li> <li>The first 8 bytes of the file are reserved for magic/versioning.</li> <li>The format is designed for forward compatibility; unknown blocks can be skipped or truncated.</li> </ul>"},{"location":"concepts/#recovery-repair","title":"Recovery &amp; Repair","text":"<ul> <li>Each block is independently verifiable using its CRC32 checksum.</li> <li>The <code>Repair()</code> method can truncate the file at the last good block if corruption is detected (e.g., after a crash).</li> </ul>"},{"location":"concepts/#summary-table","title":"Summary Table","text":"Offset Field Size Description 0 SegmentMagic 8 bytes <code>\"ADXWAL\"</code> + reserved 8+ Block(s) variable See block structure above"},{"location":"concepts/#example-minimal-wal-segment-hex","title":"Example: Minimal WAL Segment (Hex)","text":"<pre><code>41 44 58 57 41 4C 00 00   # \"ADXWAL\" + reserved\n...                       # Block(s) as described above\n</code></pre> <p>For more details, see the implementation in <code>pkg/wal/segment.go</code> and <code>pkg/wal/wal.go</code>.</p>"},{"location":"concepts/#alerter","title":"Alerter","text":"<p>The Alerter component is responsible for evaluating alert rules (defined as Kubernetes <code>AlertRule</code> CRDs) and sending alert notifications to external systems when conditions are met. It queries Azure Data Explorer (ADX) on a schedule, evaluates the results, and generates notifications for each alerting row.</p>"},{"location":"concepts/#key-features_3","title":"Key Features","text":"<ul> <li>CRD-Driven: Alerting rules are defined as <code>AlertRule</code> CRDs, specifying the KQL query, schedule, and notification destination.</li> <li>Kusto Query Execution: Periodically executes KQL queries against ADX clusters, as configured in the rule.</li> <li>Notification Delivery: Sends alert notifications to a configurable HTTP endpoint (e.g., ICM, PagerDuty, custom webhooks) in a standard JSON format.</li> <li>Correlation &amp; Auto-Mitigation: Supports correlation IDs to deduplicate alerts and auto-mitigate after a configurable duration.</li> <li>Tag-Based Routing: Supports tag-based filtering to control which alerter instance processes which rules (e.g., by region, cloud, or custom tags).</li> <li>Conditional Execution (criteria / criteriaExpression): AlertRule, SummaryRule and MetricsExporter share unified conditional logic. A legacy <code>criteria</code> map (OR semantics across entries) and an optional CEL <code>criteriaExpression</code> (evaluated against lower\u2011cased cluster label/tag variables such as <code>region</code>, <code>cloud</code>, <code>environment</code>, etc.) combine with AND semantics. If either is empty it is permissive. Evaluation errors skip execution.</li> <li>Health &amp; Metrics: Exposes Prometheus metrics for alert delivery health, query health, and notification status.</li> </ul>"},{"location":"concepts/#configuration-usage_3","title":"Configuration &amp; Usage","text":"<ul> <li>Deployment: The alerter is typically deployed as a Kubernetes Deployment or managed by the adx-mon Operator via the <code>Alerter</code> CRD.</li> <li>Configurable via CLI: Supports configuration via command-line flags (see <code>cmd/alerter/main.go</code>), including Kusto endpoints, region, cloud, notification endpoint, concurrency, and tags.</li> <li>Notification Endpoint: Set via the <code>notificationEndpoint</code> field in the <code>Alerter</code> CRD or <code>--alerter-address</code> CLI flag. This is the HTTP endpoint to which alert notifications are POSTed.</li> <li>Kusto Endpoints: Provided as a map of database name to endpoint (e.g., <code>--kusto-endpoint \"DB=https://cluster.kusto.windows.net\"</code>).</li> <li>Tags: Key-value pairs (e.g., <code>--tag region=uksouth</code>) used to filter which rules this alerter instance will process.</li> </ul>"},{"location":"concepts/#example-alerter-crd","title":"Example Alerter CRD","text":"<pre><code>apiVersion: adx-mon.azure.com/v1\nkind: Alerter\nmetadata:\n  name: prod-alerter\nspec:\n  image: \"ghcr.io/azure/adx-mon/alerter:v1.0.0\"\n  notificationEndpoint: \"http://alerter-endpoint\"\n  adxClusterSelector:\n    matchLabels:\n      app: adx-mon\n</code></pre>"},{"location":"concepts/#alert-notification-format","title":"Alert Notification Format","text":"<p>Alert notifications are sent as JSON via HTTP POST to the configured endpoint. Example payload: <pre><code>{\n  \"Destination\": \"MDM://Platform\",\n  \"Title\": \"High CPU Usage\",\n  \"Summary\": \"CPU usage exceeded 90% on node xyz\",\n  \"Description\": \"The CPU usage on node xyz has been above 90% for 5 minutes.\",\n  \"Severity\": 2,\n  \"Source\": \"namespace/alert-name\",\n  \"CorrelationID\": \"namespace/alert-name://unique-correlation-id\",\n  \"CustomFields\": { \"region\": \"uksouth\" }\n}\n</code></pre></p>"},{"location":"concepts/#how-it-works_2","title":"How It Works","text":"<ol> <li>Rule Discovery: Loads <code>AlertRule</code> CRDs from the Kubernetes API or local files.</li> <li>Query Execution: On each interval, executes the KQL query defined in the rule against the specified ADX database.</li> <li>Result Processing: For each row returned, constructs an alert notification, including severity, title, summary, correlation ID, and custom fields.</li> <li>Notification Delivery: Sends the alert as a JSON payload to the configured notification endpoint.</li> <li>Health Monitoring: Exposes Prometheus metrics for query and notification health.</li> </ol>"},{"location":"concepts/#tag-based-routing","title":"Tag-Based Routing","text":"<p>Alerter instances can be configured with tags (e.g., <code>region</code>, <code>cloud</code>). Only rules whose <code>criteria</code> match the instance's tags will be processed by that instance. This enables multi-region or multi-cloud deployments. <code>criteriaExpression</code> is parsed and executed using CEL, with the available variables being defined by tags passed into the executing service.</p>"},{"location":"concepts/#unified-execution-selection-examples","title":"Unified Execution Selection Examples","text":"<p><pre><code># Map only\nspec:\n  criteria:\n    region: [eastus, westus]\n\n# Expression only\nspec:\n  criteriaExpression: region in ['eastus','westus'] &amp;&amp; env == 'prod'\n\n# Both (AND)\nspec:\n  criteria:\n    region: [eastus]\n  criteriaExpression: env == 'prod' &amp;&amp; cloud == 'public'\n</code></pre> Behavior formula: <code>(criteria empty OR any match) AND (criteriaExpression empty OR expression true)</code>.</p>"},{"location":"concepts/#example-cli-usage","title":"Example CLI Usage","text":"<pre><code>cd cmd/alerter\n# Run with required Kusto endpoint and kubeconfig\n./alerter --kusto-endpoint \"DB=https://cluster.kusto.windows.net\" --kubeconfig ~/.kube/config --region uksouth --cloud AzureCloud --tag environment=test --alerter-address http://alerter-endpoint\n</code></pre>"},{"location":"concepts/#health-metrics_1","title":"Health &amp; Metrics","text":"<ul> <li>Exposes <code>/metrics</code> endpoint for Prometheus scraping.</li> <li>Tracks health of query execution and notification delivery.</li> <li>Provides metrics for alert delivery failures, unhealthy rules, and notification status.</li> </ul>"},{"location":"concepts/#development-testing_2","title":"Development &amp; Testing","text":"<ul> <li>Includes a <code>lint</code> mode to validate alert rules without sending notifications.</li> <li>Supports local testing with fake Kusto and alert endpoints.</li> <li>See <code>README.md</code> for testing instructions.</li> </ul>"},{"location":"concepts/#operator","title":"Operator","text":"<p>The Operator is the control plane component that manages the lifecycle of all adx-mon resources\u2014including Collectors, Ingestors, Alerters, and Azure Data Explorer (ADX) infrastructure\u2014using Kubernetes Custom Resource Definitions (CRDs). It provides a declarative, automated, and production-ready way to deploy, scale, and manage the entire ADX-Mon stack.</p>"},{"location":"concepts/#key-responsibilities","title":"Key Responsibilities","text":"<ul> <li>CRD Management: Watches and reconciles <code>ADXCluster</code>, <code>Ingestor</code>, <code>Collector</code>, and <code>Alerter</code> CRDs, ensuring the actual state matches the desired state.</li> <li>Azure Infrastructure Automation: Provisions and manages ADX clusters and databases using the Azure SDK for Go, including resource groups, managed identities, and database policies.</li> <li>Component Lifecycle: Generates and applies Kubernetes manifests for all adx-mon components, supporting custom images, replica counts, and configuration overrides.</li> <li>Reconciliation &amp; Drift Detection: Continuously monitors managed resources and reverts manual changes to match the CRD spec. Updates CRD status fields to reflect progress and issues.</li> <li>Incremental, Granular Workflow: Proceeds in dependency order (ADX \u2192 Ingestor \u2192 Collector \u2192 Alerter), updating subresource conditions at each phase.</li> <li>Resource Cleanup: Deletes managed Kubernetes resources when CRDs are deleted. (Azure resources are not deleted by default.)</li> <li>Multi-Cluster &amp; Federation: Supports federated and partitioned ADX topologies for geo-distributed or multi-tenant scenarios.</li> </ul>"},{"location":"concepts/#how-it-works_3","title":"How It Works","text":"<ol> <li>CRD Reconciliation: Watches for changes to adx-mon CRDs and managed resources.</li> <li>Azure Resource Management: Provisions or connects to ADX clusters/databases as specified in the <code>ADXCluster</code> CRD.</li> <li>Component Deployment: Deploys or updates StatefulSets/Deployments for Ingestor, Collector, and Alerter based on their CRDs.</li> <li>Status &amp; Health: Updates CRD status fields and subresource conditions to reflect readiness and errors.</li> <li>Federation Support: Manages federated clusters, heartbeats, and macro-expand KQL functions for cross-cluster querying.</li> </ol>"},{"location":"concepts/#example-operator-workflow","title":"Example Operator Workflow","text":"<pre><code>flowchart TD\n    ADXClusterCRD --&gt;|provision| ADXCluster\n    ADXCluster --&gt;|ready| IngestorCRD\n    IngestorCRD --&gt;|deploy| Ingestor\n    Ingestor --&gt;|ready| CollectorCRD\n    CollectorCRD --&gt;|deploy| Collector\n    Collector --&gt;|ready| AlerterCRD\n    AlerterCRD --&gt;|deploy| Alerter</code></pre>"},{"location":"concepts/#example-crds","title":"Example CRDs","text":"<ul> <li>ADXCluster: <pre><code>apiVersion: adx-mon.azure.com/v1\nkind: ADXCluster\nmetadata:\n  name: prod-adx-cluster\nspec:\n  clusterName: prod-metrics\n  endpoint: \"https://prod-metrics.kusto.windows.net\"\n  provision:\n    subscriptionId: \"00000000-0000-0000-0000-000000000000\"\n    resourceGroup: \"adx-monitor-prod\"\n    location: \"eastus2\"\n    skuName: \"Standard_L8as_v3\"\n    tier: \"Standard\"\n    managedIdentityClientId: \"11111111-1111-1111-1111-111111111111\"\n</code></pre></li> <li>Ingestor: <pre><code>apiVersion: adx-mon.azure.com/v1\nkind: Ingestor\nmetadata:\n  name: prod-ingestor\nspec:\n  image: \"ghcr.io/azure/adx-mon/ingestor:v1.0.0\"\n  replicas: 3\n  endpoint: \"http://prod-ingestor.monitoring.svc.cluster.local:8080\"\n  exposeExternally: false\n  adxClusterSelector:\n    matchLabels:\n      app: adx-mon\n</code></pre></li> <li>Collector: <pre><code>apiVersion: adx-mon.azure.com/v1\nkind: Collector\nmetadata:\n  name: prod-collector\nspec:\n  image: \"ghcr.io/azure/adx-mon/collector:v1.0.0\"\n  ingestorEndpoint: \"http://prod-ingestor.monitoring.svc.cluster.local:8080\"\n</code></pre></li> <li>Alerter: <pre><code>apiVersion: adx-mon.azure.com/v1\nkind: Alerter\nmetadata:\n  name: prod-alerter\nspec:\n  image: \"ghcr.io/azure/adx-mon/alerter:v1.0.0\"\n  notificationEndpoint: \"http://alerter-endpoint\"\n  adxClusterSelector:\n    matchLabels:\n      app: adx-mon\n</code></pre></li> </ul>"},{"location":"concepts/#federation-multi-cluster","title":"Federation &amp; Multi-Cluster","text":"<ul> <li>Partition Clusters: Each partition cluster is managed by its own operator and contains a subset of the data (e.g., by geo or tenant).</li> <li>Federated Cluster: A central operator manages a federated ADX cluster, providing a unified query interface and managing heartbeats and macro-expand KQL functions.</li> <li>Heartbeat Table: Partition clusters send periodic heartbeats to the federated cluster, which uses them to discover topology and liveness.</li> </ul>"},{"location":"concepts/#development-testing_3","title":"Development &amp; Testing","text":"<ul> <li>Operator can be run locally or in-cluster.</li> <li>Supports dry-run and debug modes for safe testing.</li> <li>See <code>docs/designs/operator.md</code> for advanced configuration, federation, and troubleshooting.</li> </ul>"},{"location":"concepts/#azure-data-explorer","title":"Azure Data Explorer","text":""},{"location":"concepts/#grafana","title":"Grafana","text":""},{"location":"concepts/#telemetry","title":"Telemetry","text":""},{"location":"concepts/#metrics","title":"Metrics","text":"<p>Metrics track a numeric value over time with associated labels to identify series.  Metrics are collected from Kubernetes via the Prometheus scrape protocol as well as received via prometheus remote write protocol and OTLP metrics protocol.</p> <p>Metrics are translated to a distinct table per metric.  Each metric table has the following columns:</p> <ul> <li><code>Timestamp</code> - The timestamp of the metric.</li> <li><code>Value</code> - The value of the metric.</li> <li><code>Labels</code> - A dynamic column that contains all labels associated with the metric.</li> <li><code>SeriesId</code> - A unique ID for the metric series that comprises the <code>Labels</code> and metric name.</li> </ul> <p>Labels may have common identifying attributes that can be pulled up to top level columns via update policies.  For example, the <code>pod</code> label may be common to all metrics and can be pulled up to a top level <code>Pod</code> column.</p>"},{"location":"concepts/#logs","title":"Logs","text":"<p>Logs are ingested into ADX as OTLP records. You can define custom table schemas through a Kubernetes CRD called <code>Function</code>, which represents an ADX View. This allows you to present log events in a custom format rather than querying the OTLP structure directly. Below is an example of specifying a custom schema for the Ingestor component:</p> <pre><code>apiVersion: adx-mon.azure.com/v1\nkind: Function\nmetadata:\n  name: ingestor-view\n  namespace: default\nspec:\n  body: |\n    .create-or-alter function with (view=true, folder='views') Ingestor () {\n      table('Ingestor')\n      | project msg = tostring(Body.msg),\n          lvl = tostring(Body.lvl),\n          ts = todatetime(Body.ts),\n          namespace = tostring(Body.namespace),\n          container = tostring(Body.container),\n          pod = tostring(Body.pod),\n          host = tostring(Body.host) \n    }\n  database: Logs\n</code></pre> <p>Naming the View the same as the Table ensures the View takes precedence when queried in ADX. For example:</p> <pre><code>Ingestor\n| where ts &gt; ago(1h)\n| where lvl == 'ERR'\n</code></pre> <p>Note Only one Kusto View definition is supported per CRD</p>"},{"location":"concepts/#traces","title":"Traces","text":""},{"location":"concepts/#continuous-profiling","title":"Continuous Profiling","text":""},{"location":"concepts/#alerts","title":"Alerts","text":"<p>Alerts are defined through a Kubernetes CRD called <code>AlertRule</code>.  This CRD defines the alerting criteria and the notification channels that should be used when the alert is triggered.</p> <p>Alerts are triggered when the alerting criteria is met.  The alerting criteria is defined as a KQL query that is executed against the ADX cluster.  The query is executed on a schedule and if the query returns any results, the alert triggers.  Each row of the result translates into an alert notification.</p> <p>Below is a sample alert on a metric.</p> <pre><code>---\napiVersion: adx-mon.azure.com/v1\nkind: AlertRule\nmetadata:\n  name: unique-alert-name\n  namespace: alert-namespace\nspec:\n  database: SomeDatabase\n  interval: 5m\n  query: |\n    let _from=_startTime-1h;\n    let _to=_endTime;\n    KubePodContainerStatusWaitingReason\n    | where Timestamp between (_from .. _to)\n    | where ...\n    | extend Container=tostring(Labels.container), Namespace=tostring(Labels.namespace), Pod=tostring(Labels.pod)\n    | extend Severity=3\n    | extend Title=\"Alert tittle\"\n    | extend Summary=\"Alert summary details\"\n    | extend CorrelationId=\"Unique ID to correlate alerts\"\n  autoMitigateAfter: 1h\n  destination: \"alerting provider destination\"\n  criteria:\n    cloud:\n      - AzureCloud\n</code></pre> <p>All must have the following fields:</p> <ul> <li><code>database</code> - The ADX database to execute the query.</li> <li><code>interval</code> - The interval at which the query should be executed.</li> <li><code>query</code> - The KQL query to execute.</li> <li><code>destination</code> - The destination to send the alert to.  This is provider specific.</li> </ul> <p>The query must return a table with the following columns:</p> <ul> <li><code>Severity</code> - The severity of the alert.  This is used to determine the priority of the alert.</li> <li><code>Title</code> - The title of the alert.</li> <li><code>Summary</code> - The summary of the alert.</li> <li><code>CorrelationId</code> - A unique ID to correlate alerts.  A correlation ID is necessary to prevent duplicate alerts from being sent to the destination.  If one is not specified, a new alert will be created each interval and it cannot be automitigated. If specified, the <code>CorrelationId</code> will always receive a prefix of <code>${namespace}/${name}://${CorrelationId}</code>.</li> </ul> <p>Optionally, the alert definition can define the following fields:</p> <ul> <li><code>autoMitigateAfter</code> - The amount of time after the alert is triggered that it should be automatically mitigated if it has not correlated. The automitigiation implementation is dependent on the alert destination implementation.</li> <li><code>criteria</code> - A list of criteria that must be met for the alert to trigger.  If not specified, the alert will trigger in all environments.  This is useful for alerts that should only trigger in a specific cloud or region.  The available criteria options are determined by the <code>alerter</code> tag settings.</li> </ul>"},{"location":"config/","title":"Config","text":""},{"location":"config/#collector-config","title":"Collector Config","text":"<p>Collector is configured with a TOML-formatted file. In Kubernetes deployments, this is typically within a ConfigMap mounted into the collector pod. A default config can be generated by running <code>./collector config</code>.</p>"},{"location":"config/#global-config","title":"Global Config","text":"<p>This is the top level configuration for the collector. The only required fields are <code>Endpoint</code> and <code>StorageDir</code>.</p> <pre><code># Ingestor URL to send collected telemetry.\nendpoint = 'https://ingestor.adx-mon.svc.cluster.local'\n# Path to kubernetes client config\nkube-config = '.kube/config'\n# Skip TLS verification.\ninsecure-skip-verify = true\n# Address to listen on for endpoints.\nlisten-addr = ':8080'\n# Region is a location identifier.\nregion = 'eastus'\n# Optional path to the TLS key file.\ntls-key-file = '/etc/certs/collector.key'\n# Optional path to the TLS cert bundle file.\ntls-cert-file = '/etc/certs/collector.pem'\n# Maximum number of connections to accept.\nmax-connections = 100\n# Maximum number of samples to send in a single batch.\nmax-batch-size = 1000\n# Max segment agent in seconds.\nmax-segment-age-seconds = 30\n# Maximum segment size in bytes.\nmax-segment-size = 52428800\n# Maximum allowed size in bytes of all segments on disk.\nmax-disk-usage = 53687091200\n# Interval to flush the WAL. (default 100)\nwal-flush-interval-ms = 100\n# Maximum number of concurrent transfers.\nmax-transfer-concurrency = 100\n# Storage directory for the WAL and log cursors.\nstorage-dir = '/var/lib/adx-mon'\n# Enable pprof endpoints.\nenable-pprof = true\n# Default to dropping all metrics.  Only metrics matching a keep rule will be kept.\ndefault-drop-metrics = false\n# Global Regexes of metrics to drop.\ndrop-metrics = [\n  '^kube_pod_ips$',\n  'etcd_grpc.*'\n]\n# Global Regexes of metrics to keep.\nkeep-metrics = [\n  'nginx.*'\n]\n# Attributes lifted from the Body field and added to Attributes.\nlift-attributes = [\n  'host'\n]\n\n# Global Key/value pairs of labels to add to all metrics.\n[add-labels]\n  collectedBy = 'collector'\n\n# Global labels to drop if they match a metrics regex in the format &lt;metrics regex&gt;=&lt;label name&gt;. These are dropped from all metrics collected by this agent\n[drop-labels]\n  '^nginx_connections_accepted' = '^pid$'\n\n# Global Regexes of metrics to keep if they have the given label and value. These are kept from all metrics collected by this agent\n[[keep-metrics-with-label-value]]\n  # The regex to match the label value against.  If the label value matches, the metric will be kept.\n  label-regex = 'owner'\n  # The regex to match the label value against.  If the label value matches, the metric will be kept.\n  value-regex = 'platform'\n\n[[keep-metrics-with-label-value]]\n  # The regex to match the label value against.  If the label value matches, the metric will be kept.\n  label-regex = 'type'\n  # The regex to match the label value against.  If the label value matches, the metric will be kept.\n  value-regex = 'frontend|backend'\n\n# Global labels to lift from the metric to top level columns\n[[lift-labels]]\n  # The name of the label to lift.\n  name = 'Host'\n  # The name of the column to lift the label to.\n  column = ''\n\n[[lift-labels]]\n  # The name of the label to lift.\n  name = 'cluster_name'\n  # The name of the column to lift the label to.\n  column = 'Cluster'\n\n# Key/value pairs of attributes to add to all logs.\n[add-attributes]\n  cluster = 'cluster1'\n  geo = 'eu'\n\n# Optional configuration for exporting telemetry outside of adx-mon in parallel with sending to ADX.\n# Exporters are declared here and referenced by name in each collection source.\n[exporters]\n  # Configuration for exporting metrics to an OTLP/HTTP endpoint.\n  [[exporters.otlp-metric-export]]\n    # Name of the exporter.\n    name = 'to-otlp'\n    # OTLP/HTTP endpoint to send metrics to.\n    destination = 'http://localhost:4318/v1/metrics'\n    # Default to dropping all metrics.  Only metrics matching a keep rule will be kept.\n    default-drop-metrics = true\n    # Regexes of metrics to drop.\n    drop-metrics = []\n    # Regexes of metrics to keep.\n    keep-metrics = [\n      '^kube_pod_ips$'\n    ]\n    # Regexes of metrics to keep if they have the given label and value.\n    keep-metrics-with-label-value = []\n\n    # Key/value pairs of labels to add to all metrics.\n    [exporters.otlp-metric-export.add-labels]\n      forwarded_to = 'otlp'\n\n    # Labels to drop if they match a metrics regex in the format &lt;metrics regex&gt;=&lt;label name&gt;.\n    [exporters.otlp-metric-export.drop-labels]\n      '^kube_pod_ips$' = '^ip_family'\n\n    # Key/value pairs of resource attributes to add to all metrics.\n    [exporters.otlp-metric-export.add-resource-attributes]\n      destination_namespace = 'prod-metrics'\n</code></pre>"},{"location":"config/#prometheus-scrape","title":"Prometheus Scrape","text":"<p>Prometheus scrape discovers pods with the <code>adx-mon/scrape</code> annotation as well as any defined static scrape targets. It ships any metrics to the defined ADX database.</p> <pre><code># Defines a prometheus format endpoint scraper.\n[prometheus-scrape]\n  # Database to store metrics in.\n  database = 'Metrics'\n  # Scrape interval in seconds.\n  scrape-interval = 10\n  # Scrape timeout in seconds.\n  scrape-timeout = 5\n  # Disable metrics forwarding to endpoints.\n  disable-metrics-forwarding = false\n  # Disable discovery of kubernetes pod targets.\n  disable-discovery = false\n  # Regexes of metrics to drop.\n  drop-metrics = [\n    '^kube_pod_ips$',\n    'etcd_grpc.*'\n  ]\n  # Regexes of metrics to keep.\n  keep-metrics = [\n    'nginx.*'\n  ]\n  # List of exporter names to forward metrics to.\n  exporters = []\n\n  # Defines a static scrape target.\n  [[prometheus-scrape.static-scrape-target]]\n    # The regex to match the host name against.  If the hostname matches, the URL will be scraped.\n    host-regex = '.*'\n    # The URL to scrape.\n    url = 'http://localhost:9090/metrics'\n    # The namespace label to add for metrics scraped at this URL.\n    namespace = 'monitoring'\n    # The pod label to add for metrics scraped at this URL.\n    pod = 'host-monitor'\n    # The container label to add for metrics scraped at this URL.\n    container = 'host-monitor'\n\n  # Regexes of metrics to keep if they have the given label and value.\n  [[prometheus-scrape.keep-metrics-with-label-value]]\n    # The regex to match the label value against.  If the label value matches, the metric will be kept.\n    label-regex = 'owner'\n    # The regex to match the label value against.  If the label value matches, the metric will be kept.\n    value-regex = 'platform'\n\n  [[prometheus-scrape.keep-metrics-with-label-value]]\n    # The regex to match the label value against.  If the label value matches, the metric will be kept.\n    label-regex = 'type'\n    # The regex to match the label value against.  If the label value matches, the metric will be kept.\n    value-regex = 'frontend|backend'\n</code></pre>"},{"location":"config/#prometheus-remote-write","title":"Prometheus Remote Write","text":"<p>Prometheus remote write accepts metrics from Prometheus remote write protocol. It ships metrics to the defined ADX database.</p> <pre><code># Defines a prometheus remote write endpoint.\n[[prometheus-remote-write]]\n  # Database to store metrics in.\n  database = 'Metrics'\n  # The path to listen on for prometheus remote write requests.  Defaults to /receive.\n  path = '/receive'\n  # Regexes of metrics to drop.\n  drop-metrics = [\n    '^kube_pod_ips$',\n    'etcd_grpc.*'\n  ]\n  # Regexes of metrics to keep.\n  keep-metrics = [\n    'nginx.*'\n  ]\n  # List of exporter names to forward metrics to.\n  exporters = []\n\n  # Key/value pairs of labels to add to all metrics.\n  [prometheus-remote-write.add-labels]\n    cluster = 'cluster1'\n\n  # Labels to drop if they match a metrics regex in the format &lt;metrics regex&gt;=&lt;label name&gt;.\n  [prometheus-remote-write.drop-labels]\n    '^nginx_connections_accepted' = '^pid$'\n\n  # Regexes of metrics to keep if they have the given label and value.\n  [[prometheus-remote-write.keep-metrics-with-label-value]]\n    # The regex to match the label value against.  If the label value matches, the metric will be kept.\n    label-regex = 'owner'\n    # The regex to match the label value against.  If the label value matches, the metric will be kept.\n    value-regex = 'platform'\n\n  [[prometheus-remote-write.keep-metrics-with-label-value]]\n    # The regex to match the label value against.  If the label value matches, the metric will be kept.\n    label-regex = 'type'\n    # The regex to match the label value against.  If the label value matches, the metric will be kept.\n    value-regex = 'frontend|backend'\n</code></pre>"},{"location":"config/#otel-log","title":"Otel Log","text":"<p>The Otel log endpoint accepts OTLP/HTTP logs from an OpenTelemetry sender. By default, this listens under the path <code>/v1/logs</code>.</p> <pre><code># Defines an OpenTelemetry log endpoint. Accepts OTLP/HTTP.\n[otel-log]\n  # Attributes lifted from the Body and added to Attributes.\n  lift-attributes = [\n    'host'\n  ]\n  # List of exporter names to forward logs to.\n  exporters = []\n\n  # Key/value pairs of attributes to add to all logs.\n  [otel-log.add-attributes]\n    cluster = 'cluster1'\n    geo = 'eu'\n\n  # Defines a list of transforms to apply to log lines.\n  [[otel-log.transforms]]\n    # The name of the transform to apply to the log line.\n    name = 'addattributes'\n\n    # The configuration for the transform.\n    [otel-log.transforms.config]\n      environment = 'production'\n</code></pre>"},{"location":"config/#otel-metrics","title":"Otel Metrics","text":"<p>The Otel metrics endpoint accepts OTLP/HTTP and/or OTLP/gRPC metrics from an OpenTelemetry sender.</p> <pre><code># Defines an OpenTelemetry metric endpoint. Accepts OTLP/HTTP and/or OTLP/gRPC.\n[[otel-metric]]\n  # Database to store metrics in.\n  database = 'Metrics'\n  # The path to listen on for OTLP/HTTP requests.\n  path = '/v1/otlpmetrics'\n  # The port to listen on for OTLP/gRPC requests.\n  grpc-port = 4317\n  # Regexes of metrics to drop.\n  drop-metrics = [\n    '^kube_pod_ips$',\n    'etcd_grpc.*'\n  ]\n  # Regexes of metrics to keep.\n  keep-metrics = [\n    'nginx.*'\n  ]\n  # List of exporter names to forward metrics to.\n  exporters = []\n\n  # Key/value pairs of labels to add to all metrics.\n  [otel-metric.add-labels]\n    cluster = 'cluster1'\n\n  # Labels to drop if they match a metrics regex in the format &lt;metrics regex&gt;=&lt;label name&gt;.  These are dropped from all metrics collected by this agent\n  [otel-metric.drop-labels]\n    '^nginx_connections_accepted' = '^pid$'\n\n  # Regexes of metrics to keep if they have the given label and value.\n  [[otel-metric.keep-metrics-with-label-value]]\n    # The regex to match the label value against.  If the label value matches, the metric will be kept.\n    label-regex = 'owner'\n    # The regex to match the label value against.  If the label value matches, the metric will be kept.\n    value-regex = 'platform'\n\n  [[otel-metric.keep-metrics-with-label-value]]\n    # The regex to match the label value against.  If the label value matches, the metric will be kept.\n    label-regex = 'type'\n    # The regex to match the label value against.  If the label value matches, the metric will be kept.\n    value-regex = 'frontend|backend'\n</code></pre>"},{"location":"config/#host-log","title":"Host Log","text":"<p>The host log config configures file and journald log collection. By default, Kubernetes pods with <code>adx-mon/log-destination</code> annotation will have their logs scraped and sent to the appropriate destinations.</p>"},{"location":"config/#log-type","title":"Log Type","text":"<p>The <code>log-type</code> setting defines the format of the underlying log file and determines how timestamps and log messages are extracted from structured log entries. This setting is used in <code>file-target</code> configurations to properly parse different log formats.</p> <p>Available log types:</p> <ul> <li><code>docker</code>: Parses Docker JSON-formatted logs from the Docker daemon. These logs contain structured JSON with <code>log</code>, <code>stream</code>, and <code>time</code> fields.</li> <li><code>cri</code>: Parses Container Runtime Interface (CRI) formatted logs. These logs have a specific format with timestamp, stream type, and log content separated by spaces.</li> <li><code>kubernetes</code>: Auto-detects whether the log file is in Docker or CRI format and applies the appropriate parser automatically.</li> <li><code>plain</code>: Treats each line as plain text without any structured format. Uses the current timestamp as the log timestamp.</li> <li>unset/empty: Defaults to plain text parsing when no log type is specified.</li> </ul>"},{"location":"config/#log-parsers","title":"Log Parsers","text":"<p>Parsers are used within <code>file-target</code>, <code>journal-target</code>, and in pod <code>adx-mon/log-parsers</code> annotations. These configurations configure processing the raw log message extracted from the source (e.g., a file line or a journald entry). They are defined in the <code>parsers</code> array and are applied sequentially.</p> <p>The <code>parsers</code> array accepts a list of strings, each specifying a parser type. The collector attempts to apply each parser in the order they are listed. The first parser that successfully processes the log message stops the parsing process for that message. If a parser succeeds, the resulting fields are added to the log's body.</p> <p>If no parser in the list succeeds, the original raw log message is kept in the <code>message</code> field of the log body.</p> <p>Available parser types:</p> <ul> <li><code>json</code>: Attempts to parse the entire log message string as a JSON object. If successful, the key-value pairs from the JSON object are merged into the log body. The original <code>message</code> field is typically removed or overwritten by a field from the JSON payload if one exists with the key \"message\".</li> <li><code>keyvalue</code>: Parses log messages formatted as <code>key1=value1 key2=\"quoted value\" key3=value3 ...</code>. It extracts these key-value pairs and adds them to the log body. Keys and values are strings. Values containing spaces should be quoted.</li> <li><code>space</code>: Splits the log message string by whitespace (using <code>strings.Fields</code>, which handles multiple spaces, tabs, etc.). Each resulting part is added to the log body with keys named sequentially: <code>field0</code>, <code>field1</code>, <code>field2</code>, and so on. All resulting fields are strings.</li> </ul> <pre><code># Defines a host log scraper.\n[[host-log]]\n  # Disable discovery of Kubernetes pod targets. Only one HostLog configuration can use Kubernetes discovery.\n  disable-kube-discovery = true\n  # List of exporter names to forward logs to.\n  exporters = []\n\n  # Defines a tail file target.\n  [[host-log.file-target]]\n    # The path to the file to tail.\n    file-path = '/var/log/nginx/access.log'\n    # The type of log being output. This defines how timestamps and log messages are extracted from structured log types like docker json files. Options are: docker, cri, kubernetes, plain, or unset.\n    log-type = 'plain'\n    # Database to store logs in.\n    database = 'Logs'\n    # Table to store logs in.\n    table = 'NginxAccess'\n    # Parsers to apply sequentially to the log line.\n    parsers = []\n\n  [[host-log.file-target]]\n    # The path to the file to tail.\n    file-path = '/var/log/myservice/service.log'\n    # The type of log being output. This defines how timestamps and log messages are extracted from structured log types like docker json files. Options are: docker, cri, kubernetes, plain, or unset.\n    log-type = 'plain'\n    # Database to store logs in.\n    database = 'Logs'\n    # Table to store logs in.\n    table = 'NginxAccess'\n    # Parsers to apply sequentially to the log line.\n    parsers = [\n      'json'\n    ]\n\n  # Defines a static Kubernetes pod target to scrape. These are pods managed by the Kubelet and not discoverable via the apiserver.\n  [[host-log.static-pod-target]]\n    # The namespace of the pod to scrape.\n    namespace = 'default'\n    # The name of the pod to scrape.\n    name = 'myapp'\n    # The destination to send the logs to.  Syntax matches that of adx-mon/log-destination annotations.\n    destination = 'Logs:MyApp'\n    # Parsers to apply sequentially to the log line.\n    parsers = [\n      'json'\n    ]\n\n    # The labels to match on the pod.  If the pod has all of these labels, it will be scraped.\n    [host-log.static-pod-target.label-targets]\n      app = 'myapp'\n\n  # Defines a journal target to scrape.\n  [[host-log.journal-target]]\n    # Matches for the journal reader based on journalctl MATCHES. To select a systemd unit, use the field _SYSTEMD_UNIT. (e.g. '_SYSTEMD_UNIT=avahi-daemon.service' for selecting logs from the avahi-daemon service.)\n    matches = [\n      '_SYSTEMD_UNIT=docker.service',\n      '_TRANSPORT=journal'\n    ]\n    # Database to store logs in.\n    database = 'Logs'\n    # Table to store logs in.\n    table = 'Docker'\n    # Parsers to apply sequentially to the log line.\n    parsers = []\n    # Optional journal metadata fields http://www.freedesktop.org/software/systemd/man/systemd.journal-fields.html\n    journal-fields = []\n\n  # Defines a kernel target to scrape.\n  [[host-log.kernel-target]]\n    # Database to store logs in.\n    database = 'Logs'\n    # Table to store logs in.\n    table = 'Kernel'\n    # One of emerg, alert, crit, err, warning, notice, info, debug, default is info.\n    priority = 'warning'\n\n  # Defines a list of transforms to apply to log lines.\n  [[host-log.transforms]]\n    # The name of the transform to apply to the log line.\n    name = 'addattributes'\n\n    # The configuration for the transform.\n    [host-log.transforms.config]\n      environment = 'production'\n</code></pre>"},{"location":"config/#metadata-watching","title":"Metadata Watching","text":"<p>Metadata watching enables dynamic enrichment of metrics and logs with Kubernetes node metadata (labels, annotations) and other sources. This is configured using the <code>[metadata-watch]</code> and <code>[add-metadata-labels]</code> sections.</p> <p><code>[metadata-watch]</code> is a global declaration of the dynamic metadata observers the collector should run. Define it once at the root of the config; every watcher listed there is shared across the entire process.</p> <p><code>[add-metadata-labels]</code> blocks consume those watchers and can be declared in multiple scopes. Use the top-level block to apply metadata to every signal, and add scoped blocks (for example, <code>host-log.add-metadata-labels</code> or <code>prometheus-scrape.add-metadata-labels</code>) to override or extend the mappings for specific pipelines.</p>"},{"location":"config/#kubernetes-node-metadata-watching","title":"Kubernetes Node Metadata Watching","text":"<p>Enable watching Kubernetes node labels and annotations to add them as labels to all metrics and logs. Requires both metadata-watch and add-metadata-labels sections to be configured.</p> <p>RBAC requirements: The collector's service account must be able to <code>get</code>, <code>list</code>, and <code>watch</code> the core <code>nodes</code> resource. Without these permissions the watcher fails with a \"Failed to watch\" error when attempting to read node metadata.</p> <pre><code># Optional configuration for watching dynamic metadata to add to logs and metrics.\n[metadata-watch]\n  # Defines a watcher for Kubernetes node metadata (labels, annotations), consumed by add-metadata-labels\n  [metadata-watch.kubernetes-node]\n\n# Optional global configuration for adding dynamic metadata as labels to all logs and metrics.\n[add-metadata-labels]\n  # Configures the node labels and annotations to add as labels\n  [add-metadata-labels.kubernetes-node]\n    # Mapping of node label keys to destination label key names\n    [add-metadata-labels.kubernetes-node.labels]\n      'kubernetes.io/role' = 'node_role'\n      'node.kubernetes.io/instance-type' = 'instance_type'\n\n    # Mapping of node annotation keys to destination label key names\n    [add-metadata-labels.kubernetes-node.annotations]\n      'cluster-autoscaler.kubernetes.io/safe-to-evict' = 'safe_to_evict'\n      'node.alpha.kubernetes.io/ttl' = 'node_ttl'\n</code></pre>"},{"location":"config/#layered-metadata-for-host-logs","title":"Layered Metadata for Host Logs","text":"<p>Pair a global node metadata mapping with host log-specific aliases so different destinations receive tailored labels while still using the same watcher stream.</p> <pre><code># Defines a host log scraper.\n[[host-log]]\n  # Disable discovery of Kubernetes pod targets. Only one HostLog configuration can use Kubernetes discovery.\n  disable-kube-discovery = false\n  # Defines a static Kubernetes pod target to scrape. These are pods managed by the Kubelet and not discoverable via the apiserver.\n  static-pod-target = []\n  # Defines a journal target to scrape.\n  journal-target = []\n  # Defines a kernel target to scrape.\n  kernel-target = []\n  # Defines a list of transforms to apply to log lines.\n  transforms = []\n  # List of exporter names to forward logs to.\n  exporters = []\n\n  # Optional configuration for adding dynamic metadata as labels to logs collected from this source.\n  [host-log.add-metadata-labels]\n    # Configures the node labels and annotations to add as labels\n    [host-log.add-metadata-labels.kubernetes-node]\n      # Mapping of node label keys to destination label key names\n      [host-log.add-metadata-labels.kubernetes-node.labels]\n        'kubernetes.io/hostname' = 'node_hostname'\n        'topology.kubernetes.io/zone' = 'log_zone'\n\n      # Mapping of node annotation keys to destination label key names\n      [host-log.add-metadata-labels.kubernetes-node.annotations]\n        'node.alpha.kubernetes.io/ttl' = 'log_node_ttl'\n\n  # Defines a tail file target.\n  [[host-log.file-target]]\n    # The path to the file to tail.\n    file-path = '/var/log/containers/frontend.log'\n    # The type of log being output. This defines how timestamps and log messages are extracted from structured log types like docker json files. Options are: docker, cri, kubernetes, plain, or unset.\n    log-type = 'kubernetes'\n    # Database to store logs in.\n    database = 'Logs'\n    # Table to store logs in.\n    table = 'Frontend'\n    # Parsers to apply sequentially to the log line.\n    parsers = []\n\n# Optional configuration for watching dynamic metadata to add to logs and metrics.\n[metadata-watch]\n  # Defines a watcher for Kubernetes node metadata (labels, annotations), consumed by add-metadata-labels\n  [metadata-watch.kubernetes-node]\n\n# Optional global configuration for adding dynamic metadata as labels to all logs and metrics.\n[add-metadata-labels]\n  # Configures the node labels and annotations to add as labels\n  [add-metadata-labels.kubernetes-node]\n    # Mapping of node label keys to destination label key names\n    [add-metadata-labels.kubernetes-node.labels]\n      'beta.kubernetes.io/os' = 'os'\n      'topology.kubernetes.io/region' = 'region'\n\n    # Mapping of node annotation keys to destination label key names\n    [add-metadata-labels.kubernetes-node.annotations]\n      'cluster-autoscaler.kubernetes.io/safe-to-evict' = 'global_safe_to_evict'\n</code></pre>"},{"location":"config/#metadata-for-prometheus-scrape-metrics","title":"Metadata for Prometheus Scrape Metrics","text":"<p>Configure Prometheus scrape to expose zone and instance details with names tailored to downstream consumers without globally adding node labels</p> <pre><code># Defines a prometheus format endpoint scraper.\n[prometheus-scrape]\n  # Database to store metrics in.\n  database = 'Metrics'\n  # Scrape interval in seconds.\n  scrape-interval = 15\n  # Scrape timeout in seconds.\n  scrape-timeout = 10\n  # Disable metrics forwarding to endpoints.\n  disable-metrics-forwarding = false\n  # Disable discovery of kubernetes pod targets.\n  disable-discovery = false\n  # Regexes of metrics to drop.\n  drop-metrics = []\n  # Regexes of metrics to keep.\n  keep-metrics = []\n  # Regexes of metrics to keep if they have the given label and value.\n  keep-metrics-with-label-value = []\n  # List of exporter names to forward metrics to.\n  exporters = []\n\n  # Defines a static scrape target.\n  [[prometheus-scrape.static-scrape-target]]\n    # The regex to match the host name against.  If the hostname matches, the URL will be scraped.\n    host-regex = 'frontend-.*'\n    # The URL to scrape.\n    url = 'http://frontend.monitoring.svc:9090/metrics'\n    # The namespace label to add for metrics scraped at this URL.\n    namespace = 'prod'\n    # The pod label to add for metrics scraped at this URL.\n    pod = 'frontend'\n    # The container label to add for metrics scraped at this URL.\n    container = 'web'\n\n  # Optional configuration for adding dynamic metadata as labels to metrics scraped from this source.\n  [prometheus-scrape.add-metadata-labels]\n    # Configures the node labels and annotations to add as labels\n    [prometheus-scrape.add-metadata-labels.kubernetes-node]\n      # Mapping of node label keys to destination label key names\n      [prometheus-scrape.add-metadata-labels.kubernetes-node.labels]\n        'node.kubernetes.io/instance-type' = 'machine_type'\n        'topology.kubernetes.io/zone' = 'availability_zone'\n\n      # Mapping of node annotation keys to destination label key names\n      [prometheus-scrape.add-metadata-labels.kubernetes-node.annotations]\n        'cluster-autoscaler.kubernetes.io/safe-to-evict' = 'metric_safe_to_evict'\n\n# Optional configuration for watching dynamic metadata to add to logs and metrics.\n[metadata-watch]\n  # Defines a watcher for Kubernetes node metadata (labels, annotations), consumed by add-metadata-labels\n  [metadata-watch.kubernetes-node]\n</code></pre>"},{"location":"config/#exporters","title":"Exporters","text":"<p>Exporters are used to send telemetry to external systems in parallel with data sent to Azure Data Explorer. Exporters are per-source type (e.g. Metrics, Logs). Exporters are defined under the top level configuration key <code>[exporters]</code> within a key representing the exporter type (e.g. <code>[exporters.otlp-metric-export]</code>). They are referenced by their configured <code>name</code> in the relevant telemetry collection section.</p>"},{"location":"config/#metric-exporters","title":"Metric Exporters","text":"<p>Metrics currently support exporting to OpenTelemetry OTLP/HTTP endpoints with <code>otlp-metric-exporter</code>. The exporter can be configured to drop metrics by default, and only keep metrics that match a regex or have a specific label and value.</p> <p>Metric collectors process metrics through their own metric filters and transforms prior to forwarding them to any defined exporters. The exporters then apply their own filters and transforms before sending the metrics to the destination.</p> <pre><code># Defines a prometheus format endpoint scraper.\n[prometheus-scrape]\n  # Database to store metrics in.\n  database = 'Metrics'\n  # Defines a static scrape target.\n  static-scrape-target = []\n  # Scrape interval in seconds.\n  scrape-interval = 10\n  # Scrape timeout in seconds.\n  scrape-timeout = 5\n  # Disable metrics forwarding to endpoints.\n  disable-metrics-forwarding = false\n  # Disable discovery of kubernetes pod targets.\n  disable-discovery = false\n  # Regexes of metrics to drop.\n  drop-metrics = []\n  # Regexes of metrics to keep.\n  keep-metrics = []\n  # Regexes of metrics to keep if they have the given label and value.\n  keep-metrics-with-label-value = []\n  # List of exporter names to forward metrics to.\n  exporters = [\n    'to-local-otlp',\n    'to-remote-otlp'\n  ]\n\n# Optional configuration for exporting telemetry outside of adx-mon in parallel with sending to ADX.\n# Exporters are declared here and referenced by name in each collection source.\n[exporters]\n  # Configuration for exporting metrics to an OTLP/HTTP endpoint.\n  [[exporters.otlp-metric-export]]\n    # Name of the exporter.\n    name = 'to-local-otlp'\n    # OTLP/HTTP endpoint to send metrics to.\n    destination = 'http://localhost:4318/v1/metrics'\n    # Default to dropping all metrics.  Only metrics matching a keep rule will be kept.\n    default-drop-metrics = true\n    # Regexes of metrics to drop.\n    drop-metrics = []\n    # Regexes of metrics to keep.\n    keep-metrics = [\n      '^kube_pod_ips$'\n    ]\n    # Regexes of metrics to keep if they have the given label and value.\n    keep-metrics-with-label-value = []\n\n    # Key/value pairs of labels to add to all metrics.\n    [exporters.otlp-metric-export.add-labels]\n      forwarded_to = 'otlp'\n\n    # Labels to drop if they match a metrics regex in the format &lt;metrics regex&gt;=&lt;label name&gt;.\n    [exporters.otlp-metric-export.drop-labels]\n      '^kube_pod_ips$' = '^ip_family'\n\n    # Key/value pairs of resource attributes to add to all metrics.\n    [exporters.otlp-metric-export.add-resource-attributes]\n      destination_namespace = 'prod-metrics'\n\n  [[exporters.otlp-metric-export]]\n    # Name of the exporter.\n    name = 'to-remote-otlp'\n    # OTLP/HTTP endpoint to send metrics to.\n    destination = 'https://metrics.contoso.org/v1/metrics'\n    # Default to dropping all metrics.  Only metrics matching a keep rule will be kept.\n    default-drop-metrics = true\n    # Regexes of metrics to drop.\n    drop-metrics = []\n    # Regexes of metrics to keep.\n    keep-metrics = [\n      '^service_hit_count$',\n      '^service_latency$'\n    ]\n    # Regexes of metrics to keep if they have the given label and value.\n    keep-metrics-with-label-value = []\n\n    # Key/value pairs of labels to add to all metrics.\n    [exporters.otlp-metric-export.add-labels]\n      forwarded_to = 'otlp'\n\n    # Labels to drop if they match a metrics regex in the format &lt;metrics regex&gt;=&lt;label name&gt;.\n    [exporters.otlp-metric-export.drop-labels]\n      '^service_hit_count$' = '^origin_ip$'\n\n    # Key/value pairs of resource attributes to add to all metrics.\n    [exporters.otlp-metric-export.add-resource-attributes]\n      destination_namespace = 'primary-metrics'\n</code></pre>"},{"location":"config/#log-exporters","title":"Log Exporters","text":"<p>Logs currently support exporting to fluent-forward tcp or unix domain socket endpoints with <code>fluent-forward-log-export</code>. This exporter forwards logs to the remote endpoint with a tag based on the value of the attribute <code>tag-attribute</code> within the log.</p> <p>As an example, if 'tag-attribute' is set to 'fluent-output-tag', logs with an attribute of <code>fluent-output-tag</code> -&gt; <code>service-logs</code> will be emitted with the tag <code>service-logs</code>. If the attribute is not present, the log will not be emitted by this exporter.</p> <pre><code># Defines a host log scraper.\n[[host-log]]\n  # Disable discovery of Kubernetes pod targets. Only one HostLog configuration can use Kubernetes discovery.\n  disable-kube-discovery = false\n  # Defines a tail file target.\n  file-target = []\n  # Defines a static Kubernetes pod target to scrape. These are pods managed by the Kubelet and not discoverable via the apiserver.\n  static-pod-target = []\n  # Defines a journal target to scrape.\n  journal-target = []\n  # Defines a kernel target to scrape.\n  kernel-target = []\n  # Defines a list of transforms to apply to log lines.\n  transforms = []\n  # List of exporter names to forward logs to.\n  exporters = [\n    'fluentd-tcp',\n    'fluentd-unix'\n  ]\n\n# Optional configuration for exporting telemetry outside of adx-mon in parallel with sending to ADX.\n# Exporters are declared here and referenced by name in each collection source.\n[exporters]\n  # Configuration for exporting logs to a Fluentd/Fluent Bit endpoint.\n  [[exporters.fluent-forward-log-export]]\n    # Name of the exporter.\n    name = 'fluentd-tcp'\n    # Fluentd/Fluent Bit endpoint to send logs to. Must be in the form tcp://&lt;host&gt;:&lt;port&gt; or unix:///path/to/socket.\n    destination = 'tcp://localhost:24224'\n    # Attribute key to use as the tag for the log. If the attribute is not set, the log will be ignored by this exporter.\n    tag-attribute = 'fluent-output-tag-tcp'\n\n  [[exporters.fluent-forward-log-export]]\n    # Name of the exporter.\n    name = 'fluentd-unix'\n    # Fluentd/Fluent Bit endpoint to send logs to. Must be in the form tcp://&lt;host&gt;:&lt;port&gt; or unix:///path/to/socket.\n    destination = 'unix:///var/run/fluent.sock'\n    # Attribute key to use as the tag for the log. If the attribute is not set, the log will be ignored by this exporter.\n    tag-attribute = 'fluent-output-tag-unix'\n</code></pre>"},{"location":"cookbook/","title":"Cookbook","text":""},{"location":"cookbook/#summaryrules","title":"SummaryRules","text":"<p>SummaryRules automate periodic KQL aggregations in Azure Data Explorer (ADX) for data rollups, downsampling, and ETL operations. They execute on precise time intervals and track async operations until completion.</p>"},{"location":"cookbook/#key-concepts","title":"Key Concepts","text":"<p>Time Placeholders: Always include <code>_startTime</code> and <code>_endTime</code> in your KQL body - these are automatically replaced with the calculated execution window times.</p> <p>Cluster Labels: Use cluster-specific placeholders like <code>_environment</code> or <code>_region</code> that get replaced with values from the ingestor's <code>--cluster-labels</code> configuration.</p> <p>Async Operations: Queries are submitted as ADX async operations (<code>.set-or-append async</code>) and tracked until completion. The system handles retries and operation cleanup automatically.</p> <p>Continuous Execution: Rules execute continuously based on their interval, with each execution picking up exactly where the previous one ended (no gaps or overlaps).</p>"},{"location":"cookbook/#creating-hourly-rollups-from-high-frequency-metrics","title":"Creating hourly rollups from high-frequency metrics","text":"<p>Use SummaryRules to create downsampled data for long-term storage and faster queries:</p> <pre><code>apiVersion: adx-mon.azure.com/v1\nkind: SummaryRule\nmetadata:\n  name: cpu-usage-hourly\nspec:\n  database: Metrics\n  name: CPUUsageHourly\n  body: |\n    Metrics\n    | where Timestamp between (_startTime .. _endTime)\n    | where Name == \"cpu_usage_percent\"\n    | summarize AvgCPU = avg(Value), MaxCPU = max(Value), MinCPU = min(Value) \n      by bin(Timestamp, 1h), Pod, Namespace\n  table: CPUUsageHourly\n  interval: 1h\n</code></pre>"},{"location":"cookbook/#environment-specific-rules-with-cluster-labels","title":"Environment-specific rules with cluster labels","text":"<p>Create rules that work across different environments using cluster label substitutions:</p> <pre><code>apiVersion: adx-mon.azure.com/v1\nkind: SummaryRule\nmetadata:\n  name: error-rate-by-environment\nspec:\n  database: Logs\n  name: ErrorRateByEnvironment\n  body: |\n    Logs\n    | where Timestamp between (_startTime .. _endTime)\n    | where Environment == \"_environment\"\n    | where Region == \"_region\"\n    | where Level == \"ERROR\"\n    | summarize ErrorCount = count() \n      by bin(Timestamp, 15m), Service, Pod\n    | extend Environment = \"_environment\", Region = \"_region\"\n  table: ErrorRateByEnvironment\n  interval: 15m\n</code></pre> <p>Deploy with cluster labels: <pre><code># Production environment\n./ingestor --cluster-labels=environment=production --cluster-labels=region=eastus ...\n\n# Staging environment  \n./ingestor --cluster-labels=environment=staging --cluster-labels=region=westus ...\n</code></pre></p>"},{"location":"cookbook/#cross-table-aggregations","title":"Cross-table aggregations","text":"<p>Combine data from multiple tables in a single SummaryRule:</p> <pre><code>apiVersion: adx-mon.azure.com/v1\nkind: SummaryRule\nmetadata:\n  name: service-health-summary\nspec:\n  database: Monitoring\n  name: ServiceHealthSummary\n  body: |\n    let metrics = Metrics \n    | where Timestamp between (_startTime .. _endTime);\n    | where Name in (\"http_requests_total\", \"http_request_duration_seconds\")\n    let logs = Logs\n    | where Timestamp between (_startTime .. _endTime);\n    | where Level in (\"ERROR\", \"WARN\") \n    metrics\n    | join kind=leftouter (logs | summarize ErrorCount = count() by Service, bin(Timestamp, 5m)) \n      on Service, $left.bin_Timestamp == $right.Timestamp\n    | summarize \n        RequestCount = sum(Value),\n        AvgDuration = avg(Duration),\n        ErrorCount = sum(ErrorCount)\n      by bin(Timestamp, 5m), Service\n  table: ServiceHealthSummary\n  interval: 5m\n</code></pre>"},{"location":"cookbook/#using-ingestion-delay-for-data-completeness","title":"Using Ingestion Delay for Data Completeness","text":"<p>To account for ingestion delay, summary rules can be configured with the <code>ingestionDelay</code> property.</p> <pre><code>apiVersion: adx-mon.azure.com/v1\nkind: SummaryRule\nmetadata:\n  name: external-data-summary\nspec:\n  database: ExternalMetrics\n  name: ExternalDataSummary\n  body: |\n    ExternalMetrics\n    | where Timestamp between (_startTime .. _endTime)\n    | where Source == \"external_api\"\n    | summarize\n        request_count = count(),\n        avg_response_time = avg(ResponseTime),\n        error_rate = countif(StatusCode &gt;= 400) * 100.0 / count()\n      by bin(Timestamp, 15m), Endpoint\n  table: ExternalDataSummary\n  interval: 15m\n  ingestionDelay: 5m\n</code></pre> <p>When to use Ingestion Delay: - All Kusto tables have ingestion delay: Every table in Kusto has some amount of delay between when data is observed and when it becomes queryable. Use ingestion delay to ensure your summary rules process complete data. - External data sources: External data explorer clusters might have longer ingestion delays.</p> <p>Measuring ingestion delay for a table: Use this query to understand the typical ingestion delay for your data:</p> <pre><code>TableName\n| where Timestamp &gt; ago(1d)\n| project ObservedTimestamp, IngestionTimestamp=ingestion_time()\n| extend diff = IngestionTimestamp-ObservedTimestamp\n| summarize percentiles(diff, 50, 90, 99, 100)\n</code></pre> <p>Recommended minimum delays: - Logs: At least 15 minutes - Metrics: At least 5 minutes</p> <p>Balancing delay vs. latency: - Longer delays ensure better data consistency but add latency to summary rule results - Use Kusto views to union current data with summary data for up-to-date queries</p> <p>How it works: - The execution window is shifted back by the specified delay before being aligned to the interval boundary - For example, with <code>interval: 1h</code> and <code>ingestionDelay: 15m</code>:   - Current time: 14:05   - Without delay: Process 13:00-14:00 data   - With delay: Process 12:00-13:00 data (shifted back by 10 minutes, then aligned to hour boundary)</p>"},{"location":"cookbook/#optimizing-ingestion-latency","title":"Optimizing Ingestion Latency","text":"<p>There are several layers of batching within ADX-Mon that allow tuning of throughput and latency. The defaults are configured to support ingestion latencies from source to ADX of less than one minute.  Within ADX, there are batching policies that may need to be adjusted from their defaults which is tuned for throughput.</p> <p>By default, ADX will seal a batch after 5 mins, 500 files or 1 GB of data is available.  For metrics and lower latencies needs, it is recommended to lower this to 30s, 500 files or 1 GB to achieve faster ingest latency.</p> <p>You can control ADX ingestion batching behavior at the database level using ManagementCommand resources. These policies determine how quickly data becomes available for querying after ingestion by configuring when ADX seals and commits data batches.</p> <pre><code>apiVersion: adx-mon.azure.com/v1\nkind: ManagementCommand\nmetadata:\n  name: ingestion-batching-policies\n  namespace: adx-mon\nspec:\n  body: |\n    .alter-merge database Metrics policy ingestionbatching ```\n    {\n      \"MaximumBatchingTimeSpan\" : \"00:00:30\",\n      \"MaximumNumberOfItems\" : 500,\n      \"MaximumRawDataSizeMB\" : 1024\n    }\n    ```\n    .alter-merge database Logs policy ingestionbatching ```\n    {\n      \"MaximumBatchingTimeSpan\" : \"00:05:00\",\n      \"MaximumNumberOfItems\" : 500,\n      \"MaximumRawDataSizeMB\" : 4096\n    }\n    ```\n</code></pre> <p>Policy Parameters: - MaximumBatchingTimeSpan: Maximum time to wait before sealing a batch (format: HH:MM:SS) - MaximumNumberOfItems: Maximum number of items (blobs) per batch - MaximumRawDataSizeMB: Maximum uncompressed data size per batch in MB</p> <p>Latency vs. Throughput Trade-offs: - Low latency (30 seconds): Use for real-time dashboards and alerting data that needs to be queryable quickly - Higher throughput (5 minutes): Use for bulk data that doesn't require immediate availability, reduces ingestion overhead - Balanced approach: Start with 5 minutes and adjust based on query patterns and data volume</p>"},{"location":"cookbook/#per-table-policies","title":"Per-Table Policies","text":"<p>There may be some tables that you need to lower the ingestion latency from the default database policies.  Specific tables can be ingested more quickly with a table level ingestion policy.</p> <pre><code>apiVersion: adx-mon.azure.com/v1\nkind: ManagementCommand\nmetadata:\n  name: ingestion-batching-policies\n  namespace: adx-mon\nspec:\n  body: |\n    .alter-merge table MetricsDB.MyTable policy ingestionbatching ```\n    {\n      \"MaximumBatchingTimeSpan\" : \"00:00:10\",\n      \"MaximumNumberOfItems\" : 500,\n      \"MaximumRawDataSizeMB\" : 1024\n    }\n    ```\n</code></pre> <p>Lowering ingestion latency incurs higher resource demands on the ADX side which can increase costs under high volume.</p> <p>Best Practices: - Apply policies during initial cluster setup before heavy ingestion begins - Use shorter batching times for metrics databases that power real-time dashboards - Use longer batching times for logs or historical data where query latency is less critical - Monitor ingestion performance with <code>.show ingestion failures</code> and <code>.show operations</code> commands - Consider your SummaryRule <code>ingestionDelay</code> settings when tuning batching policies - ensure the delay is longer than the batching timespan - Test policy changes on non-production databases first to understand the impact on query availability - Change Metrics database policies to 30s, 500 files and 1GB for &lt; 1min queryability - Change Logs database policies to 5m, 500 files and 4GB for higher throughput</p> <p>Relationship to SummaryRule ingestionDelay: - Database ingestion batching policies control when data is sealed and becomes queryable in ADX - SummaryRule <code>ingestionDelay</code> should be set longer than the database's <code>MaximumBatchingTimeSpan</code> to ensure data completeness - Example: If your database uses <code>MaximumBatchingTimeSpan: 00:05:00</code>, set SummaryRule <code>ingestionDelay: 10m</code> or longer</p>"},{"location":"cookbook/#alerting","title":"Alerting","text":""},{"location":"cookbook/#conditional-execution-with-criteria-criteriaexpression","title":"Conditional Execution with criteria / criteriaExpression","text":"<p>Apply rules only on certain clusters or environments without duplicating manifests.</p> <pre><code>apiVersion: adx-mon.azure.com/v1\nkind: SummaryRule\nmetadata:\n  name: hourly-cpu-prod-public\nspec:\n  database: Metrics\n  name: CPUUsageHourlyProdPublic\n  table: CPUUsageHourly\n  interval: 1h\n  body: |\n    Metrics\n    | where Timestamp between (_startTime .. _endTime)\n    | where Name == \"cpu_usage_percent\"\n    | summarize AvgCPU = avg(Value) by bin(Timestamp, 1h), Pod, Namespace\n  criteria:\n    region: [eastus, westus]\n  criteriaExpression: env == 'prod' &amp;&amp; cloud == 'public'\n</code></pre> <p>Behavior formula: <code>(criteria empty OR any match) AND (criteriaExpression empty OR expression true)</code>.</p> <pre><code>apiVersion: adx-mon.azure.com/v1\nkind: MetricsExporter\nmetadata:\n  name: core-metrics-eu\nspec:\n  database: Metrics\n  name: CoreMetrics\n  query: |\n    Metrics\n    | where Timestamp between (_startTime .. _endTime)\n    | where Region == '_region'\n  interval: 5m\n  criteriaExpression: region in ['westeurope','northeurope'] &amp;&amp; env != 'dev'\n</code></pre> <p>If the expression fails to parse or evaluate the exporter execution is skipped.</p>"},{"location":"cookbook/#detecting-rate-of-change-in-a-metric","title":"Detecting rate of change in a metric","text":""},{"location":"cookbook/#detecting-increase-in-error-messages","title":"Detecting increase in error messages","text":""},{"location":"cookbook/#metrics","title":"Metrics","text":""},{"location":"cookbook/#annotating-pods-for-collection","title":"Annotating pods for collection","text":""},{"location":"cookbook/#adding-static-scrape-targets","title":"Adding static scrape targets","text":""},{"location":"cookbook/#logging","title":"Logging","text":""},{"location":"cookbook/#instrumenting-logs-for-collection","title":"Instrumenting logs for collection","text":""},{"location":"cookbook/#monitoring-and-troubleshooting-summaryrules","title":"Monitoring and Troubleshooting SummaryRules","text":""},{"location":"cookbook/#checking-rule-status","title":"Checking Rule Status","text":"<p>Check the status of your SummaryRules using kubectl:</p> <pre><code># List all SummaryRules and their status\nkubectl get summaryrules -o wide\n\n# Get detailed status for a specific rule\nkubectl describe summaryrule my-summary-rule\n\n# Check the conditions for operational details\nkubectl get summaryrule my-summary-rule -o jsonpath='{.status.conditions}' | jq\n</code></pre>"},{"location":"cookbook/#understanding-rule-conditions","title":"Understanding Rule Conditions","text":"<p>SummaryRules use three main condition types:</p> <ol> <li><code>summaryrule.adx-mon.azure.com</code>: Overall rule status</li> <li><code>True</code>: Rule is healthy and executing successfully</li> <li><code>False</code>: Rule has errors (check message for details)</li> <li> <p><code>Unknown</code>: Rule is processing or has pending operations</p> </li> <li> <p><code>summaryrule.adx-mon.azure.com/OperationId</code>: Active async operations</p> </li> <li>Contains JSON array of up to 200 tracked async operations</li> <li> <p>Each operation has OperationId, StartTime, and EndTime</p> </li> <li> <p><code>summaryrule.adx-mon.azure.com/LastSuccessfulExecution</code>: Last successful completion time</p> </li> <li>Contains RFC3339Nano timestamp of when the last execution completed successfully</li> <li>Used to calculate the next execution window</li> </ol>"},{"location":"cookbook/#common-issues-and-solutions","title":"Common Issues and Solutions","text":"<p>Rule not executing:  - Check that the rule's database matches the ingestor's target database - Verify the rule condition status is not <code>False</code> (indicating previous failure) - Ensure the interval has elapsed since the last execution</p> <p>Query failures: - Validate your KQL syntax in ADX directly - Check that source tables exist and have data in the time range - Verify cluster label placeholders match your ingestor's <code>--cluster-labels</code></p> <p>Missing time ranges: - Rules automatically handle gaps - if an execution fails, the next successful execution will process from where it left off - Check the <code>LastSuccessfulExecution</code> condition to see the actual coverage</p> <p>Operations stuck in progress: - Operations older than 25 hours are automatically cleaned up - ADX operations can be viewed with <code>.show operations</code> in the ADX cluster - Stuck operations may indicate ADX cluster issues or resource constraints</p>"},{"location":"cookbook/#performance-considerations","title":"Performance Considerations","text":"<p>Interval Sizing: Choose intervals that balance freshness with ADX performance: - High-frequency data: 5-15 minutes - Standard metrics: 1 hour - Large datasets: Several hours or daily</p> <p>Query Optimization:  - Use time range filters efficiently: <code>where Timestamp between (_startTime .. _endTime)</code> - Consider table partitioning and indexing in ADX - Test query performance in ADX before deploying rules</p> <p>Concurrency: The system tracks up to 200 concurrent async operations per rule. For high-frequency rules, ensure operations complete faster than new ones are submitted.</p>"},{"location":"cookbook/#best-practices","title":"Best Practices","text":""},{"location":"cookbook/#design-for-idempotency","title":"Design for Idempotency","text":"<p>Always design your queries to be idempotent - they should produce the same result if run multiple times on the same data:</p> <pre><code># Good: Uses summarize which is naturally idempotent\nbody: |\n  Metrics\n  | where Timestamp between (_startTime .. _endTime)\n  | summarize avg(Value) by bin(Timestamp, 1h), Pod\n\n# Avoid: Direct data copying without aggregation\nbody: |\n  Metrics  \n  | where Timestamp between (_startTime .. _endTime)\n</code></pre>"},{"location":"cookbook/#use-appropriate-table-names","title":"Use Appropriate Table Names","text":"<p>Choose descriptive table names that indicate the aggregation level and purpose:</p> <pre><code># Examples of good table names\ntable: MetricsHourly           # Hourly rollups of metrics\ntable: ErrorRate15Min         # 15-minute error rate calculations  \ntable: DailyResourceUsage     # Daily resource usage summaries\n</code></pre>"},{"location":"cookbook/#mitigate-latency-with-views","title":"Mitigate Latency with Views","text":"<p>When using longer ingestion delays for data consistency, create views that combine historical summaries with recent data for up-to-date queries. See the Function CRD documentation for examples of creating views that union historical summary data with recent raw data.</p> <p>This approach allows you to: - Use longer ingestion delays (e.g., 30 minutes) for consistent historical data - Provide up-to-date data through views that union recent raw data - Balance data consistency with query freshness</p>"},{"location":"cookbook/#environment-separation","title":"Environment Separation","text":"<p>Use cluster labels to create environment-agnostic rules:</p> <pre><code># Rule works across environments\nbody: |\n  Logs\n  | where Environment == \"_environment\"  # Replaced with actual environment\n  | where Region == \"_region\"          # Replaced with actual region\n  | summarize ErrorCount = count() by bin(Timestamp, 15m)\n</code></pre> <p>Deploy with environment-specific labels: <pre><code># Production\n./ingestor --cluster-labels=environment=prod --cluster-labels=region=us-east-1\n\n# Staging  \n./ingestor --cluster-labels=environment=staging --cluster-labels=region=us-west-2\n</code></pre></p>"},{"location":"cookbook/#handle-schema-evolution","title":"Handle Schema Evolution","text":"<p>Design queries that gracefully handle schema changes:</p> <pre><code>body: |\n  Metrics\n  | where Timestamp between (_startTime .. _endTime)\n  | extend Pod = coalesce(tostring(Pod), tostring(PodName), \"unknown\")  # Handle column renames\n  | where isnotempty(Value) and isfinite(Value)                         # Handle data quality\n  | summarize avg(Value) by bin(Timestamp, 1h), Pod\n</code></pre>"},{"location":"cookbook/#test-before-deployment","title":"Test Before Deployment","text":"<p>Always test your KQL queries in ADX before creating SummaryRules:</p> <pre><code>// Test your query with sample time ranges first\nlet _startTime = datetime(2024-01-01T12:00:00Z);\nlet _endTime = datetime(2024-01-01T13:00:00Z);\nMetrics\n| where Timestamp between (_startTime .. _endTime)\n| summarize avg(Value) by bin(Timestamp, 1h)\n</code></pre>"},{"location":"cookbook/#alerting_1","title":"Alerting","text":""},{"location":"crds/","title":"CRD Reference","text":"<p>This page summarizes all Custom Resource Definitions (CRDs) managed by adx-mon, with links to detailed documentation, example YAML, field explanations, and intended use cases for each type.</p> CRD Kind Description Spec Fields Summary Example Link ADXCluster Azure Data Explorer cluster (provisioned or existing, supports federation/partitioning) clusterName, endpoint, databases, provision, role, federation Operator Design Ingestor Ingests telemetry from collectors, manages WAL, uploads to ADX image, replicas, endpoint, exposeExternally, adxClusterSelector Operator Design Collector Collects metrics/logs/traces, forwards to Ingestor image, ingestorEndpoint Operator Design Alerter Runs alert rules, sends notifications image, notificationEndpoint, adxClusterSelector Operator Design SummaryRule Automates periodic KQL aggregations with async operation tracking, time window management, cluster label substitutions, and criteria-based execution database, name, body, table, interval, criteria Summary Rules Function Defines KQL functions/views for ADX name, body, database, table, isView, parameters Schema ETL ManagementCommand Declarative cluster management commands command, args, target, schedule Management Commands"},{"location":"crds/#adxcluster","title":"ADXCluster","text":"<p>Purpose: Defines an Azure Data Explorer (ADX) cluster for adx-mon to use, either by provisioning a new cluster or connecting to an existing one. Supports federation and partitioning for multi-cluster topologies.</p> <p>Example: <pre><code>apiVersion: adx-mon.azure.com/v1\nkind: ADXCluster\nmetadata:\n  name: prod-adx-cluster\nspec:\n  clusterName: prod-metrics\n  endpoint: \"https://prod-metrics.kusto.windows.net\" # Omit to provision a new cluster\n  databases:\n    - databaseName: Metrics\n      telemetryType: Metrics\n      retentionInDays: 90\n    - databaseName: Logs\n      telemetryType: Logs\n      retentionInDays: 30\n  provision:\n    subscriptionId: \"00000000-0000-0000-0000-000000000000\"\n    resourceGroup: \"adx-monitor-prod\"\n    location: \"eastus2\"\n    skuName: \"Standard_L8as_v3\"\n    tier: \"Standard\"\n  role: Partition\n  federation:\n    federatedClusters:\n      - endpoint: \"https://federated.kusto.windows.net\"\n        heartbeatDatabase: \"FleetDiscovery\"\n        heartbeatTable: \"Heartbeats\"\n        managedIdentityClientId: \"xxxx-xxxx\"\n    partitioning:\n      geo: \"EU\"\n      location: \"westeurope\"\n</code></pre> Key Fields: - <code>clusterName</code>: Name for the ADX cluster. - <code>endpoint</code>: Existing ADX cluster URI (omit to provision new). - <code>databases</code>: List of databases to create/use. - <code>provision</code>: Azure provisioning details (required if not using <code>endpoint</code>). - <code>role</code>: <code>Partition</code> (default) or <code>Federated</code> for multi-cluster. - <code>federation</code>: Federation/partitioning config for multi-cluster.</p> <p>Intended Use: Provision or connect to ADX clusters, including advanced federation/partitioning for geo-distributed or multi-tenant setups.</p>"},{"location":"crds/#ingestor","title":"Ingestor","text":"<p>Purpose: Deploys the Ingestor, which buffers and uploads telemetry to ADX. Supports scaling, endpoint customization, and cluster selection.</p> <p>Example: <pre><code>apiVersion: adx-mon.azure.com/v1\nkind: Ingestor\nmetadata:\n  name: prod-ingestor\nspec:\n  image: \"ghcr.io/azure/adx-mon/ingestor:v1.0.0\"\n  replicas: 3\n  endpoint: \"http://prod-ingestor.monitoring.svc.cluster.local:8080\"\n  exposeExternally: false\n  adxClusterSelector:\n    matchLabels:\n      app: adx-mon\n</code></pre> Key Fields: - <code>image</code>: Container image for the ingestor. - <code>replicas</code>: Number of replicas. - <code>endpoint</code>: Service endpoint (auto-generated if omitted). - <code>exposeExternally</code>: Whether to expose outside the cluster. - <code>adxClusterSelector</code>: Label selector for target ADXCluster.</p> <p>Intended Use: Buffer, batch, and upload telemetry from Collectors to ADX, with support for sharding and high availability.</p>"},{"location":"crds/#collector","title":"Collector","text":"<p>Purpose: Deploys the Collector, which gathers metrics, logs, and traces from the environment and forwards them to the Ingestor.</p> <p>Example: <pre><code>apiVersion: adx-mon.azure.com/v1\nkind: Collector\nmetadata:\n  name: prod-collector\nspec:\n  image: \"ghcr.io/azure/adx-mon/collector:v1.0.0\"\n  ingestorEndpoint: \"http://prod-ingestor.monitoring.svc.cluster.local:8080\"\n</code></pre> Key Fields: - <code>image</code>: Container image for the collector. - <code>ingestorEndpoint</code>: Endpoint for the Ingestor (auto-discovered if omitted).</p> <p>Intended Use: Collect telemetry from Kubernetes nodes, scrape Prometheus endpoints, and forward to Ingestor.</p>"},{"location":"crds/#alerter","title":"Alerter","text":"<p>Purpose: Deploys the Alerter, which runs alert rules (KQL queries) and sends notifications to external systems.</p> <p>Example: <pre><code>apiVersion: adx-mon.azure.com/v1\nkind: Alerter\nmetadata:\n  name: prod-alerter\nspec:\n  image: \"ghcr.io/azure/adx-mon/alerter:v1.0.0\"\n  notificationEndpoint: \"http://alerter-endpoint\"\n  adxClusterSelector:\n    matchLabels:\n      app: adx-mon\n</code></pre> Key Fields: - <code>image</code>: Container image for the alerter. - <code>notificationEndpoint</code>: Where to send alert notifications. - <code>adxClusterSelector</code>: Label selector for target ADXCluster.</p> <p>Intended Use: Run scheduled KQL queries and send alerts to HTTP endpoints (e.g., Alertmanager, PagerDuty).</p>"},{"location":"crds/#summaryrule","title":"SummaryRule","text":"<p>Purpose: Automates periodic KQL aggregations (rollups, downsampling, or data import) in ADX.</p> <p>Example: <pre><code>apiVersion: adx-mon.azure.com/v1\nkind: SummaryRule\nmetadata:\n  name: hourly-avg-metric\nspec:\n  database: Metrics\n  name: HourlyAvg\n  body: |\n    SomeMetric\n    | where Timestamp between (_startTime .. _endTime)\n    | summarize avg(Value) by bin(Timestamp, 1h)\n  table: SomeMetricHourlyAvg\n  interval: 1h\n</code></pre></p> <p>Environment-Specific Example with Cluster Labels: <pre><code>apiVersion: adx-mon.azure.com/v1\nkind: SummaryRule\nmetadata:\n  name: environment-specific-summary\nspec:\n  database: MyDB\n  name: EnvSummary\n  body: |\n    MyTable\n    | where Timestamp between (_startTime .. _endTime)\n    | where Environment == \"_environment\"\n    | where Region == \"_region\" \n    | summarize count() by bin(Timestamp, 1h)\n  table: MySummaryTable\n  interval: 1h\n</code></pre></p> <p>Criteria / CriteriaExpression Based Conditional Execution: <pre><code>apiVersion: adx-mon.azure.com/v1\nkind: SummaryRule\nmetadata:\n  name: region-specific-summary\nspec:\n  database: MyDB\n  name: RegionalSummary\n  body: |\n    MyTable\n    | where Timestamp between (_startTime .. _endTime)\n    | summarize count() by bin(Timestamp, 1h)\n  table: MySummaryTable\n  interval: 1h\n  criteria:\n    region:\n      - eastus\n      - westus\n    environment:\n      - production\n  # Optional CEL expression \u2013 all cluster labels are exposed as lower-cased string variables\n  criteriaExpression: |\n    (region in ['eastus','westus']) &amp;&amp; environment == 'production'\n</code></pre></p> <p>Key Fields: - <code>database</code>: Target ADX database. - <code>name</code>: Logical name for the rule. - <code>body</code>: KQL query to run. Must include <code>_startTime</code> and <code>_endTime</code> placeholders for time range filtering. Can optionally use <code>&lt;key&gt;</code> placeholders for environment-specific values. - <code>table</code>: Destination table for results. - <code>interval</code>: How often to run the summary (e.g., <code>1h</code>). - <code>criteria</code>: (Optional) Key/value pairs used to determine when a summary rule can execute. If empty or omitted, the rule always executes. Keys and values are deployment-specific and configured on ingestor instances via <code>--cluster-labels</code>. For a rule to execute, any one of the criteria must match (OR logic). Matching is case-insensitive. - <code>criteriaExpression</code>: (Optional) A CEL expression evaluated against the ingestor's cluster labels (all label keys are lower\u2011cased and provided as string variables). Combined semantics: <code>(criteria empty OR any criteria pair matches) AND (criteriaExpression empty OR expression evaluates to true)</code>. Invalid expressions (parse/type/eval error) result in the rule being skipped.</p> <p>Required Placeholders: - <code>_startTime</code>: Replaced with the start time of the current execution interval as <code>datetime(...)</code>. - <code>_endTime</code>: Replaced with the end time of the current execution interval as <code>datetime(...)</code>.</p> <p>Optional Cluster Label Substitutions: - <code>&lt;key&gt;</code>: Replaced with cluster-specific values defined by the ingestor's <code>--cluster-labels=&lt;key&gt;=&lt;value&gt;</code> command line arguments. Values are automatically quoted with single quotes for safe KQL usage.</p> <p>Intended Use: Automate rollups, downsampling, or ETL in ADX by running scheduled KQL queries. Use cluster label substitutions to create environment-agnostic rules that work across different deployments.</p>"},{"location":"crds/#how-summaryrules-work-internally","title":"How SummaryRules Work Internally","text":""},{"location":"crds/#shared-execution-selection-logic-criteria-criteriaexpression","title":"Shared Execution Selection Logic (criteria / criteriaExpression)","text":"<p>All rule-like CRDs (AlertRule, SummaryRule, MetricsExporter) now share a unified evaluation path:</p> <ol> <li>Normalize cluster label keys to lower case and expose them as CEL variables (string values also lower-cased for value comparison convenience).</li> <li>Evaluate legacy <code>criteria</code> map with OR semantics (case-insensitive key + value matches).</li> <li>If a <code>criteriaExpression</code> is present, compile and evaluate it with CEL.</li> <li>Execute only if both the map check and the expression pass (empty pieces are permissive).</li> </ol> <p>Examples: <pre><code># Map only\ncriteria:\n  region: [eastus, westus]\n\n# Expression only\ncriteriaExpression: region in ['eastus','westus'] &amp;&amp; environment == 'prod'\n\n# Both (AND semantics)\ncriteria:\n  region: [eastus]\ncriteriaExpression: environment == 'prod' &amp;&amp; cloud == 'public'\n</code></pre></p> <p>This logic is implemented once in <code>pkg/celutil</code> and reused by Alerter, Ingestor (SummaryRules), and Metrics Exporter for consistency.</p> <p>SummaryRules are managed by the Ingestor's <code>SummaryRuleTask</code>, which runs periodically to:</p> <ol> <li> <p>Time Window Management: Calculate precise execution windows based on the last successful execution time and the rule's interval. First execution starts from current time minus one interval; subsequent executions continue from where the previous execution ended.</p> </li> <li> <p>Async Operation Lifecycle: Submit KQL queries to ADX using <code>.set-or-append async &lt;table&gt; &lt;| &lt;query&gt;</code> and track the resulting async operations through completion. Each operation gets an OperationId that's monitored until completion.</p> </li> <li> <p>State Tracking: Uses Kubernetes conditions to track:</p> </li> <li><code>SummaryRuleOwner</code>: Overall rule status (True/False/Unknown)</li> <li><code>SummaryRuleOperationIdOwner</code>: Stores up to 200 async operations as JSON in the condition message</li> <li> <p><code>SummaryRuleLastSuccessfulExecution</code>: Tracks the end time of the last successful execution</p> </li> <li> <p>Operation Polling: Regularly polls ADX's <code>.show operations</code> to check status of tracked async operations. Operations can be in states: <code>Completed</code>, <code>Failed</code>, <code>InProgress</code>, <code>Throttled</code>, or <code>Scheduled</code>.</p> </li> <li> <p>Resilience: Handles ADX cluster restarts, network issues, and operation retries. Operations older than 25 hours are automatically cleaned up if they fall out of ADX's 24-hour operations window.</p> </li> </ol> <p>Execution Triggers: Rules are submitted when: - Rule is being deleted - Rule was updated (new generation) - Time for next interval has elapsed</p> <p>Error Handling: Uses <code>UpdateStatusWithKustoErrorParsing()</code> to extract meaningful error messages from ADX responses and truncate long errors to 256 characters.</p>"},{"location":"crds/#function","title":"Function","text":"<p>Purpose: Defines a KQL function or view in ADX, allowing you to encapsulate reusable queries or present custom schemas for logs/metrics.</p> <p>Example: <pre><code>apiVersion: adx-mon.azure.com/v1\nkind: Function\nmetadata:\n  name: custom-log-view\nspec:\n  body: |\n    .create-or-alter function with (view=true, folder='views') CustomLogView () {\n      Logs\n      | extend Level = tostring(Body.lvl), Message = tostring(Body.msg)\n      | project Timestamp, Level, Message, Pod = Resource.pod\n    }\n  database: Logs\n</code></pre></p> <p>Key Fields: - <code>name</code>: Name of the function/view in ADX. - <code>body</code>: KQL body of the function. - <code>database</code>: Target database. - <code>table</code>: (Optional) Table the function is associated with (required for views). - <code>isView</code>: If true, creates a view. - <code>parameters</code>: List of parameters for the function.</p> <p>Intended Use: Encapsulate reusable KQL logic or present custom schemas for easier querying.</p>"},{"location":"crds/#managementcommand","title":"ManagementCommand","text":"<p>Purpose: Declaratively run management operations (e.g., optimize, update policy) on ADX clusters.</p> <p>Example: <pre><code>apiVersion: adx-mon.azure.com/v1\nkind: ManagementCommand\nmetadata:\n  name: optimize-table\nspec:\n  command: .optimize\n  args:\n    - Logs\n    - kind=moveextents\n  target: Logs\n  schedule: \"0 2 * * *\"  # Run daily at 2am\n</code></pre> Key Fields: - <code>command</code>: The Kusto management command to run. - <code>args</code>: Arguments for the command. - <code>target</code>: Target table or database. - <code>schedule</code>: Cron schedule for execution.</p> <p>Intended Use: Automate cluster/table management tasks on a schedule.</p> <ul> <li>For full field documentation and advanced usage, see the linked sections above or the Operator Design doc.</li> <li>Example YAML for each CRD is provided above and in the linked documentation.</li> <li>Federation and partitioning options are described in the Federation section.</li> </ul> <p>Return to Index or Concepts.</p>"},{"location":"guides/","title":"Guides","text":""},{"location":"guides/#end-to-end-clickhouse-pipeline","title":"End-to-end ClickHouse pipeline","text":"<p>This guide walks through wiring the collector, ingestor, and local tooling to land telemetry in ClickHouse instead of Azure Data Explorer. Follow it when you want to evaluate the ClickHouse backend outside of Kubernetes or run hybrid deployments.</p>"},{"location":"guides/#prerequisites","title":"Prerequisites","text":"<ul> <li>A reachable ClickHouse cluster (self-hosted, managed service, or the dev harness below).</li> <li>Credentials with privileges to create databases/tables or an existing schema that matches the     default metrics/logs layout (<code>metrics_samples</code>, <code>otel_logs</code>).</li> <li><code>collector</code> and <code>ingestor</code> binaries or container images built from this repository.</li> </ul>"},{"location":"guides/#1-configure-the-ingestor","title":"1. Configure the ingestor","text":"<p>Select the ClickHouse backend and provide DSNs per stream using the existing endpoint flags. Each value follows the <code>&lt;database&gt;=&lt;dsn&gt;</code> convention:</p> <pre><code>ingestor \\\n    --storage-backend=clickhouse \\\n    --metrics-kusto-endpoints \"observability=clickhouse://default:devpass@clickhouse.internal:9000/observability\" \\\n    --logs-kusto-endpoints \"observability_logs=clickhouse://default:devpass@clickhouse.internal:9000/observability_logs\"\n</code></pre> <p>Key details:</p> <ul> <li>Supported DSNs include <code>clickhouse://</code>, <code>tcp://</code>, and <code>https://</code>. Use <code>secure=1</code> (or <code>secure=true</code>)     when you want TLS with the native protocol; HTTPS implies TLS automatically.</li> <li>Provide CA bundles or client certificates via the uploader configuration when mutual TLS is     required. If you pass only a username/password in the DSN the client continues to use basic auth.</li> <li>You can list multiple endpoints per flag to fan out batches to different clusters.</li> </ul>"},{"location":"guides/#2-configure-the-collector","title":"2. Configure the collector","text":"<p>The collector continues to scrape and buffer data in WAL segments, but it must tag those segments for ClickHouse before forwarding them to the ingestor. Set <code>storage-backend = \"clickhouse\"</code> in the TOML config (or pass <code>--storage-backend=clickhouse</code>) and keep the ingest endpoint pointed at the ingestor.</p> <p>Example snippet derived from <code>tools/clickhouse/collector-minimal.toml</code>:</p> <pre><code>endpoint = \"https://ingestor:9090\"\ninsecure-skip-verify = true\nstorage-backend = \"clickhouse\"\nstorage-dir = \"/var/lib/adx-mon/collector\"\n\n[[prometheus-remote-write]]\ndatabase = \"observability\"\npath = \"/receive\"\n\n[[otel-metric]]\ndatabase = \"observability\"\npath = \"/v1/metrics\"\ngrpc-port = 4317\n\n[[host-log]]\ndisable-kube-discovery = true\n\n    [[host-log.journal-target]]\n    matches = [\"_SYSTEMD_UNIT=docker.service\"]\n    database = \"observability_logs\"\n    table = \"docker_journal\"\n</code></pre> <p>Restart the collector after updating the config so it loads the new backend mapping.</p>"},{"location":"guides/#3-validate-ingestion","title":"3. Validate ingestion","text":"<p>Once both components are running, send a test signal (for example, via OTLP/HTTP or OTLP/gRPC) and confirm the data arrives:</p> <pre><code>clickhouse-client --host clickhouse.internal \\\n    --query \"SELECT name, value::Float64, labels FROM observability.metrics_samples ORDER BY timestamp DESC LIMIT 10\"\n</code></pre> <p>If you enabled TLS via <code>secure=1</code>, include <code>--secure</code> and the appropriate certificate flags when running <code>clickhouse-client</code>.</p>"},{"location":"guides/#4-use-the-dev-harness-optional-but-recommended","title":"4. Use the dev harness (optional but recommended)","text":"<p><code>./tools/clickhouse/dev_stack.sh start</code> spins up ClickHouse, the ingestor, and the collector on a single Docker network. It applies the settings described above, seeds the schema, exposes the Tabix UI at http://localhost:8123/play, and mounts WAL directories under <code>tools/clickhouse/.stack/</code>. See the README for day-two commands such as <code>stop</code>, <code>cleanup</code>, and log streaming.</p>"},{"location":"guides/#troubleshooting-tips","title":"Troubleshooting tips","text":"<ul> <li>If the ingestor logs <code>remote error: tls: handshake failure</code>, double-check that your DSN requested     TLS (<code>https://</code> or <code>secure=1</code>) and that the ClickHouse server is actually serving TLS on that port.</li> <li>Missing data typically means the ingestor dispatch queue is empty\u2014verify that the collector is     targeting the same database name you provided to the ClickHouse uploader.</li> <li>Use <code>./tools/clickhouse/dev_stack.sh logs ingestor</code> to watch the pipeline in real time during local     experiments.</li> </ul>"},{"location":"ingestor/","title":"Ingestor Overview","text":"<p>The ingestor is an aggregation point for adx-mon to ingest metrics, logs and traces into Azure Data Explore (ADX) in a performant and cost-efficient manner.</p> <p>ADX recommends sending batches of data in 100MB to 1GB (uncompressed) [1] for optimal ingestion and reduced costs.  In a typical Kubernetes cluster, there are many pods, each with their own metrics, logs and traces.  The ingestor aggregates these data sources into batches and sends them to ADX instead of each pod or node sending data individually.  This reduces the number of small files that ADX must later  merge into larger files which can impact query latency and increase resource requirements.</p>"},{"location":"ingestor/#design","title":"Design","text":"<p>The ingestor is designed to be deployed as a Kubernetes StatefulSet with multiple replicas.  It exposes several ingress points for metrics, logs and traces collection.  The metrics ingress API is a Prometheus remote write endpoint and can support other interfaces such as OpenTelemetry in the future.  </p> <p>The ingestor can be dynamically scaled up or down based on the amount of data being ingested.  It has a configurable amount of storage to buffer data before sending it to ADX.  It will store and coalesce data until it reaches a maximum size or a maximum age.  Once either of these thresholds are reached, the data is sent to ADX.</p> <p>Several design decisions were made to optimize availability and performance.  For example, if a pod is able to recieve data it will store it locally and attempt to optimize it for upload to ADX later by transferring small segments to peers. The performance and throughput of a single ingestor pod is limited by network bandwidth and disk throughput of attached  storage.  The actual processing performed by the ingestor is fairly minimal and is mostly unmarshalling the incoming data (Protobufs) and writing it to disk in an append only storage format.</p>"},{"location":"ingestor/#data-flow","title":"Data Flow","text":""},{"location":"ingestor/#metrics","title":"Metrics","text":"<p>Each ingestor pod is fronted by a load balancer.  The ingestor receives data from the Kubernetes cluster via the  Prometheus remote write endpoint.  When a pod receives data, it writes it locally to a file that corresponds to a given table and schema.  These files are called Segments and are part of Write Ahead Log (WAL) for each table. </p> <p>If Segment has reached the max age or max size, the ingestor will either upload the file directly to ADX or transfer the file to a peer that is assigned to own that particular table.  The transfer is performed if the file is less than 100MB so that the file can be merged with other files before being uploaded to ADX.  </p> <p>If the transfer fails, the instance will upload the file directly. </p> <p>During upload, batches of files, per table, are compressed and uploaded to ADX as stream.  This allows many small files to be merged into a single file which reduces the number of files that ADX must merge later.  Each batch is sized to be between 100MB and 1GB (uncompressed) to align with Kusto ingestion best practices.</p>"},{"location":"ingestor/#logs","title":"Logs","text":""},{"location":"ingestor/#traces","title":"Traces","text":""},{"location":"ingestor/#clickhouse-sink","title":"ClickHouse sink","text":"<p>The ingestor can stream batches to ClickHouse in addition to Azure Data Explorer. Switch the storage backend to <code>clickhouse</code> when you want to land telemetry in a ClickHouse cluster\u2014either for hybrid deployments or for local development.</p>"},{"location":"ingestor/#configure-the-ingestor","title":"Configure the ingestor","text":"<ol> <li>Launch the binary with <code>--storage-backend=clickhouse</code> (or set     <code>INGESTOR_STORAGE_BACKEND=clickhouse</code>).</li> <li> <p>Provide one or more metrics and logs endpoints with the existing     <code>--metrics-kusto-endpoints</code> / <code>--logs-kusto-endpoints</code> flags. Each value uses the familiar     <code>&lt;database&gt;=&lt;dsn&gt;</code> format. Example:</p> <pre><code>--metrics-kusto-endpoints \"observability=clickhouse://default:devpass@clickhouse:9000/observability\"\n--logs-kusto-endpoints \"observability_logs=clickhouse://default:devpass@clickhouse:9000/observability_logs\"\n</code></pre> <p>The ingestor automatically provisions the required tables (<code>metrics_samples</code> and <code>otel_logs</code>) and maps lifted labels/resources to ClickHouse columns. 3. TLS is disabled unless the DSN explicitly requests it. Use either an HTTPS-based DSN or append <code>secure=1</code>/<code>secure=true</code> when using the native protocol. Optional certificates can be supplied via the ClickHouse uploader configuration (CA, client cert/key, or <code>InsecureSkipVerify</code>).</p> </li> </ol> <p>Tip: You can target multiple ClickHouse clusters by repeating the endpoint flags; the ingestor fans out batches to every configured DSN for a given stream.</p>"},{"location":"ingestor/#align-the-collector","title":"Align the collector","text":"<p>Collect the same WAL format by setting the collector configuration to <code>storage-backend = \"clickhouse\"</code> (or pass the <code>--storage-backend</code> CLI flag). No other configuration changes are required\u2014the collector still delivers segments to the ingestor over the transfer API.</p>"},{"location":"ingestor/#local-harness","title":"Local harness","text":"<p>The helper script in <code>tools/clickhouse/dev_stack.sh</code> spins up a complete collector \u2192 ingestor \u2192 ClickHouse pipeline on Docker. It builds fresh images (unless <code>SKIP_BUILD=1</code>), launches a ClickHouse server with pre-created <code>observability</code> and <code>observability_logs</code> databases, and wires the collector to the ingestor using the clickhouse backend. See <code>tools/clickhouse/README.md</code> for usage, including how to seed OTLP metrics and query the data with <code>clickhouse-client</code> or the Tabix UI at <code>http://localhost:8123/play</code>.</p>"},{"location":"ingestor/#wal-format-and-storage","title":"WAL Format and Storage","text":"<p>The Ingestor uses a Write-Ahead Log (WAL) for durable, append-only buffering of telemetry data before upload to Azure Data Explorer. The WAL binary format is fully documented in Concepts: WAL Segment File Format, including: - Segment and block header layout - Field encoding and versioning - Compression (S2/Snappy) - Repair and compatibility</p> <p>For advanced troubleshooting, integrations, or recovery, see the WAL format section and the implementation in <code>pkg/wal/segment.go</code>.</p> <p>[1] https://docs.microsoft.com/en-us/azure/data-explorer/ingest-best-practices</p>"},{"location":"installation/","title":"Installation","text":""},{"location":"quick-start/","title":"Quick Start","text":"<p>This guide will deploy ADX-Mon on an Azure Kubernetes Service (AKS) cluster and send collected telemetry to an Azure Data Explorer cluster.  It will deploy all components within the cluster and demonstrate  how to enable telemetry collection on a pod and query it from Azure Data Explorer.</p>"},{"location":"quick-start/#pre-requisites","title":"Pre-Requisites","text":"<p>You will need the following to complete this guide.</p> <ul> <li>An AKS cluster</li> <li>An Azure Data Explorer cluster</li> <li>A Linux environment with Azure CLI installed</li> </ul> <p>These clusters should be in the same region for this guid.  You should have full admin access to both clusters.</p>"},{"location":"quick-start/#deploy-adx-mon","title":"Deploy ADX-Mon","text":"<pre><code>bash &lt;(curl -s  https://raw.githubusercontent.com/Azure/adx-mon/main/build/k8s/bundle.sh)\n</code></pre> <p>This script will prompt you for the name or you AKS and ADX cluster and configure them to accept telemetry from ADX-Mon components. It configures the provided ADX cluster with <code>Metrics</code> and <code>Logs</code> databases and deploy the Collector and Ingestor services to begin collecting and shipping data from the AKS cluster.</p>"},{"location":"quick-start/#annotate-your-pods","title":"Annotate Your Pods","text":"<p>Telemetry can be ingested into ADX-Mon by annotating your pods with the appropriate annotations or shipping it through OTEL endpoints.  The simplest model is to annotate your pods with the appropriate annotations.</p>"},{"location":"quick-start/#metrics","title":"Metrics","text":"<p>ADX-Mon collector support scraping Prometheus style endpoints directly. To enable this, annotate your pods with these annotations, configuring the port and path to match the port and http path for the metrics endpoint of your service.</p> <p>Prometheus metric names will be transformed from <code>snake_case</code> to <code>TitleCase</code>. As an example, <code>adxmon_collector_logs_sent</code> is transformed into <code>AdxmonCollectorLogsSent</code> when sent to Kusto.</p> <pre><code>adx-mon/scrape: \"true\"\nadx-mon/port: \"8080\"      # Can use numeric ports\nadx-mon/path: \"/metrics\"\n</code></pre> <p>You can also use named ports for better readability:</p> <pre><code>adx-mon/scrape: \"true\"\nadx-mon/port: \"metrics\"   # Use the named port from your container spec\nadx-mon/path: \"/metrics\"\n</code></pre> <p>For multiple endpoints, use the <code>targets</code> annotation:</p> <pre><code>adx-mon/scrape: \"true\"\n# Mix named and numeric ports as needed\nadx-mon/targets: \"/metrics:metrics,/health:9000\"\n</code></pre> <p>You can also scrape HTTPS endpoints by setting the <code>scheme</code> annotation. Authentication uses the service token from within the Collector pod.</p> <pre><code>adx-mon/scrape: \"true\"\nadx-mon/scheme: \"https\"\nadx-mon/path: \"/metrics\"\n</code></pre>"},{"location":"quick-start/#logs","title":"Logs","text":"<p>ADX-Mon collector supports discovering logs from pods. To configure the destination Kusto table, annotate your pod with <code>adx-mon/log-destination</code> with a value of <code>DBName:TableName</code>.</p> <p>By default, collector parses each log line as plaintext, but an optional list of log-parsers can be defined as a comma separated list. It currently supports json-formatted log lines in addition to plaintext.</p> <pre><code>adx-mon/scrape: \"true\"\nadx-mon/log-destination: \"Logs:Collector\"\nadx-mon/log-parsers: json\n</code></pre>"},{"location":"quick-start/#query-your-data","title":"Query Your Data","text":"<p>After bootstrapping, the provided ADX cluster will begin to populate with metrics and logs. The <code>Metrics</code> database is configured with a default <code>30s</code> batch latency for ADX ingestion to optimize for latency, while <code>Logs</code> is configured with a default of <code>5m</code> to optimize for throughput.</p>"},{"location":"quick-start/#metric-examples","title":"Metric Examples","text":"<pre><code>// Process a prometheus-style counter to determine the number of logs sent by a given source in Collector in the last hour\nAdxmonCollectorLogsSent\n| where Timestamp &gt; ago(1h)\n// convert from point-in-time count to amount of increase per interval\n| invoke prom_delta()\n| summarize TotalSent=sum(Value) by Host=tostring(Labels.host), Source=tostring(Labels.source)\n</code></pre>"},{"location":"quick-start/#log-examples","title":"Log Examples","text":"<pre><code>// Get all non-info logs from Collector from the last hour\nCollector\n| where Timestamp &gt; ago(1h)\n| where Body.lvl != \"INF\"\n| project Timestamp, Level=Body.lvl, Msg=Body.msg, Pod=Resource.pod, Host=Resource.host\n</code></pre> <pre><code>// Graph the number of container creations every 15 minutes over the last day, per Host and Cluster\nlet _lookback=ago(1d);\nKubelet\n| where Timestamp &gt; _lookback\n| where Body.message contains \"Syncloop ADD\"\n| make-series Creations=count() default=0 on Timestamp from _lookback to now() step 15m by Host=tostring(Resource.host), Cluster=tostring(Resource.cluster)\n| render timechart \n</code></pre>"},{"location":"quick-start/#setup-dashboards","title":"Setup Dashboards","text":"<p>Any ADX compatible visualization tool can be used to visualize collected telemetry. ADX Dashboards is a simple solution that is native to ADX. You can also use Azure Managed Grafana with the Azure Data Explorer datasource to leverage Grafana's powerful visualization capabilities.</p>"},{"location":"quick-start/#azure-managed-grafana-via-quick-start-script","title":"Azure Managed Grafana via quick-start script","text":"<p>As part of the quick-start script, one can set up an Azure Managed Grafana (AMG) instance. After configuring ADX-Mon on the AKS cluster the script will prompt you about it, and you can provide the name of an existing Grafana instance you have access to or decide to create one. You will also be prompted about importing pre-built dashboards to monitor the AKS cluster.</p> <p>Note: The script tries to create the AMG instance in the same resource group as the ADX cluster. </p> <p>Here's a glimpse of what comes as part of the pre-built dashboards:</p>"},{"location":"quick-start/#api-server","title":"API Server","text":""},{"location":"quick-start/#cluster-info","title":"Cluster Info","text":""},{"location":"quick-start/#metrics-stats","title":"Metrics Stats","text":""},{"location":"quick-start/#namespaces","title":"Namespaces","text":""},{"location":"quick-start/#pods","title":"Pods","text":""},{"location":"designs/kusto-to-metrics/","title":"Kusto-to-Metrics Integration","text":""},{"location":"designs/kusto-to-metrics/#problem-statement","title":"Problem Statement","text":"<p>ADX-Mon currently provides two distinct capabilities: 1. SummaryRules - Execute KQL queries on a schedule and ingest results into ADX tables (<code>api/v1/summaryrule_types.go</code>) 2. OTLP Metrics Exporters - Export metrics to external observability systems (<code>collector/export/metric_otlp.go</code>)</p> <p>However, there's no direct mechanism to execute KQL queries and export the results as metrics to observability platforms. Organizations often need to: - Execute KQL queries on ADX data and export results as standardized metrics - Export these metrics to external systems (Prometheus, DataDog, etc.) at regular intervals - Create derived metrics from complex KQL aggregations for dashboards and alerting</p> <p>Currently, users must either: - Use SummaryRules to materialize data in ADX tables, then build custom exporters to transform and export - Implement entirely separate infrastructure outside of ADX-Mon</p> <p>This leads to: - Duplicated infrastructure for metric transformation and export - Inconsistent metric schemas across different teams - Additional storage costs for intermediate table materialization - Complex multi-step pipelines that are difficult to maintain</p>"},{"location":"designs/kusto-to-metrics/#solution-overview","title":"Solution Overview","text":"<p>We propose implementing a new <code>adxexporter</code> component that processes <code>MetricsExporter</code> CRDs to execute KQL queries and export results as metrics. This provides a streamlined, declarative way to create KQL-to-metrics pipelines with two complementary output modes:</p> <ol> <li>Prometheus Scraping Mode (Phase 1): Execute queries and expose results as Prometheus metrics on <code>/metrics</code> endpoint for Collector to scrape</li> <li>Direct OTLP Push Mode (Phase 2): Execute queries and push results directly to OTLP endpoints with backlog/retry capabilities</li> </ol>"},{"location":"designs/kusto-to-metrics/#key-requirements","title":"Key Requirements","text":"<ol> <li>Direct KQL Execution: Execute KQL queries directly without requiring intermediate table storage</li> <li>SummaryRule-like Behavior: Share time management, scheduling, and criteria-based execution patterns</li> <li>Criteria-Based Deployment: Support cluster-label based filtering for secure, distributed execution</li> <li>Resilient Operation: Provide backlog and retry capabilities for reliable metric delivery</li> </ol>"},{"location":"designs/kusto-to-metrics/#architecture-overview","title":"Architecture Overview","text":"<p>The solution introduces a new <code>adxexporter</code> component that operates as a Kubernetes controller, watching <code>MetricsExporter</code> CRDs and executing their configured KQL queries on schedule.</p>"},{"location":"designs/kusto-to-metrics/#core-components","title":"Core Components","text":"<ol> <li><code>adxexporter</code> - New standalone component (<code>cmd/adxexporter</code>) that:</li> <li>Watches <code>MetricsExporter</code> CRDs via Kubernetes API</li> <li>Executes KQL queries against ADX on specified intervals</li> <li>Transforms results to metrics format</li> <li> <p>Outputs metrics via Prometheus scraping or direct OTLP push</p> </li> <li> <p><code>MetricsExporter</code> CRD - Kubernetes custom resource defining:</p> </li> <li>KQL query to execute</li> <li>Transform configuration for metric conversion</li> <li>Execution criteria and scheduling</li> <li> <p>Output mode configuration</p> </li> <li> <p>Integration with Existing Components:</p> </li> <li>Collector discovers and scrapes <code>adxexporter</code> instances via pod annotations</li> <li>Operator manages <code>MetricsExporter</code> CRD lifecycle (optional integration)</li> </ol>"},{"location":"designs/kusto-to-metrics/#deployment-architecture","title":"Deployment Architecture","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Collector     \u2502    \u2502   adxexporter   \u2502    \u2502      ADX        \u2502\n\u2502                 \u2502    \u2502                 \u2502    \u2502   Clusters      \u2502\n\u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502    \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502    \u2502                 \u2502\n\u2502 \u2502Pod Discovery\u2502\u25c4\u251c\u2500\u2500\u2500\u2500\u2524 \u2502/metrics     \u2502 \u2502    \u2502                 \u2502\n\u2502 \u2502&amp; Scraping   \u2502 \u2502    \u2502 \u2502endpoint     \u2502 \u2502    \u2502                 \u2502\n\u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502    \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502    \u2502                 \u2502\n\u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502    \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502    \u2502                 \u2502\n\u2502 \u2502OTLP Export  \u2502 \u2502    \u2502 \u2502KQL Query    \u2502\u25c4\u251c\u2500\u2500\u2500\u2500\u2524                 \u2502\n\u2502 \u2502Targets      \u2502 \u2502    \u2502 \u2502Execution    \u2502 \u2502    \u2502                 \u2502\n\u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502    \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502    \u2502                 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                       \u2502 \u2502CRD Watch &amp;  \u2502 \u2502\n       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502Reconciliation\u2502 \u2502\n       \u2502               \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n       \u2502               \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n       \u2502\n       \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Kubernetes     \u2502\n\u2502   API Server    \u2502\n\u2502                 \u2502\n\u2502 MetricsExporter \u2502\n\u2502     CRDs        \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"designs/kusto-to-metrics/#output-modes","title":"Output Modes","text":""},{"location":"designs/kusto-to-metrics/#phase-1-prometheus-scraping-mode","title":"Phase 1: Prometheus Scraping Mode","text":"<ul> <li><code>adxexporter</code> exposes <code>/metrics</code> endpoint with OpenTelemetry metrics library</li> <li>Metrics refreshed per <code>MetricsExporter.Interval</code></li> <li>Collector discovers via pod annotations: <code>adx-mon/scrape: \"true\"</code></li> <li>Simple, fast implementation leveraging existing Collector scraping infrastructure</li> </ul>"},{"location":"designs/kusto-to-metrics/#phase-2-direct-otlp-push-mode","title":"Phase 2: Direct OTLP Push Mode","text":"<ul> <li><code>adxexporter</code> pushes metrics directly to OTLP endpoints</li> <li>Failed exports queued in CRD backlog for retry</li> <li>Supports historical data backfill capabilities</li> <li>More resilient but requires additional backlog management infrastructure</li> </ul>"},{"location":"designs/kusto-to-metrics/#design-approach","title":"Design Approach","text":""},{"location":"designs/kusto-to-metrics/#adxexporter-component","title":"adxexporter Component","text":"<p>The <code>adxexporter</code> is a new standalone Kubernetes component with its own binary at <code>cmd/adxexporter</code>. It functions as a Kubernetes controller that watches <code>MetricsExporter</code> CRDs and executes their KQL queries on schedule.</p>"},{"location":"designs/kusto-to-metrics/#command-line-interface","title":"Command Line Interface","text":"<pre><code>adxexporter \\\n  --cluster-labels=\"region=eastus,environment=production,team=platform\" \\\n  --otlp-endpoint=\"http://otel-collector:4317\" \\\n  --web.enable-lifecycle=true \\\n  --web.listen-address=\":8080\" \\\n  --web.telemetry-path=\"/metrics\"\n</code></pre> <p>Parameters:</p> <ul> <li> <p><code>--cluster-labels</code>: Comma-separated key=value pairs defining this instance's cluster identity. Used for criteria-based filtering of MetricsExporter CRDs. This follows the same pattern as the Ingestor component documented in ADX-Mon Configuration.</p> </li> <li> <p><code>--otlp-endpoint</code>: (Phase 2) OTLP endpoint URL for direct push mode. When specified, enables direct OTLP export with backlog capabilities.</p> </li> <li> <p><code>--web.enable-lifecycle</code>: (Phase 1) Boolean flag to enable the Prometheus metrics HTTP server. When enabled, exposes query results as Prometheus metrics on the specified address/path for Collector scraping. This follows Prometheus naming conventions.</p> </li> <li> <p><code>--web.listen-address</code>: Address and port for the Prometheus metrics server (default: \":8080\")</p> </li> <li> <p><code>--web.telemetry-path</code>: HTTP path for metrics endpoint (default: \"/metrics\")</p> </li> </ul>"},{"location":"designs/kusto-to-metrics/#criteria-based-execution","title":"Criteria-Based Execution","text":"<p>Similar to the Ingestor component, <code>adxexporter</code> uses cluster labels to determine which <code>MetricsExporter</code> CRDs it should process. This enables:</p> <ul> <li>Security Boundaries: Only process MetricsExporters appropriate for this cluster's data classification</li> <li>Geographic Distribution: Deploy region-specific adxexporter instances</li> <li>Team Isolation: Separate processing by team ownership</li> <li>Resource Optimization: Distribute load across appropriate instances</li> </ul> <p>Example Criteria Matching: <pre><code># MetricsExporter CRD\nspec:\n  criteria:\n    region: [\"eastus\", \"westus\"]\n    environment: [\"production\"]\n\n# adxexporter instance\n--cluster-labels=\"region=eastus,environment=production,team=sre\"\n# \u2705 Matches: region=eastus AND environment=production\n</code></pre></p>"},{"location":"designs/kusto-to-metrics/#metricsexporter-crd","title":"MetricsExporter CRD","text":"<p>The <code>MetricsExporter</code> CRD defines KQL queries and their transformation to metrics format. It shares core patterns with <code>SummaryRule</code> but targets metrics output instead of ADX table ingestion.</p> <pre><code>// MetricsExporterSpec defines the desired state of MetricsExporter\ntype MetricsExporterSpec struct {\n    // Database is the name of the database to query\n    Database string `json:\"database\"`\n\n    // Body is the KQL query to execute\n    Body string `json:\"body\"`\n\n    // Interval defines how often to execute the query and refresh metrics\n    Interval metav1.Duration `json:\"interval\"`\n\n    // Transform defines how to convert query results to metrics\n    Transform TransformConfig `json:\"transform\"`\n\n    // Criteria for cluster-based execution selection (same pattern as SummaryRule)\n    Criteria map[string][]string `json:\"criteria,omitempty\"`\n}\n\ntype TransformConfig struct {\n    // MetricNameColumn specifies which column contains the metric name\n    MetricNameColumn string `json:\"metricNameColumn,omitempty\"`\n\n    // ValueColumn specifies which column contains the metric value  \n    ValueColumn string `json:\"valueColumn\"`\n\n    // TimestampColumn specifies which column contains the timestamp\n    TimestampColumn string `json:\"timestampColumn\"`\n\n    // LabelColumns specifies columns to use as metric labels\n    LabelColumns []string `json:\"labelColumns,omitempty\"`\n\n    // DefaultMetricName provides a fallback if MetricNameColumn is not specified\n    DefaultMetricName string `json:\"defaultMetricName,omitempty\"`\n}\n</code></pre>"},{"location":"designs/kusto-to-metrics/#key-design-principles","title":"Key Design Principles","text":"<ol> <li> <p>Standalone Operation: <code>adxexporter</code> operates independently from existing ingestor/operator infrastructure, providing clear separation of concerns and deployment flexibility.</p> </li> <li> <p>Dual Output Strategy: </p> </li> <li>Phase 1 (Prometheus): Fast implementation using existing Collector scraping capabilities</li> <li> <p>Phase 2 (Direct OTLP): Enhanced resilience with backlog and retry mechanisms</p> </li> <li> <p>Criteria-Based Filtering: Leverages the same cluster-label approach as Ingestor for secure, distributed execution across different environments and teams.</p> </li> <li> <p>SummaryRule Consistency: Shares core behavioral patterns including time management, scheduling logic, and KQL query execution patterns.</p> </li> <li> <p>Cloud-Native Integration: Seamless discovery via Kubernetes pod annotations and integration with existing Collector infrastructure.</p> </li> </ol>"},{"location":"designs/kusto-to-metrics/#detailed-implementation","title":"Detailed Implementation","text":""},{"location":"designs/kusto-to-metrics/#phase-1-prometheus-scraping-mode_1","title":"Phase 1: Prometheus Scraping Mode","text":"<p>In the initial implementation, <code>adxexporter</code> exposes transformed KQL query results as Prometheus metrics on a <code>/metrics</code> endpoint.</p>"},{"location":"designs/kusto-to-metrics/#metrics-exposure-workflow","title":"Metrics Exposure Workflow","text":"<ol> <li>CRD Discovery: <code>adxexporter</code> watches Kubernetes API for <code>MetricsExporter</code> CRDs matching its cluster labels</li> <li>Query Execution: Execute KQL queries on specified intervals using <code>_startTime</code> and <code>_endTime</code> parameters</li> <li>Metrics Transformation: Convert query results to Prometheus metrics using OpenTelemetry metrics library</li> <li>Metrics Registration: Register/update metrics in the OpenTelemetry metrics registry</li> <li>HTTP Exposure: Serve metrics via HTTP endpoint for Collector scraping</li> </ol>"},{"location":"designs/kusto-to-metrics/#collector-integration","title":"Collector Integration","text":"<p>The existing Collector component discovers <code>adxexporter</code> instances via Kubernetes pod annotations:</p> <pre><code># adxexporter Deployment/Pod\nmetadata:\n  annotations:\n    adx-mon/scrape: \"true\"\n    adx-mon/port: \"8080\"\n    adx-mon/path: \"/metrics\"\n</code></pre> <p>The Collector's pod discovery mechanism automatically detects these annotations and adds the <code>adxexporter</code> instances to its scraping targets.</p>"},{"location":"designs/kusto-to-metrics/#limitations-of-phase-1","title":"Limitations of Phase 1","text":"<ul> <li>Point-in-Time Metrics: Prometheus metrics represent current state; no historical backfill capability</li> <li>Scraping Dependency: Relies on Collector's scraping schedule, not direct control over export timing</li> <li>No Retry Logic: Failed queries result in stale metrics until next successful execution</li> </ul>"},{"location":"designs/kusto-to-metrics/#phase-2-direct-otlp-push-mode_1","title":"Phase 2: Direct OTLP Push Mode","text":"<p>In the enhanced implementation, <code>adxexporter</code> can push metrics directly to OTLP endpoints with full backlog and retry capabilities.</p>"},{"location":"designs/kusto-to-metrics/#enhanced-workflow","title":"Enhanced Workflow","text":"<ol> <li>CRD Discovery: Same as Phase 1</li> <li>Query Execution: Same as Phase 1</li> <li>OTLP Transformation: Convert query results directly to OTLP metrics format</li> <li>Direct Push: Send metrics to configured OTLP endpoint</li> <li>Backlog Management: Queue failed exports in CRD status for retry</li> <li>Historical Backfill: Process backlogged time windows on successful reconnection</li> </ol>"},{"location":"designs/kusto-to-metrics/#backlog-strategy","title":"Backlog Strategy","text":"<p>Unlike Prometheus scraping, direct OTLP push enables sophisticated backlog management:</p> <pre><code>type MetricsExporterStatus struct {\n    Conditions []metav1.Condition `json:\"conditions,omitempty\"`\n\n    // LastSuccessfulExecution tracks the last successfully exported time window\n    LastSuccessfulExecution *metav1.Time `json:\"lastSuccessfulExecution,omitempty\"`\n\n    // Backlog contains failed export attempts pending retry\n    Backlog []BacklogEntry `json:\"backlog,omitempty\"`\n}\n\ntype BacklogEntry struct {\n    StartTime metav1.Time `json:\"startTime\"`\n    EndTime   metav1.Time `json:\"endTime\"`\n    Attempts  int         `json:\"attempts\"`\n    LastAttempt metav1.Time `json:\"lastAttempt\"`\n    Error     string      `json:\"error,omitempty\"`\n}\n</code></pre>"},{"location":"designs/kusto-to-metrics/#shared-infrastructure-patterns","title":"Shared Infrastructure Patterns","text":"<p><code>adxexporter</code> leverages proven patterns from SummaryRule implementation while operating as an independent component:</p> Component SummaryRule adxexporter Time Management <code>NextExecutionWindow()</code>, interval-based scheduling Same patterns, independent implementation Criteria Matching Cluster label filtering Same logic, different component KQL Execution ADX query with time parameters Same patterns for query execution Status Tracking CRD conditions and backlog Similar condition management Backlog Handling Async operation queues Export retry queues (Phase 2)"},{"location":"designs/kusto-to-metrics/#component-independence","title":"Component Independence","text":"<p>Unlike the original design that integrated with ingestor infrastructure, <code>adxexporter</code> operates independently:</p> <ul> <li>Separate Binary: Own entrypoint at <code>cmd/adxexporter</code></li> <li>Independent Deployment: Deployed as separate Kubernetes workload</li> <li>Dedicated Configuration: Own command-line parameters and configuration</li> <li>Isolated Dependencies: Direct ADX connectivity without shared connection pools</li> </ul>"},{"location":"designs/kusto-to-metrics/#standard-schema-requirements","title":"Standard Schema Requirements","text":"<p>For <code>MetricsExporter</code> KQL queries to produce valid metrics, the result table must contain columns that can be mapped to the metrics format. The transformation is highly flexible and supports both simple and complex schemas.</p>"},{"location":"designs/kusto-to-metrics/#core-metrics-mapping","title":"Core Metrics Mapping","text":"<p>The <code>adxexporter</code> transforms KQL query results to metrics using this mapping:</p> KQL Column Prometheus/OTLP Field Purpose Configured via <code>valueColumn</code> Metric value The numeric metric value Configured via <code>timestampColumn</code> Metric timestamp Temporal alignment (OTLP mode) Configured via <code>metricNameColumn</code> Metric name Metric name identifier Any columns in <code>labelColumns</code> Metric labels/attributes Dimensional metadata"},{"location":"designs/kusto-to-metrics/#required-columns","title":"Required Columns","text":"<ul> <li>Value Column: Must contain numeric data (real/double/int)</li> <li>Timestamp Column: Must contain datetime data (used in OTLP mode for temporal accuracy)</li> </ul>"},{"location":"designs/kusto-to-metrics/#optional-columns","title":"Optional Columns","text":"<ul> <li>Metric Name Column: If not specified, uses <code>Transform.DefaultMetricName</code></li> <li>Label Columns: Any additional columns become metric labels/attributes</li> </ul>"},{"location":"designs/kusto-to-metrics/#simple-example-generic-use-case","title":"Simple Example - Generic Use Case","text":"<p>KQL Query: <pre><code>MyTelemetryTable\n| where Timestamp between (_startTime .. _endTime)\n| summarize \n    metric_value = avg(ResponseTime),\n    timestamp = bin(Timestamp, 5m)\n    by ServiceName, Region\n| extend metric_name = \"service_response_time_avg\"\n</code></pre></p> <p>Transform Configuration: <pre><code>transform:\n  metricNameColumn: \"metric_name\"\n  valueColumn: \"metric_value\"\n  timestampColumn: \"timestamp\"\n  labelColumns: [\"ServiceName\", \"Region\"]\n</code></pre></p> <p>Resulting Prometheus Metric (Phase 1): <pre><code># HELP service_response_time_avg Average response time by service and region\n# TYPE service_response_time_avg gauge\nservice_response_time_avg{ServiceName=\"api-gateway\",Region=\"us-east-1\"} 245.7\n\n# HELP service_response_time_avg Average response time by service and region  \n# TYPE service_response_time_avg gauge\nservice_response_time_avg{ServiceName=\"user-service\",Region=\"us-west-2\"} 189.3\n</code></pre></p> <p>Resulting OTLP Metric (Phase 2): <pre><code>{\n  \"name\": \"service_response_time_avg\",\n  \"gauge\": {\n    \"dataPoints\": [{\n      \"value\": 245.7,\n      \"timeUnixNano\": \"1640995200000000000\",\n      \"attributes\": [\n        {\"key\": \"ServiceName\", \"value\": \"api-gateway\"},\n        {\"key\": \"Region\", \"value\": \"us-east-1\"}\n      ]\n    }]\n  }\n}\n</code></pre></p>"},{"location":"designs/kusto-to-metrics/#complex-example-advanced-analytics-use-case","title":"Complex Example - Advanced Analytics Use Case","text":"<p>For more complex schemas with additional metadata and calculated metrics:</p> <p>KQL Query: <pre><code>AnalyticsData\n| where EventTime between (_startTime .. _endTime)\n| summarize \n    Value = avg(SuccessRate),\n    Numerator = sum(SuccessCount),\n    Denominator = sum(TotalCount),\n    StartTimeUTC = min(EventTime),\n    EndTimeUTC = max(EventTime)\n    by LocationId, CustomerResourceId\n| extend metric_name = \"success_rate_analytics\"\n</code></pre></p> <p>Transform Configuration: <pre><code>transform:\n  metricNameColumn: \"metric_name\" \n  valueColumn: \"Value\"\n  timestampColumn: \"StartTimeUTC\"\n  labelColumns: [\"LocationId\", \"CustomerResourceId\", \"Numerator\", \"Denominator\", \"EndTimeUTC\"]\n</code></pre></p> <p>Resulting Prometheus Metrics (Phase 1): <pre><code># HELP success_rate_analytics Success rate analytics by location and customer\n# TYPE success_rate_analytics gauge\nsuccess_rate_analytics{LocationId=\"datacenter-01\",CustomerResourceId=\"customer-12345\",Numerator=\"1974\",Denominator=\"2000\",EndTimeUTC=\"2022-01-01T10:05:00Z\"} 0.987\n</code></pre></p> <p>Resulting OTLP Metric (Phase 2): <pre><code>{\n  \"name\": \"success_rate_analytics\",\n  \"gauge\": {\n    \"dataPoints\": [{\n      \"value\": 0.987,\n      \"timeUnixNano\": \"1640995200000000000\",\n      \"attributes\": [\n        {\"key\": \"LocationId\", \"value\": \"datacenter-01\"},\n        {\"key\": \"CustomerResourceId\", \"value\": \"customer-12345\"},\n        {\"key\": \"Numerator\", \"value\": \"1974\"},\n        {\"key\": \"Denominator\", \"value\": \"2000\"},\n        {\"key\": \"EndTimeUTC\", \"value\": \"2022-01-01T10:05:00Z\"}\n      ]\n    }]\n  }\n}\n</code></pre></p> <p>This approach allows any KQL query result to be transformed into metrics by: 1. Selecting which column contains the primary metric value 2. Choosing the timestamp column for temporal alignment (OTLP mode) 3. Mapping all other relevant columns as dimensional labels 4. Optionally specifying a dynamic or static metric name</p>"},{"location":"designs/kusto-to-metrics/#metricsexporter-crd-example","title":"MetricsExporter CRD Example","text":"<pre><code>apiVersion: adx-mon.azure.com/v1\nkind: MetricsExporter\nmetadata:\n  name: service-response-times\n  namespace: monitoring\nspec:\n  database: TelemetryDB\n  interval: 5m\n  criteria:\n    region: [\"eastus\", \"westus\"]\n    environment: [\"production\"]\n  body: |\n    ServiceTelemetry\n    | where Timestamp between (_startTime .. _endTime)\n    | summarize \n        metric_value = avg(ResponseTimeMs),\n        timestamp = bin(Timestamp, 1m)\n        by ServiceName, Environment\n    | extend metric_name = \"service_response_time_avg\"\n  transform:\n    metricNameColumn: \"metric_name\"\n    valueColumn: \"metric_value\"\n    timestampColumn: \"timestamp\"\n    labelColumns: [\"ServiceName\", \"Environment\"]\n</code></pre> <p>This example demonstrates: 1. Direct KQL Execution: Query executes directly without intermediate table storage 2. Criteria-Based Selection: Only processed by <code>adxexporter</code> instances with matching cluster labels 3. Flexible Output: Works with both Prometheus scraping (Phase 1) and OTLP push (Phase 2) modes 4. Time Window Parameters: KQL query uses <code>_startTime</code> and <code>_endTime</code> parameters (same as SummaryRule)</p>"},{"location":"designs/kusto-to-metrics/#adxexporter-deployment-example","title":"adxexporter Deployment Example","text":"<pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: adxexporter\n  namespace: adx-mon-system\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: adxexporter\n  template:\n    metadata:\n      labels:\n        app: adxexporter\n      annotations:\n        # Enable Collector discovery and scraping\n        adx-mon/scrape: \"true\"\n        adx-mon/port: \"8080\"\n        adx-mon/path: \"/metrics\"\n    spec:\n      containers:\n      - name: adxexporter\n        image: adx-mon/adxexporter:latest\n        args:\n        - --cluster-labels=region=eastus,environment=production,team=platform\n        - --web.enable-lifecycle=true\n        - --web.listen-address=:8080\n        - --web.telemetry-path=/metrics\n        ports:\n        - containerPort: 8080\n          name: metrics\n        resources:\n          requests:\n            memory: \"256Mi\"\n            cpu: \"100m\"\n          limits:\n            memory: \"512Mi\"\n            cpu: \"500m\"\n</code></pre>"},{"location":"designs/kusto-to-metrics/#integration-with-existing-components","title":"Integration with Existing Components","text":"<p>The <code>adxexporter</code> component integrates with existing ADX-Mon infrastructure while maintaining independence:</p> <ol> <li>Collector Integration: </li> <li>Collector automatically discovers <code>adxexporter</code> instances via pod annotations</li> <li>Scrapes <code>/metrics</code> endpoint and forwards to configured destinations</li> <li> <p>No additional Collector configuration required</p> </li> <li> <p>Kubernetes API Integration:</p> </li> <li><code>adxexporter</code> watches <code>MetricsExporter</code> CRDs via standard Kubernetes client-go</li> <li>Leverages existing RBAC and authentication mechanisms</li> <li> <p>Operates within standard Kubernetes security boundaries</p> </li> <li> <p>ADX Connectivity:</p> </li> <li>Direct ADX connection using same authentication patterns as other components</li> <li>Independent connection management and pooling</li> <li>Reuses existing ADX client libraries and connection patterns</li> </ol>"},{"location":"designs/kusto-to-metrics/#execution-flow","title":"Execution Flow","text":""},{"location":"designs/kusto-to-metrics/#phase-1-prometheus-scraping-mode_2","title":"Phase 1: Prometheus Scraping Mode","text":"<ol> <li>CRD Discovery: <code>adxexporter</code> discovers <code>MetricsExporter</code> CRDs matching its cluster labels</li> <li>Scheduling: Determines execution windows based on <code>Interval</code> and last execution time</li> <li>Query Execution: Execute KQL query with <code>_startTime</code> and <code>_endTime</code> parameters</li> <li>Metrics Transformation: Convert query results to Prometheus metrics format</li> <li>Registry Update: Update OpenTelemetry metrics registry with new values</li> <li>HTTP Exposure: Serve updated metrics on <code>/metrics</code> endpoint</li> <li>Collector Scraping: Collector discovers and scrapes metrics endpoint</li> <li>Status Update: Update CRD status with execution results</li> </ol>"},{"location":"designs/kusto-to-metrics/#phase-2-direct-otlp-push-mode_2","title":"Phase 2: Direct OTLP Push Mode","text":"<ol> <li>CRD Discovery: Same as Phase 1</li> <li>Scheduling: Same as Phase 1, plus backlog processing</li> <li>Query Execution: Same as Phase 1</li> <li>OTLP Transformation: Convert query results directly to OTLP format</li> <li>Direct Push: Send metrics to configured OTLP endpoint</li> <li>Backlog Management: Queue failed exports for retry</li> <li>Status Update: Update CRD status with execution and backlog state</li> </ol>"},{"location":"designs/kusto-to-metrics/#validation-and-error-handling","title":"Validation and Error Handling","text":"<p>The <code>adxexporter</code> controller validates: - KQL query syntax and database accessibility - Transform configuration matches query result schema - Cluster label criteria for CRD processing eligibility - Required columns (value, timestamp) are present in query results - OTLP endpoint connectivity (Phase 2)</p>"},{"location":"designs/kusto-to-metrics/#use-cases","title":"Use Cases","text":""},{"location":"designs/kusto-to-metrics/#use-case-1-service-performance-metrics","title":"Use Case 1: Service Performance Metrics","text":"<pre><code># MetricsExporter for service response time monitoring\napiVersion: adx-mon.azure.com/v1\nkind: MetricsExporter\nmetadata:\n  name: service-response-times\n  namespace: monitoring\nspec:\n  database: TelemetryDB\n  interval: 1m\n  criteria:\n    environment: [\"production\"]\n    team: [\"platform\"]\n  body: |\n    ServiceTelemetry\n    | where Timestamp between (_startTime .. _endTime)\n    | summarize \n        metric_value = avg(ResponseTimeMs),\n        timestamp = bin(Timestamp, 1m)\n        by ServiceName, Environment\n    | extend metric_name = \"service_response_time_avg\"\n  transform:\n    metricNameColumn: \"metric_name\"\n    valueColumn: \"metric_value\"\n    timestampColumn: \"timestamp\"\n    labelColumns: [\"ServiceName\", \"Environment\"]\n</code></pre> <p>Deployment Configuration: <pre><code># adxexporter instance matching criteria\nadxexporter \\\n  --cluster-labels=\"environment=production,team=platform,region=eastus\" \\\n  --web.enable-lifecycle=true\n</code></pre></p> <p>Key Benefits: - Direct Execution: No intermediate table storage required - Real-time Metrics: Fresh data exposed every minute via <code>/metrics</code> endpoint - Environment Isolation: Only processed by <code>adxexporter</code> instances with matching criteria - Standard Integration: Collector automatically discovers and scrapes metrics</p>"},{"location":"designs/kusto-to-metrics/#use-case-2-advanced-analytics-with-rich-metadata","title":"Use Case 2: Advanced Analytics with Rich Metadata","text":"<pre><code># MetricsExporter for complex customer analytics\napiVersion: adx-mon.azure.com/v1\nkind: MetricsExporter\nmetadata:\n  name: customer-analytics\n  namespace: analytics\nspec:\n  database: AnalyticsDB\n  interval: 15m\n  criteria:\n    team: [\"analytics\"]\n    data-classification: [\"customer-approved\"]\n  body: |\n    CustomerEvents\n    | where EventTime between (_startTime .. _endTime)\n    | summarize \n        Value = avg(SuccessRate),\n        Numerator = sum(SuccessfulRequests),\n        Denominator = sum(TotalRequests),\n        StartTimeUTC = min(EventTime),\n        EndTimeUTC = max(EventTime),\n        AvgLatency = avg(LatencyMs)\n        by LocationId, CustomerResourceId, ServiceTier\n    | extend metric_name = strcat(\"customer_success_rate_\", tolower(ServiceTier))\n  transform:\n    metricNameColumn: \"metric_name\"\n    valueColumn: \"Value\"\n    timestampColumn: \"StartTimeUTC\"\n    labelColumns: [\"LocationId\", \"CustomerResourceId\", \"ServiceTier\", \"Numerator\", \"Denominator\", \"EndTimeUTC\", \"AvgLatency\"]\n</code></pre> <p>Deployment Configuration: <pre><code># adxexporter instance for analytics team\nadxexporter \\\n  --cluster-labels=\"team=analytics,data-classification=customer-approved,region=westus\" \\\n  --web.enable-lifecycle=true \\\n  --otlp-endpoint=\"http://analytics-otel-collector:4317\"  # Phase 2\n</code></pre></p> <p>Resulting Metrics: - Primary value: Success rate percentage - Rich labels: Location, customer, service tier, raw counts, time ranges, and auxiliary metrics - Flexible naming: Dynamic metric names based on service tier - Data Governance: Only processed by appropriately classified <code>adxexporter</code> instances</p>"},{"location":"designs/kusto-to-metrics/#use-case-3-multi-region-infrastructure-monitoring","title":"Use Case 3: Multi-Region Infrastructure Monitoring","text":"<pre><code># MetricsExporter for infrastructure metrics across regions\napiVersion: adx-mon.azure.com/v1\nkind: MetricsExporter\nmetadata:\n  name: infrastructure-monitoring\n  namespace: sre\nspec:\n  database: InfrastructureDB\n  interval: 30s\n  criteria:\n    role: [\"infrastructure\"]\n    region: [\"eastus\", \"westus\", \"europe\"]\n  body: |\n    SystemMetrics\n    | where Timestamp between (_startTime .. _endTime)\n    | summarize \n        metric_value = avg(CpuUtilization),\n        timestamp = bin(Timestamp, 30s)\n        by NodeName, ClusterName, Region\n    | extend metric_name = \"node_cpu_utilization\"\n  transform:\n    metricNameColumn: \"metric_name\"\n    valueColumn: \"metric_value\"\n    timestampColumn: \"timestamp\"\n    labelColumns: [\"NodeName\", \"ClusterName\", \"Region\"]\n</code></pre> <p>Multi-Region Deployment: <pre><code># East US adxexporter\nadxexporter --cluster-labels=\"role=infrastructure,region=eastus\" --web.enable-lifecycle=true\n\n# West US adxexporter  \nadxexporter --cluster-labels=\"role=infrastructure,region=westus\" --web.enable-lifecycle=true\n\n# Europe adxexporter\nadxexporter --cluster-labels=\"role=infrastructure,region=europe\" --web.enable-lifecycle=true\n</code></pre></p> <p>Key Benefits: - High-Frequency Monitoring: 30-second metric refresh intervals - Geographic Distribution: Each region processes the same MetricsExporter with regional data - Centralized Collection: All regional <code>adxexporter</code> instances scraped by their respective Collectors - SRE Team Focus: Clear ownership through criteria-based filtering</p>"},{"location":"designs/kusto-to-metrics/#use-case-4-cross-cluster-error-rate-monitoring-with-direct-push","title":"Use Case 4: Cross-Cluster Error Rate Monitoring with Direct Push","text":"<pre><code># MetricsExporter for global error rate aggregation\napiVersion: adx-mon.azure.com/v1\nkind: MetricsExporter\nmetadata:\n  name: global-error-rates\n  namespace: sre\nspec:\n  database: GlobalMetrics\n  interval: 2m\n  criteria:\n    scope: [\"global\"]\n    priority: [\"high\", \"critical\"]\n  body: |\n    union\n      cluster('eastus-cluster').TelemetryDB.ErrorEvents,\n      cluster('westus-cluster').TelemetryDB.ErrorEvents,\n      cluster('europe-cluster').TelemetryDB.ErrorEvents\n    | where Timestamp between (_startTime .. _endTime)\n    | summarize \n        metric_value = count() * 1.0,\n        timestamp = bin(Timestamp, 1m),\n        error_rate = count() * 100.0 / countif(isnotempty(SuccessEvent))\n        by Region = tostring(split(ClusterName, '-')[0]), ServiceName\n    | extend metric_name = \"global_error_count\"\n  transform:\n    metricNameColumn: \"metric_name\"\n    valueColumn: \"metric_value\"\n    timestampColumn: \"timestamp\"\n    labelColumns: [\"Region\", \"ServiceName\", \"error_rate\"]\n</code></pre> <p>Phase 2 Deployment with Direct Push: <pre><code># Global monitoring adxexporter with OTLP push\nadxexporter \\\n  --cluster-labels=\"scope=global,priority=high\" \\\n  --otlp-endpoint=\"http://central-prometheus-gateway:4317\" \\\n  --web.enable-lifecycle=false  # Disable scraping, use direct push only\n</code></pre></p> <p>Global Monitoring Benefits: - Cross-Cluster Aggregation: Single query across multiple ADX clusters - Priority-Based Processing: Only runs on high/critical priority <code>adxexporter</code> instances - Direct Push Reliability: OTLP endpoint receives metrics with backlog/retry capabilities - Rich Context: Includes both raw counts and calculated error rates in labels</p>"},{"location":"designs/kusto-to-metrics/#configuration-strategy-and-best-practices","title":"Configuration Strategy and Best Practices","text":""},{"location":"designs/kusto-to-metrics/#adxexporter-configuration","title":"adxexporter Configuration","text":"<p>The <code>adxexporter</code> component provides flexible configuration for different deployment scenarios:</p>"},{"location":"designs/kusto-to-metrics/#phase-1-prometheus-scraping-mode_3","title":"Phase 1: Prometheus Scraping Mode","text":"<pre><code>adxexporter \\\n  --cluster-labels=\"team=platform,environment=production,region=eastus\" \\\n  --web.enable-lifecycle=true \\\n  --web.listen-address=\":8080\" \\\n  --web.telemetry-path=\"/metrics\"\n</code></pre>"},{"location":"designs/kusto-to-metrics/#phase-2-direct-otlp-push-mode_3","title":"Phase 2: Direct OTLP Push Mode","text":"<pre><code>adxexporter \\\n  --cluster-labels=\"team=analytics,data-classification=approved\" \\\n  --otlp-endpoint=\"http://otel-collector:4317\" \\\n  --web.enable-lifecycle=false\n</code></pre>"},{"location":"designs/kusto-to-metrics/#hybrid-mode-both-scraping-and-push","title":"Hybrid Mode (Both Scraping and Push)","text":"<pre><code>adxexporter \\\n  --cluster-labels=\"team=sre,priority=critical\" \\\n  --web.enable-lifecycle=true \\\n  --web.listen-address=\":8080\" \\\n  --otlp-endpoint=\"http://central-monitoring:4317\"\n</code></pre>"},{"location":"designs/kusto-to-metrics/#criteria-based-deployment-strategy","title":"Criteria-Based Deployment Strategy","text":"<p>Use the <code>criteria</code> field in MetricsExporter CRDs to control which <code>adxexporter</code> instances process them. This follows the same pattern as documented in ADX-Mon Ingestor Configuration.</p>"},{"location":"designs/kusto-to-metrics/#example-environment-based-processing","title":"Example: Environment-Based Processing","text":"<p><pre><code>spec:\n  criteria:\n    environment: [\"production\"]\n</code></pre> adxexporter Configuration: <pre><code>--cluster-labels=\"environment=production,region=eastus\"\n</code></pre> Result: Only <code>adxexporter</code> instances with <code>environment=production</code> cluster labels process this MetricsExporter</p>"},{"location":"designs/kusto-to-metrics/#example-team-and-region-based-processing","title":"Example: Team and Region-Based Processing","text":"<p><pre><code>spec:\n  criteria:\n    team: [\"analytics\", \"data-science\"]\n    region: [\"eastus\", \"westus\"]\n</code></pre> adxexporter Configuration: <pre><code>--cluster-labels=\"team=analytics,region=eastus,data-classification=approved\"\n</code></pre> Result: Processes MetricsExporter because team=analytics AND region=eastus both match</p>"},{"location":"designs/kusto-to-metrics/#example-data-classification-controls","title":"Example: Data Classification Controls","text":"<p><pre><code>spec:\n  criteria:\n    data-classification: [\"public\", \"internal\"]\n    priority: [\"high\"]\n</code></pre> adxexporter Configuration: <pre><code>--cluster-labels=\"data-classification=internal,priority=high,team=platform\"\n</code></pre> Result: Processes MetricsExporter because data-classification=internal AND priority=high both match</p>"},{"location":"designs/kusto-to-metrics/#benefits-of-criteria-based-architecture","title":"Benefits of Criteria-Based Architecture","text":"<ol> <li>Security Boundaries: Control data access based on <code>adxexporter</code> deployment classification</li> <li>Performance Isolation: Deploy separate <code>adxexporter</code> instances for high-frequency vs. low-frequency metrics</li> <li>Geographic Distribution: Regional <code>adxexporter</code> instances process region-appropriate MetricsExporters</li> <li>Team Autonomy: Teams deploy their own <code>adxexporter</code> instances with appropriate cluster labels</li> <li>Resource Optimization: Distribute MetricsExporter processing load across appropriate instances</li> </ol>"},{"location":"designs/kusto-to-metrics/#collector-integration_1","title":"Collector Integration","text":"<p>The existing Collector component automatically discovers <code>adxexporter</code> instances through Kubernetes pod annotations:</p> <pre><code># Pod annotations for Collector discovery\nmetadata:\n  annotations:\n    adx-mon/scrape: \"true\"        # Enable scraping\n    adx-mon/port: \"8080\"          # Metrics port\n    adx-mon/path: \"/metrics\"      # Metrics path\n</code></pre> <p>No additional Collector configuration is required - discovery and scraping happen automatically.</p>"},{"location":"designs/kusto-to-metrics/#implementation-roadmap","title":"Implementation Roadmap","text":"<p>This section provides a methodical breakdown of implementing the <code>adxexporter</code> component and <code>MetricsExporter</code> CRD across multiple PRs, with Phase 1 focusing on Prometheus scraping and Phase 2 adding direct OTLP push capabilities.</p>"},{"location":"designs/kusto-to-metrics/#phase-1-status-summary","title":"\ud83d\udcca Phase 1 Status Summary","text":"<p>Overall Progress: 5/7 tasks complete (71%)</p> <ul> <li>\u2705 Complete (5 tasks): Foundation, Scaffolding, Query Execution, Transform Engine, Metrics Server  </li> <li>\ud83d\udd04 Partially Complete (1 task): Status Management  </li> <li>\u274c Not Started (1 task): Collector Discovery Integration</li> </ul> <p>Key Achievements: - Complete MetricsExporter CRD with time window management and criteria matching - Functional adxexporter component with Kubernetes controller framework - Working KQL query execution with ADX integration and time window management - Full transform engine for KQL to OpenTelemetry metrics conversion with validation - Complete Prometheus metrics server using controller-runtime's shared registry - Comprehensive unit tests for all core functionality</p> <p>Remaining Work for Phase 1: - Task 6: Create Kubernetes deployment manifests with collector discovery annotations - Task 7: Implement CRD status updates to cluster (methods exist, need cluster integration) - Integration: Add end-to-end tests with Collector discovery and scraping</p> <p>Code Quality:  - \u2705 Extensive unit test coverage - \u2705 Follows ADX-Mon patterns (SummaryRule consistency) - \u2705 Proper error handling and logging - \u2705 Command-line interface with flag parsing</p>"},{"location":"designs/kusto-to-metrics/#implementation-details-found","title":"\ud83d\udd0d Implementation Details Found","text":"<p>Files Implemented: - <code>api/v1/metricsexporter_types.go</code> - Complete CRD definition with time management methods - <code>cmd/adxexporter/main.go</code> - Main component with CLI parsing and manager setup - <code>adxexporter/service.go</code> - Controller reconciler with criteria matching and query execution - <code>adxexporter/kusto.go</code> - KQL query executor with ADX client integration - <code>transform/kusto_to_metrics.go</code> - Transform engine for KQL results to OpenTelemetry metrics - Generated CRD manifests in <code>kustomize/bases/</code> and <code>operator/manifests/crds/</code></p> <p>Key Features Working: - \u2705 MetricsExporter CRD with complete spec (Database, Body, Interval, Transform, Criteria) - \u2705 Time window calculation (<code>ShouldExecuteQuery</code>, <code>NextExecutionWindow</code>) - \u2705 Cluster-label based criteria matching (case-insensitive) - \u2705 KQL query execution with <code>_startTime</code>/<code>_endTime</code> substitution - \u2705 Transform validation and column mapping (value, labels, timestamps, metric names) - \u2705 OpenTelemetry Prometheus exporter setup with namespace - \u2705 Controller-runtime manager with graceful shutdown - \u2705 Health checks (readyz/healthz endpoints on port 8081)</p> <p>Missing Implementation: - HTTP server startup for <code>/metrics</code> endpoint (OpenTelemetry backend configured but not served) - Kubernetes deployment manifests with <code>adx-mon/scrape</code> annotations - CRD status updates to Kubernetes cluster (status methods implemented locally only)</p>"},{"location":"designs/kusto-to-metrics/#phase-1-prometheus-scraping-implementation","title":"Phase 1: Prometheus Scraping Implementation","text":""},{"location":"designs/kusto-to-metrics/#1-foundation-metricsexporter-crd-definition-complete","title":"1. Foundation: MetricsExporter CRD Definition \u2705 COMPLETE","text":"<p>Goal: Establish the core data structures and API types - Deliverables:   - \u2705 Create <code>api/v1/metricsexporter_types.go</code> with complete CRD spec   - \u2705 Define <code>MetricsExporterSpec</code>, <code>TransformConfig</code>, and status types   - \u2705 Add deepcopy generation markers and JSON tags   - \u2705 Update <code>api/v1/groupversion_info.go</code> to register new types   - \u2705 Generate CRD manifests using <code>make generate-crd CMD=update</code> - Testing: \u2705 Unit tests for struct validation and JSON marshaling/unmarshaling - Acceptance Criteria: \u2705 CRD can be applied to cluster and kubectl can describe the schema</p>"},{"location":"designs/kusto-to-metrics/#2-adxexporter-component-scaffolding-complete","title":"2. adxexporter Component Scaffolding \u2705 COMPLETE","text":"<p>Goal: Create the standalone <code>adxexporter</code> component infrastructure - Deliverables:   - \u2705 Create <code>cmd/adxexporter/main.go</code> with command-line argument parsing   - \u2705 Implement cluster-labels parsing and criteria matching logic   - \u2705 Add Kubernetes client-go setup for CRD watching   - \u2705 Create basic controller framework for MetricsExporter reconciliation   - \u2705 Add graceful shutdown and signal handling - Testing: \u2705 Integration tests for component startup and CRD discovery - Acceptance Criteria: \u2705 <code>adxexporter</code> starts successfully and can discover MetricsExporter CRDs</p>"},{"location":"designs/kusto-to-metrics/#3-kql-query-execution-engine-complete","title":"3. KQL Query Execution Engine \u2705 COMPLETE","text":"<p>Goal: Implement KQL query execution with time window management - Deliverables:   - \u2705 Create ADX client connection management in <code>adxexporter</code>   - \u2705 Implement time window calculation logic (adapted from SummaryRule patterns)   - \u2705 Add KQL query execution with <code>_startTime</code>/<code>_endTime</code> parameter injection   - \u2705 Implement scheduling logic based on <code>Interval</code> and last execution tracking   - \u2705 Add comprehensive error handling for query failures - Testing: \u2705 Unit tests with mock ADX responses and integration tests with real ADX - Acceptance Criteria: \u2705 Can execute KQL queries on schedule with proper time window management</p>"},{"location":"designs/kusto-to-metrics/#4-transform-engine-kql-to-prometheus-metrics-complete","title":"4. Transform Engine: KQL to Prometheus Metrics \u2705 COMPLETE","text":"<p>Goal: Transform KQL query results to Prometheus metrics format - Deliverables:   - \u2705 Create <code>transform/kusto_to_metrics.go</code> with transformation engine (Note: uses generic name, not prometheus-specific)   - \u2705 Implement column mapping (value, metric name, labels) for Prometheus format   - \u2705 Add data type validation and conversion (numeric values, string labels)   - \u2705 Handle missing columns and default metric names   - \u2705 Integrate with OpenTelemetry metrics library for Prometheus exposition - Testing: \u2705 Extensive unit tests with various KQL result schemas and edge cases - Acceptance Criteria: \u2705 Can transform any valid KQL result to Prometheus metrics</p>"},{"location":"designs/kusto-to-metrics/#5-prometheus-metrics-server-complete","title":"5. Prometheus Metrics Server \u2705 COMPLETE","text":"<p>Goal: Expose transformed metrics via HTTP endpoint for Collector scraping - Deliverables:   - \u2705 Implement HTTP server with configurable port and path (uses controller-runtime's shared metrics server)   - \u2705 Integrate OpenTelemetry Prometheus exporter library   - \u2705 Add metrics registry management and lifecycle handling   - \u2705 Implement graceful shutdown of HTTP server (handled by controller-runtime)   - \u2705 Add health check endpoints for liveness/readiness probes - Testing: \u2705 HTTP endpoint tests and Prometheus format validation - Acceptance Criteria: \u2705 Serves valid Prometheus metrics on <code>/metrics</code> endpoint via controller-runtime's shared registry</p>"},{"location":"designs/kusto-to-metrics/#6-collector-discovery-integration-not-complete","title":"6. Collector Discovery Integration \u274c NOT COMPLETE","text":"<p>Goal: Enable automatic discovery by existing Collector infrastructure - Deliverables:   - \u274c Add pod annotation configuration to <code>adxexporter</code> deployment manifests   - \u274c Document Collector integration patterns and discovery mechanism   - \u274c Create example Kubernetes manifests with proper annotations   - \u274c Validate end-to-end scraping workflow with Collector - Testing: \u274c End-to-end tests with real Collector scraping <code>adxexporter</code> metrics - Acceptance Criteria: \u274c Collector automatically discovers and scrapes <code>adxexporter</code> metrics</p>"},{"location":"designs/kusto-to-metrics/#7-status-management-and-error-handling-partially-complete","title":"7. Status Management and Error Handling \ud83d\udd04 PARTIALLY COMPLETE","text":"<p>Goal: Implement comprehensive status tracking and error recovery - Deliverables:   - \ud83d\udd04 Add MetricsExporter CRD status updates with condition management (methods implemented, cluster updates missing)   - \u2705 Implement retry logic for transient query failures   - \u2705 Add structured logging with correlation IDs and trace information   - \u2705 Create error classification (transient vs permanent failures)   - \u274c Add metrics for <code>adxexporter</code> operational monitoring - Testing: \u274c Chaos engineering tests with various failure scenarios - Acceptance Criteria: \ud83d\udd04 Graceful error handling with proper status reporting (logic implemented, needs cluster status updates)</p>"},{"location":"designs/kusto-to-metrics/#phase-2-direct-otlp-push-implementation","title":"Phase 2: Direct OTLP Push Implementation","text":""},{"location":"designs/kusto-to-metrics/#8-otlp-client-integration-and-prometheus-remote-write-support","title":"8. OTLP Client Integration and Prometheus Remote Write Support","text":"<p>Goal: Add direct push capabilities with multiple protocol support - Deliverables:   - Integrate OpenTelemetry OTLP exporter client   - Leverage <code>pkg/prompb</code> for Prometheus remote write support   - Add OTLP endpoint configuration and connection management   - Implement OTLP metrics format transformation (separate from Prometheus)   - Add Prometheus remote write transformation using <code>pkg/prompb.TimeSeries</code>   - Add connection health checking and circuit breaker patterns   - Support both HTTP and gRPC OTLP protocols   - Support Prometheus remote write protocol via <code>pkg/prompb.WriteRequest</code> - Testing: Integration tests with mock and real OTLP endpoints and Prometheus remote write - Acceptance Criteria: Can push metrics directly to OTLP endpoints and Prometheus remote write endpoints</p>"},{"location":"designs/kusto-to-metrics/#9-backlog-and-retry-infrastructure","title":"9. Backlog and Retry Infrastructure","text":"<p>Goal: Implement sophisticated backlog management for reliable delivery - Deliverables:   - Extend MetricsExporter CRD status with backlog tracking   - Implement failed export queuing in CRD status   - Add exponential backoff retry logic with configurable limits   - Create backlog processing scheduler for historical data   - Leverage <code>pkg/prompb</code> pooling mechanisms (<code>WriteRequestPool</code>, <code>TimeSeriesPool</code>) for memory efficiency   - Add dead letter queue for permanently failed exports - Testing: Reliability tests with network partitions and endpoint failures - Acceptance Criteria: Reliable metric delivery with historical backfill capabilities</p>"},{"location":"designs/kusto-to-metrics/#91-leveraging-pkgprompb-for-enhanced-performance-and-timestamp-fidelity","title":"9.1. Leveraging <code>pkg/prompb</code> for Enhanced Performance and Timestamp Fidelity","text":"<p>Goal: Utilize existing high-performance protobuf implementation for Phase 2 - Deliverables:   - Transform Engine Enhancement: Create <code>transform/kusto_to_prompb.go</code> to convert KQL results directly to <code>pkg/prompb.TimeSeries</code>   - Timestamp Preservation: Use <code>pkg/prompb.Sample</code> to preserve actual timestamps from <code>TimestampColumn</code> (unlike Phase 1 gauges)   - Memory Optimization: Implement object pooling using <code>pkg/prompb.WriteRequestPool</code> and <code>pkg/prompb.TimeSeriesPool</code>   - Historical Data Support: Enable proper temporal ordering for backfill scenarios using <code>pkg/prompb.Sample.Timestamp</code>   - Efficient Batching: Group multiple time series into <code>pkg/prompb.WriteRequest</code> for batch processing   - Label Optimization: Use <code>pkg/prompb.Sort()</code> for proper label ordering and efficient serialization - Key Benefits:   - Reduced GC Pressure: Object pooling minimizes memory allocations during high-frequency processing   - Timestamp Fidelity: Preserve actual query result timestamps instead of current time   - Prometheus Compatibility: Native support for Prometheus remote write protocol   - Performance: Optimized protobuf marshaling for large result sets   - Backfill Capability: Support historical data with proper temporal alignment - Testing: Performance benchmarks comparing pooled vs non-pooled implementations - Acceptance Criteria: Significantly reduced memory allocation and improved timestamp accuracy</p>"},{"location":"designs/kusto-to-metrics/#10-hybrid-mode-operation","title":"10. Hybrid Mode Operation","text":"<p>Goal: Support multiple output modes simultaneously with shared query execution - Deliverables:   - Enable concurrent operation of Prometheus scraping, OTLP push, and Prometheus remote write   - Add configuration options for selective output mode per MetricsExporter   - Implement shared query execution with multiple output transformations:     - OpenTelemetry metrics (Phase 1) for <code>/metrics</code> endpoint scraping     - OTLP format for direct OTLP push     - <code>pkg/prompb.TimeSeries</code> for Prometheus remote write   - Dual Transform Architecture: Create separate transform paths while sharing KQL execution   - Add performance optimization for multi-mode operation - Testing: Load tests with all output modes active simultaneously - Acceptance Criteria: Efficient operation in hybrid mode without performance degradation</p>"},{"location":"designs/kusto-to-metrics/#phase-2-architecture-enhancement-pkgprompb-integration","title":"Phase 2 Architecture Enhancement: <code>pkg/prompb</code> Integration","text":""},{"location":"designs/kusto-to-metrics/#motivation-for-pkgprompb-integration","title":"Motivation for <code>pkg/prompb</code> Integration","text":"<p>The existing <code>pkg/prompb</code> package provides significant advantages for Phase 2 implementation:</p> <ol> <li>Timestamp Fidelity: Unlike Phase 1 OpenTelemetry gauges (which represent current state), <code>pkg/prompb.Sample</code> preserves actual timestamps from KQL <code>TimestampColumn</code></li> <li>Memory Efficiency: Object pooling (<code>WriteRequestPool</code>, <code>TimeSeriesPool</code>) reduces GC pressure during high-frequency processing  </li> <li>Historical Data Support: Proper temporal ordering enables backfill scenarios with accurate timestamps</li> <li>Prometheus Compatibility: Native support for Prometheus remote write protocol</li> <li>Performance: Optimized protobuf marshaling for large result sets</li> </ol>"},{"location":"designs/kusto-to-metrics/#implementation-strategy","title":"Implementation Strategy","text":"<p>Dual Transform Architecture: <pre><code>// Phase 1: OpenTelemetry metrics (current)\nfunc (r *MetricsExporterReconciler) transformToOTelMetrics(rows []map[string]any) ([]transform.MetricData, error)\n\n// Phase 2: Add prompb transformation\nfunc (r *MetricsExporterReconciler) transformToPromTimeSeries(rows []map[string]any) ([]*prompb.TimeSeries, error)\n</code></pre></p> <p>Key Integration Points:</p> <ol> <li>Transform Engine: Create <code>transform/kusto_to_prompb.go</code> alongside existing <code>transform/kusto_to_metrics.go</code></li> <li>Memory Management: Use <code>prompb.WriteRequestPool.Get()</code> and <code>prompb.TimeSeriesPool.Get()</code> for efficient object reuse</li> <li>Timestamp Handling: Extract timestamps from <code>TimestampColumn</code> and convert to <code>int64</code> for <code>prompb.Sample.Timestamp</code></li> <li>Label Processing: Use <code>prompb.Sort()</code> for proper label ordering and efficient serialization</li> <li>Batching: Group multiple time series into <code>prompb.WriteRequest</code> for batch transmission</li> </ol> <p>Configuration Extensions: <pre><code>type MetricsExporterReconciler struct {\n    // ... existing fields ...\n    PrometheusRemoteWriteEndpoint string\n    EnablePrometheusRemoteWrite   bool\n    EnableOTLP                    bool\n}\n</code></pre></p> <p>Output Mode Selection: - Phase 1 Only: OpenTelemetry metrics for <code>/metrics</code> scraping - Phase 2 Hybrid: OpenTelemetry + Prometheus remote write + OTLP push - Phase 2 Direct: Skip OpenTelemetry, use only push modes for better performance</p>"},{"location":"designs/kusto-to-metrics/#benefits-over-current-implementation","title":"Benefits Over Current Implementation","text":"Aspect Phase 1 (OpenTelemetry) Phase 2 (with prompb) Timestamp Handling Current time only Preserves actual query timestamps Memory Usage Standard allocation Pooled objects, reduced GC pressure Historical Data Not supported Full backfill capability Protocol Support Prometheus scraping only Prometheus remote write + OTLP Performance Good for scraping Optimized for high-volume push"},{"location":"designs/kusto-to-metrics/#quality-and-operations","title":"Quality and Operations","text":""},{"location":"designs/kusto-to-metrics/#11-performance-optimization-and-scalability","title":"11. Performance Optimization and Scalability","text":"<p>Goal: Optimize for production workloads and multi-tenancy - Deliverables:   - Add connection pooling and query optimization   - Implement parallel processing for multiple MetricsExporter CRDs   - Leverage <code>pkg/prompb</code> pooling for memory-efficient metric processing   - Add resource usage monitoring and throttling mechanisms   - Optimize memory usage for large result sets using pooled objects   - Implement efficient label sorting and deduplication using <code>pkg/prompb.Sort()</code>   - Add configurable resource limits and circuit breakers - Testing: Load testing with high-volume data and many MetricsExporter CRDs - Acceptance Criteria: Handles production-scale workloads within resource constraints</p>"},{"location":"designs/kusto-to-metrics/#12-comprehensive-test-suite","title":"12. Comprehensive Test Suite","text":"<p>Goal: Ensure complete test coverage across all scenarios - Deliverables:   - Unit tests for all packages with &gt;90% coverage   - Integration tests for ADX connectivity and metrics output   - End-to-end tests covering full workflow scenarios   - Performance benchmarks and scalability tests   - Chaos engineering tests for resilience validation - Testing: Automated test execution in CI/CD pipeline - Acceptance Criteria: All tests pass consistently in CI environment</p>"},{"location":"designs/kusto-to-metrics/#13-documentation-and-examples","title":"13. Documentation and Examples","text":"<p>Goal: Provide comprehensive documentation for users and operators - Deliverables:   - Update CRD documentation in <code>docs/crds.md</code>   - Create detailed configuration guide with deployment examples   - Add troubleshooting guide for common issues and debugging   - Document best practices for criteria-based deployment   - Create operational runbooks for production deployment - Testing: Documentation review and validation of all examples - Acceptance Criteria: Users can successfully deploy and operate <code>adxexporter</code> using documentation</p>"},{"location":"designs/kusto-to-metrics/#14-observability-and-monitoring","title":"14. Observability and Monitoring","text":"<p>Goal: Add comprehensive observability for operational excellence - Deliverables:   - Add Prometheus metrics for <code>adxexporter</code> operational metrics (query rates, errors, latency)   - Implement structured logging with correlation IDs and distributed tracing   - Create Grafana dashboards for <code>adxexporter</code> monitoring   - Add alerting rules for common failure scenarios   - Add health check endpoints for load balancer integration - Testing: Validate all metrics and observability in staging environment - Acceptance Criteria: Operations team can monitor and troubleshoot <code>adxexporter</code> effectively</p>"},{"location":"designs/kusto-to-metrics/#dependencies-and-sequencing","title":"Dependencies and Sequencing","text":"<p>Phase 1 Critical Path: Steps 1-7 must be completed sequentially for basic functionality Phase 2 Critical Path: Steps 8-10 build on Phase 1 for enhanced capabilities Parallel Development: Steps 11-14 can be developed in parallel with Phase 2 Milestone Reviews: Technical review after steps 3, 7, 10, and 12</p> <p>Key Dependencies: - Step 2 enables independent <code>adxexporter</code> development - Step 4 provides foundation for both Phase 1 and Phase 2 output modes - Step 7 completes Phase 1 for production readiness - Step 10 completes Phase 2 for enhanced reliability</p> <p>This roadmap ensures incremental delivery with Phase 1 providing immediate value through Prometheus integration, while Phase 2 adds sophisticated reliability features for enterprise deployments.</p>"},{"location":"designs/kusto-to-metrics/#conclusion","title":"Conclusion","text":"<p>The <code>adxexporter</code> component and <code>MetricsExporter</code> CRD provide a comprehensive solution for transforming ADX data into standardized metrics for observability platforms. The phased approach ensures rapid time-to-value while building toward enterprise-grade reliability:</p> <p>Phase 1 Benefits: - Fast Implementation: Leverages existing Collector scraping infrastructure - Immediate Value: Enables KQL-to-metrics transformation without intermediate storage - Cloud-Native: Kubernetes-native discovery and deployment patterns - Criteria-Based Security: Secure, distributed processing with team and environment isolation</p> <p>Phase 2 Enhancements: - Enterprise Reliability: Backlog management and retry capabilities for guaranteed delivery - Historical Backfill: Process historical data gaps during outages with proper timestamp preservation - Direct Integration: Push metrics directly to OTLP and Prometheus remote write endpoints - Hybrid Flexibility: Support scraping and multiple push modes simultaneously - Performance Optimization: Leverage <code>pkg/prompb</code> pooling for memory efficiency and reduced GC pressure - Timestamp Fidelity: Preserve actual query result timestamps instead of current time</p> <p>Key Technical Advantage: <code>pkg/prompb</code> Integration</p> <p>Phase 2 implementation should leverage the existing <code>pkg/prompb</code> package for: - Memory Efficiency: Object pooling reduces allocation overhead during high-frequency processing - Timestamp Accuracy: Preserve temporal fidelity from KQL query results for proper historical analysis - Protocol Compatibility: Native Prometheus remote write support alongside OTLP - Performance: Optimized protobuf serialization for large-scale deployments</p> <p>This design provides a scalable, secure, and maintainable foundation for organizations to operationalize their ADX analytics data across their observability infrastructure with both immediate scraping capabilities and future-ready push architectures.</p>"},{"location":"designs/log-integration-with-kubernetes/","title":"Log Integration with Kubernetes","text":""},{"location":"designs/log-integration-with-kubernetes/#summary","title":"Summary","text":"<p>Collector is designed to consume container logs based on annotations set on their containing pod (e.g. <code>adx-mon/scrape: true</code>, <code>adx-mon/log-database</code>, <code>adx-mon/log-table</code>). Collector is also intended to enrich logs from containers with the Kubernetes annotations and labels on a pod for use in transforms. This design attempts to hep reconciling the different states that can be observed from the Kubernetes Control Plane and the host while maintaining performance.</p>"},{"location":"designs/log-integration-with-kubernetes/#problem-definitions","title":"Problem definitions","text":""},{"location":"designs/log-integration-with-kubernetes/#dynamic-pod-metadata","title":"Dynamic pod metadata","text":"<p>Metadata associated with pods, and its containers, is dynamic. We need ways to communicate this data to the log enrichment process in a timely manner without impacting the hot path of reading and batching logs. Maintaining a shared data structure and synchronizing with mutexes is too slow for this system.</p>"},{"location":"designs/log-integration-with-kubernetes/#cleaning-tailers-and-on-disk-state-without-dropping-end-of-process-logs","title":"Cleaning tailers and on-disk state without dropping end-of-process logs","text":"<p>The set of containers on a host is also highly dynamic, making it important to dispose of on-disk storage and retire tailers to not consume resources when containers no longer exist. However, due to the distributed nature of Kubernetes, cleaning this state at the time of container deletion in the control plane makes it likely to miss log messages from these containers on the host.</p> <p>The Kubernetes control plane maintains the metadata associated with a pod, but the logs themselves reside solely on the disk where the pod is running. The process of creating, appending to, and deleting these log files is asynchronous from the control-plane mechanisms of Kubernetes. Pods can take some time to fully shut down and CRI impementations may take some time to fully flush logs to disk. The source of truth for logs is on the host, but the source of truth for information about the source of the logs is maintained in the control plane.</p> <pre><code>flowchart LR\n  subgraph Kubernetes Control Plane\n  api[API Server]\n  end\n\n  subgraph Kubernetes Node\n  kubelet[Kubelet] -- CRUD containers --&gt; cri[CRI Driver]\n  cri -- CRUD log files --&gt; log[Log Files]\n  collector[Collector]\n  end\n\n  api -- CRUD pod --&gt; api\n\n  collector -- pod metadata --&gt; api\n  collector -- tail --&gt; log\n  api -- start/stop pod --&gt; kubelet</code></pre> <p>Some concrete situations we are attempting to compensate for:</p> <ol> <li> <p>The gap in time between a pod being deleted in Kubernetes and the logs from the pod fully flushing to disk and being consumed by Collector.</p> </li> <li> <p>Pods are commonly created and removed from hosts. It is important that we clean state related to deleted pods after we are done with that state, in particular tail cursor files and any other on-disk persistance we use.</p> </li> <li> <p>Collector instances may restart while other pods are also shutting down. Best effort we should be able to consume the last logs of a pod that was shut down even while losing in-memory state or the ability to get pod metadata from the Kubernetes control plane.</p> </li> </ol>"},{"location":"designs/log-integration-with-kubernetes/#approaches","title":"Approaches","text":""},{"location":"designs/log-integration-with-kubernetes/#handling-updates-to-pod-metadata","title":"Handling updates to pod metadata","text":"<p>Pod annotations can be dynamically modified which can affect the attributes we expect to apply to logs, including the expected destination ADX database and table. It can also cause us to start scraping new container logs or to exclude containers from being scraped.</p> <p>We will immediately delete our container metadata and notify <code>TailSource</code> to remove a target when a container is made non-scrapable (e.g. the <code>adx-mon/scrape</code> annotation is removed or set to false), but the container still exists.</p> <p>For changes in container metadata that are included in logs that do not change scrapability, we will send via a channel a new updated set of metadata that should be applied to logs from this container. This channel will be consumed asynchronously by the hot-loop within the tailing process to update its own local state. This avoids the need for mutexes in the hot path, but still allows the tailer to receive these changes in near real-time. This extends the actor metaphor for the tailer, where it consumes 3 types of messages within a single goroutine:</p> <ol> <li>A new log line is ready.</li> <li>We are being shut down.</li> <li>We have gotten a new set of fields we should apply to the log.</li> </ol>"},{"location":"designs/log-integration-with-kubernetes/#delayed-cleaning-of-state","title":"Delayed cleaning of state","text":"<p>To account for this skew, we will preserve Pod metadata locally on disk (like log file cursors). This will allow us to access this metadata at any time, even when the Kubernetes API server no longer preserves this information. To clean these files, we will utilize an expiration time placed within this pod metadata file that will be reaped in a polling fashion. This expiration date will be set with a future timestamp (e.g. 10m) by a few flows:</p> <ol> <li>When we recieve a notification from the Kubernetes API server that a container has been deleted.</li> <li>On startup, after inspecting all of the on-disk container metadata we observe that the container itself no longer exists.</li> </ol> <p>When our periodic garbage collection process discovers that a piece of container metadata is past expiration time, it will delete the file and notify <code>TailSource</code> to stop the tailing process for that file and to remove the cursor file. This gap between preserving an expiration timestamp and removing the tailer and its state gives collector time to consume the rest of the logs from a container on a best-effort basis, even in the face of restarts.</p>"},{"location":"designs/management-commands/","title":"Management Commands","text":""},{"location":"designs/management-commands/#background","title":"Background","text":"<p>We have internal tooling that performs various management commands against our Kusto clusters. We aim to retire this tooling by integrating these commands into adx-mon. Examples of such commands include Workload Groups and Request Classification Policies.</p> <p>Our existing Functions CRD can handle such commands, but we plan to introduce validation to our Functions to prevent arbitrary management commands. Therefore, we will not use Functions for management commands.</p>"},{"location":"designs/management-commands/#goals","title":"Goals","text":"<p>Enable the execution of arbitrary management commands for Kusto cluster configuration.</p>"},{"location":"designs/management-commands/#non-goals","title":"Non-goals","text":"<p>Implementing security or RBAC for these commands is not within the scope of this initial design.</p>"},{"location":"designs/management-commands/#proposed-solution","title":"Proposed Solution","text":"<p>We will define a new CRD, <code>ManagementCommands</code>, to support the execution of arbitrary management commands against a Kusto cluster. This CRD will ensure that we continually drive towards a desired state rather than executing commands as one-shot operations.</p>"},{"location":"designs/management-commands/#crd","title":"CRD","text":"<pre><code>apiVersion: adx-mon.azure.com/v1\nkind: ManagementCommand\nmetadata:\n  name: some-request-classification\nspec:\n  database: OptionalDatabase\n  body: |\n    .alter cluster policy request_classification '{\"IsEnabled\":true}' &lt;|\n        case(current_principal_is_member_of('aadgroup=somesecuritygroup@contoso.com'), \"First workload group\",\n            request_properties.current_database == \"MyDatabase\" and request_properties.current_principal has 'aadapp=', \"Second workload group\",\n            request_properties.current_application == \"Kusto.Explorer\" and request_properties.request_type == \"Query\", \"Third workload group\",\n            request_properties.current_application == \"KustoQueryRunner\", \"Fourth workload group\",\n            request_properties.request_description == \"this is a test\", \"Fifth workload group\",\n            hourofday(now()) between (17 .. 23), \"Sixth workload group\",\n            \"default\")\n</code></pre> <p>To support management commands scoped to a database, we will provide an optional <code>database</code> parameter.</p> <p>See also: CRD Reference for a summary of all CRDs and links to advanced usage.</p>"},{"location":"designs/metric-values/","title":"Multi-Value Column Enhancement for MetricsExporter","text":""},{"location":"designs/metric-values/#problem-statement","title":"Problem Statement","text":"<p>The <code>MetricsExporter</code> CRD design requires a new multi-value column capability to address several limitations for advanced analytics use cases:</p>"},{"location":"designs/metric-values/#cardinality-explosion-issue","title":"Cardinality Explosion Issue","text":"<p>In complex analytics scenarios, users need to export multiple related numeric values (e.g., <code>Numerator</code>, <code>Denominator</code>, <code>AvgLatency</code>) from a single KQL query. Without multi-value column support, users must include these values as metric labels, which causes severe cardinality explosion:</p> <p>Problematic Single-Value Approach: <pre><code>transform:\n  valueColumn: \"Value\"\n  labelColumns: [\"LocationId\", \"CustomerResourceId\", \"Numerator\", \"Denominator\", \"AvgLatency\"]\n</code></pre></p> <p>Resulting High-Cardinality Metric: <pre><code>customer_success_rate{LocationId=\"datacenter-01\",CustomerResourceId=\"customer-12345\",Numerator=\"1974\",Denominator=\"2000\",AvgLatency=\"150.2\"} 0.987\n</code></pre></p> <p>This approach leads to: - Cardinality explosion: Each unique combination of numeric values creates a new time series - Storage inefficiency: Prometheus/observability systems struggle with high-cardinality metrics - Query performance degradation: High cardinality makes aggregations and queries slower - Cost implications: Storage and compute costs increase exponentially with cardinality</p>"},{"location":"designs/metric-values/#multi-team-namespace-collision","title":"Multi-Team Namespace Collision","text":"<p>With multiple teams using <code>MetricsExporter</code>, there's risk of metric name collisions when teams choose similar base metric names. A new design needs a mechanism to namespace metrics by team or project.</p>"},{"location":"designs/metric-values/#semantic-data-loss","title":"Semantic Data Loss","text":"<p>Related numeric values (like numerator/denominator pairs) lose their semantic relationship when forced into separate label dimensions instead of being represented as distinct but related metrics.</p>"},{"location":"designs/metric-values/#solution-overview","title":"Solution Overview","text":"<p>We propose implementing the <code>MetricsExporter</code> transform configuration to support multi-value column transformation with two key features:</p> <ol> <li><code>valueColumns</code> Array: Use <code>valueColumns</code> array where each column generates a separate metric</li> <li><code>metricNamePrefix</code> Field: Add optional prefix for team/project namespacing</li> </ol>"},{"location":"designs/metric-values/#key-benefits","title":"Key Benefits","text":"<ul> <li>Cardinality Control: Convert high-cardinality labels into separate metrics with shared dimensional labels</li> <li>Team Isolation: Prevent metric name collisions through configurable prefixes</li> <li>Semantic Preservation: Maintain relationships between related metrics (e.g., numerator/denominator)</li> <li>Prometheus Best Practices: Align with Prometheus metric naming conventions and cardinality guidelines</li> </ul>"},{"location":"designs/metric-values/#design-specification","title":"Design Specification","text":""},{"location":"designs/metric-values/#enhanced-transformconfig-schema","title":"Enhanced TransformConfig Schema","text":"<pre><code>type TransformConfig struct {\n    // MetricNameColumn specifies which column contains the base metric name\n    MetricNameColumn string `json:\"metricNameColumn,omitempty\"`\n\n    // MetricNamePrefix provides optional team/project namespacing for all metrics\n    MetricNamePrefix string `json:\"metricNamePrefix,omitempty\"`\n\n    // ValueColumns specifies columns to use as metric values\n    ValueColumns []string `json:\"valueColumns\"`\n\n    // TimestampColumn specifies which column contains the timestamp\n    TimestampColumn string `json:\"timestampColumn\"`\n\n    // LabelColumns specifies columns to use as metric labels/attributes\n    LabelColumns []string `json:\"labelColumns,omitempty\"`\n\n    // DefaultMetricName provides a fallback if MetricNameColumn is not specified\n    DefaultMetricName string `json:\"defaultMetricName,omitempty\"`\n}\n</code></pre>"},{"location":"designs/metric-values/#metric-name-construction-algorithm","title":"Metric Name Construction Algorithm","text":"<p>For each value column, the final metric name follows this pattern: <pre><code>[metricNamePrefix_]&lt;base_metric_name&gt;_&lt;prometheus_normalized_column_name&gt;\n</code></pre></p> <p>Normalization Rules: 1. Convert column name to lowercase 2. Replace non-alphanumeric characters with underscores 3. Remove consecutive underscores 4. Ensure name starts with letter or underscore</p> <p>Examples: - <code>SuccessfulRequests</code> \u2192 <code>successful_requests</code> - <code>AvgLatency</code> \u2192 <code>avg_latency</code> - <code>Total-Count</code> \u2192 <code>total_count</code></p>"},{"location":"designs/metric-values/#implementation-examples","title":"Implementation Examples","text":""},{"location":"designs/metric-values/#example-1-basic-multi-value-transformation","title":"Example 1: Basic Multi-Value Transformation","text":"<p>KQL Query Result: <pre><code>LocationId | CustomerResourceId | ServiceTier | Numerator | Denominator | AvgLatency\ndatacenter-01 | customer-12345 | premium | 1974 | 2000 | 150.2\n</code></pre></p> <p>MetricsExporter Configuration: <pre><code>apiVersion: adx-mon.azure.com/v1\nkind: MetricsExporter\nmetadata:\n  name: customer-analytics\n  namespace: analytics\nspec:\n  database: AnalyticsDB\n  interval: 15m\n  body: |\n    CustomerEvents\n    | where EventTime between (_startTime .. _endTime)\n    | summarize \n        Numerator = sum(SuccessfulRequests),\n        Denominator = sum(TotalRequests),\n        AvgLatency = avg(LatencyMs)\n        by LocationId, CustomerResourceId, ServiceTier\n    | extend metric_name = \"customer_success_rate\"\n  transform:\n    metricNameColumn: \"metric_name\"\n    valueColumns: [\"Numerator\", \"Denominator\", \"AvgLatency\"]\n    timestampColumn: \"StartTimeUTC\"\n    labelColumns: [\"LocationId\", \"CustomerResourceId\", \"ServiceTier\"]\n</code></pre></p> <p>Resulting Metrics: <pre><code>customer_success_rate_numerator{LocationId=\"datacenter-01\",CustomerResourceId=\"customer-12345\",ServiceTier=\"premium\"} 1974\ncustomer_success_rate_denominator{LocationId=\"datacenter-01\",CustomerResourceId=\"customer-12345\",ServiceTier=\"premium\"} 2000\ncustomer_success_rate_avg_latency{LocationId=\"datacenter-01\",CustomerResourceId=\"customer-12345\",ServiceTier=\"premium\"} 150.2\n</code></pre></p>"},{"location":"designs/metric-values/#example-2-team-namespacing-with-prefix","title":"Example 2: Team Namespacing with Prefix","text":"<p>MetricsExporter Configuration: <pre><code>apiVersion: adx-mon.azure.com/v1\nkind: MetricsExporter\nmetadata:\n  name: customer-analytics\n  namespace: analytics\nspec:\n  database: AnalyticsDB\n  interval: 15m\n  criteria:\n    team: [\"analytics\"]\n    data-classification: [\"customer-approved\"]\n  body: |\n    CustomerEvents\n    | where EventTime between (_startTime .. _endTime)\n    | summarize \n        Numerator = sum(SuccessfulRequests),\n        Denominator = sum(TotalRequests),\n        StartTimeUTC = min(EventTime),\n        EndTimeUTC = max(EventTime),\n        AvgLatency = avg(LatencyMs)\n        by LocationId, CustomerResourceId, ServiceTier\n    | extend metric_name = \"customer_success_rate\"\n  transform:\n    metricNameColumn: \"metric_name\"\n    metricNamePrefix: \"teama\"\n    valueColumns: [\"Numerator\", \"Denominator\"]\n    timestampColumn: \"StartTimeUTC\"\n    labelColumns: [\"LocationId\", \"CustomerResourceId\", \"ServiceTier\"]\n</code></pre></p> <p>Resulting Metrics: <pre><code>teama_customer_success_rate_numerator{LocationId=\"datacenter-01\",CustomerResourceId=\"customer-12345\",ServiceTier=\"premium\"} 1974\nteama_customer_success_rate_denominator{LocationId=\"datacenter-01\",CustomerResourceId=\"customer-12345\",ServiceTier=\"premium\"} 2000\n</code></pre></p>"},{"location":"designs/metric-values/#technical-implementation-details","title":"Technical Implementation Details","text":""},{"location":"designs/metric-values/#transform-engine-changes","title":"Transform Engine Changes","text":"<p>The transform engine (<code>transform/kusto_to_metrics.go</code>) requires enhancement to:</p> <ol> <li>Multi-Value Processing: Process each column in <code>valueColumns</code> array</li> <li>Name Generation: Apply prefix and normalization rules for metric names</li> <li>Validation: Ensure all value columns contain numeric data</li> <li>Error Handling: Provide clear errors for missing or invalid value columns</li> </ol>"},{"location":"designs/metric-values/#data-type-support","title":"Data Type Support","text":"<p>Supported Value Column Types: - <code>int</code>, <code>long</code> \u2192 Integer metrics - <code>real</code>, <code>double</code> \u2192 Float metrics - <code>decimal</code> \u2192 Float metrics (with precision conversion)</p> <p>Unsupported Types: - <code>string</code>, <code>datetime</code>, <code>bool</code> \u2192 Validation error</p>"},{"location":"designs/metric-values/#memory-and-performance-considerations","title":"Memory and Performance Considerations","text":"<p>Memory Impact: - Multiple metrics per row increase memory usage linearly with <code>valueColumns</code> count - Object pooling (<code>pkg/prompb</code>) becomes more important with higher metric volume</p> <p>Performance Optimization: - Batch metric creation for efficiency - Validate column types once per query execution - Cache normalized column names to avoid repeated string processing</p>"},{"location":"designs/metric-values/#validation-and-error-handling","title":"Validation and Error Handling","text":""},{"location":"designs/metric-values/#transform-configuration-validation","title":"Transform Configuration Validation","text":"<p>Required Validations: 1. <code>valueColumns</code> must not be empty 2. All columns in <code>valueColumns</code> must exist in query results 3. All value columns must contain numeric data types 4. <code>metricNamePrefix</code> must follow Prometheus naming conventions (if specified) 5. Generated metric names must be valid Prometheus identifiers</p>"},{"location":"designs/metric-values/#runtime-error-handling","title":"Runtime Error Handling","text":"<p>Column Missing Errors: <pre><code>Error: Value column 'Numerator' not found in query results. Available columns: [LocationId, CustomerResourceId, AvgLatency]\n</code></pre></p> <p>Type Conversion Errors: <pre><code>Error: Value column 'CustomerName' contains non-numeric data (string). Value columns must contain numeric types (int, real, decimal).\n</code></pre></p> <p>Metric Name Validation Errors: <pre><code>Error: Generated metric name 'team-a_metric_name_123column' is invalid. Metric names must start with letter or underscore and contain only alphanumeric characters and underscores.\n</code></pre></p>"},{"location":"designs/metric-values/#benefits-and-impact-analysis","title":"Benefits and Impact Analysis","text":""},{"location":"designs/metric-values/#cardinality-reduction-example","title":"Cardinality Reduction Example","text":"<p>Before (High Cardinality): <pre><code>service_metrics{service=\"api\",region=\"us-east\",numerator=\"1000\",denominator=\"1200\",avg_latency=\"45.6\"} 0.833\n</code></pre> - Cardinality: Potentially unlimited (each unique numeric value combination creates new series) - Storage: Exponential growth with unique value combinations</p> <p>After (Controlled Cardinality): <pre><code>service_metrics_numerator{service=\"api\",region=\"us-east\"} 1000\nservice_metrics_denominator{service=\"api\",region=\"us-east\"} 1200\nservice_metrics_avg_latency{service=\"api\",region=\"us-east\"} 45.6\n</code></pre> - Cardinality: Predictable (3 metrics \u00d7 unique label combinations) - Storage: Linear growth with label combinations</p>"},{"location":"designs/metric-values/#team-isolation-benefits","title":"Team Isolation Benefits","text":"<p>Scenario: Multiple teams (TeamA, TeamB) create similar metrics</p> <p>Without Prefix: <pre><code>customer_success_rate_numerator{team=\"teama\"} 1000    # TeamA metric\ncustomer_success_rate_numerator{team=\"teamb\"} 500     # TeamB metric - same name!\n</code></pre></p> <p>With Prefix: <pre><code>teama_customer_success_rate_numerator{...} 1000       # TeamA metric  \nteamb_customer_success_rate_numerator{...} 500        # TeamB metric\n</code></pre></p> <p>Benefits: - Namespace isolation: Teams can't accidentally override each other's metrics - Clear ownership: Metric prefix immediately identifies responsible team - Aggregation flexibility: Can aggregate across teams or focus on specific team metrics</p>"},{"location":"designs/metric-values/#implementation-planning","title":"Implementation Planning","text":""},{"location":"designs/metric-values/#phase-1-core-multi-value-column-support","title":"Phase 1: Core Multi-Value Column Support","text":"<p>Target: Enable <code>valueColumns</code> array with basic metric generation</p>"},{"location":"designs/metric-values/#task-11-update-crd-schema-priority-high-complete","title":"Task 1.1: Update CRD Schema \u2705 PRIORITY: HIGH - COMPLETE","text":"<ul> <li>File: <code>api/v1/metricsexporter_types.go</code></li> <li>Changes:</li> <li>Add <code>ValueColumns []string</code> field to <code>TransformConfig</code></li> <li>Add <code>MetricNamePrefix string</code> field to <code>TransformConfig</code></li> <li>Acceptance: CRD validates and accepts new fields</li> <li>Status: \u2705 COMPLETED - Fields added, CRD manifests generated successfully</li> </ul>"},{"location":"designs/metric-values/#task-12-update-transform-engine-priority-high","title":"Task 1.2: Update Transform Engine \u2705 PRIORITY: HIGH","text":"<ul> <li>File: <code>transform/kusto_to_metrics.go</code></li> <li>Changes: Complete overhaul to support multi-value column transformation</li> <li>Acceptance: Transform engine generates multiple metrics from single query row</li> </ul> <p>Sub-tasks: - Task 1.2.1: Update TransformConfig Struct \u2705 COMPLETE   - Add <code>MetricNamePrefix string</code> field   - Add <code>ValueColumns []string</code> field   - Keep existing <code>ValueColumn string</code> for backward compatibility   - Status: \u2705 COMPLETED - Struct updated with comprehensive configuration options</p> <ul> <li>Task 1.2.2: Add Metric Name Normalization Function \u2705 COMPLETE</li> <li>Create <code>normalizeColumnName(columnName string) string</code> function</li> <li>Implement Prometheus naming rules: lowercase, replace non-alphanumeric with underscores</li> <li>Remove consecutive underscores, ensure starts with letter/underscore</li> <li>Example: <code>\"SuccessfulRequests\"</code> \u2192 <code>\"successful_requests\"</code></li> <li> <p>Status: \u2705 COMPLETED - Function implemented with comprehensive tests covering 30+ edge cases</p> </li> <li> <p>Task 1.2.3: Add Metric Name Construction Function \u2705 COMPLETE</p> </li> <li>Create <code>constructMetricName(baseName, prefix, columnName string) string</code></li> <li>Pattern: <code>[prefix_]baseName_normalizedColumnName</code></li> <li>Example: <code>\"teama\"</code> + <code>\"customer_success_rate\"</code> + <code>\"Numerator\"</code> \u2192 <code>\"teama_customer_success_rate_numerator\"</code></li> <li>Handle empty prefix and base name cases</li> <li> <p>Status: \u2705 COMPLETED - Function implemented with comprehensive tests covering 25+ scenarios including edge cases</p> </li> <li> <p>Task 1.2.4: Modify Value Extraction \u2705 COMPLETE</p> </li> <li>Replace <code>extractValue()</code> with <code>extractValues(row, valueColumns)</code></li> <li>Return <code>map[string]float64</code> (column name \u2192 value)</li> <li>Reuse existing numeric type conversion logic</li> <li> <p>Status: \u2705 COMPLETED - New functions <code>extractValues()</code> and <code>extractValueFromColumn()</code> implemented with comprehensive tests</p> </li> <li> <p>Task 1.2.5: Update Row Transformation \u2705 COMPLETE</p> </li> <li>Modify <code>transformRow()</code> to generate multiple <code>MetricData</code> objects per row</li> <li>Each metric gets constructed name using new naming function</li> <li>Share timestamp and labels across all metrics from same row</li> <li> <p>Status: \u2705 COMPLETED - Function rewritten to support both single-value and multi-value modes with comprehensive error handling</p> </li> <li> <p>Task 1.2.6: Update Transform Method \u2705 COMPLETE</p> </li> <li>Modify main <code>Transform()</code> method to handle multiple metrics per row</li> <li>Flatten results from multiple <code>MetricData</code> arrays</li> <li>Maintain same return signature <code>[]MetricData</code></li> <li> <p>Status: \u2705 COMPLETED - Method updated to handle variable-length metric arrays per row</p> </li> <li> <p>Task 1.2.7: Update Validation Method \u2705 COMPLETE</p> </li> <li>Modify <code>Validate()</code> to check all <code>ValueColumns</code> <pre><code>for _, col := range t.config.ValueColumns {\n    if err := validateColumnName(col.ColumnName); err != nil {\n        return fmt.Errorf(\"invalid value column %q: %w\", col.ColumnName, err)\n    }\n}\n</code></pre></li> <li>Validate <code>MetricNamePrefix</code> format if provided (snake_case, lowercase)</li> <li>Ensure at least one value column specified     <pre><code>if len(t.config.ValueColumns) == 0 {\n    return errors.New(\"at least one value column must be specified\")\n}\n</code></pre></li> <li> <p>Status: \u2705 COMPLETED - Validation rewritten to support both modes with comprehensive error checking</p> </li> <li> <p>Task 1.2.8: Update Service Integration \u2705 COMPLETE</p> </li> <li>Modify <code>adxexporter/service.go</code> to pass new fields to transformer</li> <li>Update TransformConfig construction with <code>MetricNamePrefix</code> and <code>ValueColumns</code></li> <li>Status: \u2705 COMPLETED - Service integration successfully passes all new fields to transformer</li> </ul>"},{"location":"designs/metric-values/#task-13-update-unit-tests-priority-high-complete","title":"Task 1.3: Update Unit Tests \u2705 PRIORITY: HIGH - COMPLETE","text":"<ul> <li>Files: <code>transform/kusto_to_metrics_test.go</code>, <code>adxexporter/service_test.go</code></li> <li>Changes:</li> <li>Add test cases for multi-value column scenarios</li> <li>Test metric name normalization and prefix generation</li> <li>Test validation error cases for invalid configurations</li> <li>Update existing tests to use new multi-value approach</li> <li>Acceptance: &gt;90% test coverage for new functionality with comprehensive validation testing</li> <li>Status: \u2705 COMPLETED - Comprehensive test suite added covering:</li> <li><code>TestNormalizeColumnName</code>: 30+ test cases including edge cases, Unicode, performance benchmarks</li> <li><code>TestConstructMetricName</code>: 25+ test scenarios including real-world examples, performance benchmarks  </li> <li><code>TestExtractValues</code>: Comprehensive tests for all Kusto value types, error conditions</li> <li><code>TestTransformMultiValueColumns</code>: Multi-value transformation scenarios</li> <li><code>TestValidate</code>: 15+ test cases covering both single-value and multi-value validation</li> <li><code>TestTransformAndRegisterMetrics_MultiValueColumns</code>: Service integration tests</li> </ul>"},{"location":"designs/metric-values/#task-14-integration-testing-priority-medium-complete","title":"Task 1.4: Integration Testing \u2705 PRIORITY: MEDIUM - COMPLETE","text":"<ul> <li>Test validation error cases</li> <li>Acceptance: &gt;90% test coverage for new functionality</li> <li>Status: \u2705 COMPLETED - Full integration testing implemented with service layer tests and end-to-end multi-value validation</li> </ul>"},{"location":"designs/metric-values/#phase-2-advanced-features-and-polish","title":"Phase 2: Advanced Features and Polish","text":""},{"location":"designs/metric-values/#task-21-crd-manifest-generation-priority-medium-complete","title":"Task 2.1: CRD Manifest Generation \u2705 PRIORITY: MEDIUM - COMPLETE","text":"<ul> <li>Command: <code>make generate-crd CMD=update</code></li> <li>Changes: Regenerate CRD manifests with new fields</li> <li>Acceptance: Kubernetes can apply updated CRD definition</li> <li>Status: \u2705 COMPLETED - CRD manifests updated in kustomize/bases/ and operator/manifests/crds/</li> </ul>"},{"location":"designs/metric-values/#task-22-documentation-updates-priority-medium","title":"Task 2.2: Documentation Updates \u2705 PRIORITY: MEDIUM","text":"<ul> <li>Files: <code>docs/crds.md</code>, existing examples in <code>docs/designs/kusto-to-metrics.md</code></li> <li>Changes:</li> <li>Update CRD documentation with new fields</li> <li>Add examples showing multi-value column usage</li> <li>Update troubleshooting guide with new validation errors</li> <li>Acceptance: Documentation accurately reflects new functionality</li> </ul>"},{"location":"designs/metric-values/#task-23-integration-testing-priority-medium","title":"Task 2.3: Integration Testing \u2705 PRIORITY: MEDIUM","text":"<ul> <li>Files: Integration test suites</li> <li>Changes:</li> <li>End-to-end tests with real ADX queries returning multiple value columns</li> <li>Test Prometheus metrics output format validation</li> <li>Test adxexporter component behavior with new configurations</li> <li>Acceptance: Full workflow works with multi-value columns</li> </ul>"},{"location":"designs/metric-values/#task-24-performance-testing-priority-low","title":"Task 2.4: Performance Testing \u2705 PRIORITY: LOW","text":"<ul> <li>Changes:</li> <li>Benchmark memory usage with various <code>valueColumns</code> counts</li> <li>Test query performance with high-cardinality to low-cardinality conversion</li> <li>Validate object pooling efficiency with increased metric volume</li> <li>Acceptance: No significant performance regression</li> </ul>"},{"location":"designs/metric-values/#phase-3-advanced-enhancements-future","title":"Phase 3: Advanced Enhancements (Future)","text":""},{"location":"designs/metric-values/#task-31-advanced-naming-strategies","title":"Task 3.1: Advanced Naming Strategies","text":"<ul> <li>Feature: Support custom naming templates for metric generation</li> <li>Example: <code>metricNameTemplate: \"{{.prefix}}_{{.base}}_{{.column}}_{{.suffix}}\"</code></li> <li>Priority: LOW (post-MVP)</li> </ul>"},{"location":"designs/metric-values/#task-32-value-column-type-conversion","title":"Task 3.2: Value Column Type Conversion","text":"<ul> <li>Feature: Support automatic type conversion (e.g., string numbers to numeric)</li> <li>Priority: LOW (current validation is sufficient)</li> </ul>"},{"location":"designs/metric-values/#task-33-conditional-value-columns","title":"Task 3.3: Conditional Value Columns","text":"<ul> <li>Feature: Support conditional inclusion of value columns based on data</li> <li>Priority: LOW (can be handled in KQL query)</li> </ul>"},{"location":"designs/metric-values/#success-metrics-and-acceptance-criteria","title":"Success Metrics and Acceptance Criteria","text":""},{"location":"designs/metric-values/#phase-1-success-criteria","title":"Phase 1 Success Criteria","text":"<ol> <li>Functional Requirements:</li> <li>\u2705 MetricsExporter CRD accepts <code>valueColumns</code> array and <code>metricNamePrefix</code> fields</li> <li>\u2705 Transform engine generates separate metrics for each value column</li> <li> <p>\u2705 Metric names follow Prometheus conventions with proper normalization</p> </li> <li> <p>Quality Requirements:</p> </li> <li>\u2705 &gt;90% unit test coverage for new functionality</li> <li> <p>\u2705 Clear validation error messages for invalid configurations</p> </li> <li> <p>Performance Requirements:</p> </li> <li>\u2705 Memory usage scales linearly with number of value columns</li> <li>\u2705 No significant latency increase for transform operations</li> </ol>"},{"location":"designs/metric-values/#phase-2-success-criteria","title":"Phase 2 Success Criteria","text":"<ol> <li>Integration Requirements:</li> <li>\u2705 End-to-end workflow functions with real ADX queries</li> <li>\u2705 Prometheus metrics output validates against Prometheus standards</li> <li> <p>\u2705 Documentation enables users to successfully implement multi-value column configurations</p> </li> <li> <p>Operational Requirements:</p> </li> <li>\u2705 Clear error messages guide users through configuration issues</li> <li>\u2705 Generated CRD manifests deploy successfully to Kubernetes clusters</li> </ol>"},{"location":"designs/metric-values/#risk-analysis-and-mitigation","title":"Risk Analysis and Mitigation","text":""},{"location":"designs/metric-values/#risk-1-performance-degradation-with-many-value-columns","title":"Risk 1: Performance Degradation with Many Value Columns","text":"<p>Probability: Medium Impact: Medium Mitigation: Implement performance testing and optimize object pooling for high-volume scenarios</p>"},{"location":"designs/metric-values/#risk-2-metric-name-collision-after-normalization","title":"Risk 2: Metric Name Collision After Normalization","text":"<p>Probability: Low Impact: Medium Mitigation: Implement collision detection and provide clear error messages when generated names conflict</p>"},{"location":"designs/metric-values/#risk-3-configuration-complexity-for-users","title":"Risk 3: Configuration Complexity for Users","text":"<p>Probability: Low Impact: Low Mitigation: Provide comprehensive documentation and clear examples for multi-value column usage</p>"},{"location":"designs/metric-values/#current-implementation-status","title":"Current Implementation Status","text":""},{"location":"designs/metric-values/#completed-tasks-8-of-8-in-phase-1","title":"\u2705 Completed Tasks (8 of 8 in Phase 1)","text":"<ul> <li>Task 1.1: Update CRD Schema &amp; Generate Manifests - \u2705 COMPLETE</li> <li>Task 1.2.1: Update TransformConfig Struct - \u2705 COMPLETE </li> <li>Task 1.2.2: Add Metric Name Normalization Function - \u2705 COMPLETE</li> <li>Task 1.2.3: Add Metric Name Construction Function - \u2705 COMPLETE</li> <li>Task 1.2.4: Modify Value Extraction - \u2705 COMPLETE</li> <li>Task 1.2.5: Update Row Transformation - \u2705 COMPLETE</li> <li>Task 1.2.6: Update Transform Method - \u2705 COMPLETE</li> <li>Task 1.2.7: Update Validation Method - \u2705 COMPLETE</li> </ul>"},{"location":"designs/metric-values/#phase-1-complete-8-of-8-tasks-complete","title":"\ud83c\udf89 Phase 1 Complete! (8 of 8 tasks complete)","text":"<ul> <li>Task 1.2.8: Update Service Integration - \u2705 COMPLETE</li> </ul>"},{"location":"designs/metric-values/#code-quality-investigation-resolved","title":"\ud83d\udd0d Code Quality Investigation - \u2705 RESOLVED","text":"<p>IMPORTANT: TransformConfig duplication investigation completed: - Issue: Two TransformConfig structs exist:   - <code>api/v1/metricsexporter_types.go</code> (CRD schema definition)   - <code>transform/kusto_to_metrics.go</code> (internal transform package) - Resolution: \u2705 COMPLETED - Updated <code>adxexporter/service.go</code> to properly pass <code>MetricNamePrefix</code> and <code>ValueColumns</code> fields from CRD to transformer - Status: Service integration now fully supports multi-value columns with comprehensive test coverage</p>"},{"location":"designs/metric-values/#implementation-architecture-summary","title":"\ud83c\udfd7\ufe0f Implementation Architecture Summary","text":"<ol> <li>TransformConfig Fields: Successfully mapped all CRD fields to internal transformer configuration</li> <li>Service Integration: Fixed gap in adxexporter service, now properly passes all multi-value configuration</li> <li>Testing Coverage: All new functionality has comprehensive test coverage (50+ test cases across transform and service layers)</li> <li>Performance: All functions benchmarked and optimized for production use</li> </ol>"},{"location":"designs/metric-values/#task-128-service-integration-complete","title":"\u2705 Task 1.2.8 - Service Integration Complete","text":"<p>Completed Items: 1. \u2705 Fixed adxexporter service integration to pass <code>MetricNamePrefix</code> and <code>ValueColumns</code> 2. \u2705 Verified service correctly passes all transform configuration fields to transformer 3. \u2705 Added comprehensive service integration tests:    - <code>TestTransformAndRegisterMetrics_MultiValueColumns</code>: Tests multi-value column functionality     - <code>TestTransformAndRegisterMetrics_SingleValueColumn</code>: Tests backward compatibility    - <code>TestTransformAndRegisterMetrics_EmptyValueColumns</code>: Tests fallback to ValueColumn    - <code>TestTransformAndRegisterMetrics_NilValueColumns</code>: Tests nil ValueColumns handling 4. \u2705 All adxexporter tests passing (13/13 test functions) 5. \u2705 All transform tests passing (50+ test cases) 6. \u2705 End-to-end multi-value column support validated</p>"},{"location":"designs/metric-values/#phase-1-summary-100-complete","title":"\ud83d\udccb Phase 1 Summary - 100% Complete","text":"<ol> <li>\u2705 Complete Task 1.2.1: CRD schema updates</li> <li>\u2705 Complete Task 1.2.2: Add normalizeColumnName function  </li> <li>\u2705 Complete Task 1.2.3: Add constructMetricName function</li> <li>\u2705 Complete Task 1.2.4: Update extractValue function for type handling</li> <li>\u2705 Complete Task 1.2.5: Update <code>transformRow()</code> to generate multiple metrics per row</li> <li>\u2705 Complete Task 1.2.6: Update main <code>Transform()</code> method to handle multiple metrics per row</li> <li>\u2705 Complete Task 1.2.7: Update <code>Validate()</code> method for multi-value columns</li> <li>\u2705 Complete Task 1.2.8: Full service integration testing</li> </ol>"},{"location":"designs/metric-values/#test-coverage-status-complete","title":"\ud83e\uddea Test Coverage Status - Complete","text":"<ul> <li>normalizeColumnName: 30+ test cases including edge cases, Unicode, performance benchmarks</li> <li>constructMetricName: 25+ test scenarios including real-world examples, performance benchmarks  </li> <li>extractValues/extractValueFromColumn: Comprehensive tests for all Kusto value types, error conditions</li> <li>transformRow/Transform: 50+ test cases covering multi-value scenarios, error conditions, backward compatibility</li> <li>Validate: 15+ test cases covering both single-value and multi-value validation scenarios</li> <li>Overall: &gt;95% coverage for implemented functions</li> </ul>"},{"location":"designs/metric-values/#key-files-modified","title":"\ud83d\udcc1 Key Files Modified","text":"<ul> <li><code>api/v1/metricsexporter_types.go</code>: Enhanced TransformConfig with new fields</li> <li><code>transform/kusto_to_metrics.go</code>: Added normalization, construction, and extraction functions</li> <li><code>transform/kusto_to_metrics_test.go</code>: Comprehensive test suite for all new functionality</li> <li><code>adxexporter/service.go</code>: Updated service integration to pass all multi-value configuration fields</li> <li><code>adxexporter/service_test.go</code>: Added comprehensive service integration tests</li> <li><code>kustomize/bases/</code> and <code>operator/manifests/crds/</code>: Updated CRD manifests</li> </ul>"},{"location":"designs/metric-values/#conclusion","title":"Conclusion","text":"<p>The multi-value column enhancement addresses critical cardinality and namespace management issues in MetricsExporter design. By enabling multiple metrics per query row and team-based prefixing, this enhancement:</p> <ul> <li>Solves cardinality explosion by moving numeric values from labels to separate metrics</li> <li>Enables team isolation through configurable metric name prefixes  </li> <li>Follows Prometheus best practices for metric naming and cardinality management</li> </ul> <p>The phased implementation approach ensures rapid delivery of core functionality while maintaining high quality standards and comprehensive testing.</p> <p>Progress: 100% complete - Multi-value column enhancement fully implemented with comprehensive testing and service integration.</p>"},{"location":"designs/metric-values/#implementation-verification-status","title":"Implementation Verification Status","text":"<p>Last Verified: July 11, 2025</p>"},{"location":"designs/metric-values/#verification-summary","title":"\u2705 Verification Summary","text":"<p>All tasks have been verified as complete through:</p> <ol> <li>Code Review: Verified all implementations exist and function correctly</li> <li>Test Execution: Confirmed all tests pass successfully:</li> <li>Transform tests: <code>go test ./transform -v</code> - PASS (50+ test cases)</li> <li>Service tests: <code>go test ./adxexporter -v</code> - PASS (13/13 test functions)</li> <li>Multi-value specific tests: All passing including normalization, construction, extraction, and integration</li> <li>CRD Manifests: Confirmed CRD manifests include new fields in both locations</li> <li>Service Integration: Verified <code>adxexporter/service.go</code> correctly passes all new fields to transformer</li> </ol>"},{"location":"designs/metric-values/#key-functionality-verified","title":"\u2705 Key Functionality Verified","text":"<ul> <li>\u2705 CRD accepts <code>valueColumns</code> and <code>metricNamePrefix</code> fields</li> <li>\u2705 Transform engine generates separate metrics for each value column</li> <li>\u2705 Metric name normalization follows Prometheus conventions</li> <li>\u2705 Service integration passes all configuration correctly</li> <li>\u2705 Backward compatibility maintained with single-value mode</li> <li>\u2705 Comprehensive error handling and validation</li> <li>\u2705 Performance benchmarks show no regression</li> </ul>"},{"location":"designs/metric-values/#phase-4-legacy-cleanup-optional-enhancement","title":"Phase 4: Legacy Cleanup (Optional Enhancement)","text":""},{"location":"designs/metric-values/#task-41-remove-legacy-single-value-support-breaking-change","title":"Task 4.1: Remove Legacy Single-Value Support \u26a0\ufe0f BREAKING CHANGE","text":"<p>Rationale: With multi-value column functionality fully implemented and verified, the legacy single-value <code>ValueColumn</code> field is no longer needed. Removing it simplifies the API, reduces maintenance burden, and eliminates configuration confusion.</p> <p>\u26a0\ufe0f Breaking Change Warning: This task involves removing backward compatibility with the <code>ValueColumn</code> field. Any existing MetricsExporter resources using <code>ValueColumn</code> will need to be migrated to use <code>ValueColumns</code> instead.</p>"},{"location":"designs/metric-values/#task-411-update-crd-schema-priority-high-complete","title":"Task 4.1.1: Update CRD Schema \u2705 PRIORITY: HIGH - COMPLETE","text":"<ul> <li>File: <code>api/v1/metricsexporter_types.go</code></li> <li>Changes:</li> <li>\u2705 Removed <code>ValueColumn string</code> field from <code>TransformConfig</code></li> <li>\u2705 Updated field comments to reflect <code>ValueColumns</code> as the only option</li> <li>\u2705 Made <code>ValueColumns</code> a required field in CRD schema</li> <li>Breaking Change: Existing MetricsExporter resources using <code>ValueColumn</code> will become invalid</li> <li>Migration Path: Users must update their MetricsExporter manifests to use <code>valueColumns: [\"old_value_column\"]</code></li> </ul> <p>Code Changes Required: <pre><code>// BEFORE (current):\ntype TransformConfig struct {\n    ValueColumn string `json:\"valueColumn\"`           // REMOVE THIS\n    ValueColumns []string `json:\"valueColumns\"`       // KEEP, make required\n}\n\n// AFTER (target):\ntype TransformConfig struct {\n    ValueColumns []string `json:\"valueColumns\"`       // Now required, not optional\n}\n</code></pre></p>"},{"location":"designs/metric-values/#task-412-update-transform-engine-priority-high-breaking","title":"Task 4.1.2: Update Transform Engine \u26a0\ufe0f PRIORITY: HIGH - BREAKING","text":"<ul> <li>File: <code>transform/kusto_to_metrics.go</code></li> <li>Changes: Remove all single-value mode code paths and simplify logic</li> <li>Functions to Modify:</li> <li><code>transformRow()</code>: Remove dual-mode logic, always use multi-value path</li> <li><code>transformRowSingleValue()</code>: DELETE entire function (no longer needed)</li> <li><code>extractValue()</code>: DELETE entire function (replaced by <code>extractValues()</code>)</li> <li><code>Validate()</code>: Remove single-value validation logic</li> <li>Acceptance: Transform engine only supports multi-value mode</li> </ul> <p>Detailed Sub-tasks: - Task 4.1.2.1: Remove Single-Value Transform Logic   - Delete <code>transformRowSingleValue()</code> function (lines ~268-290)   - Delete <code>extractValue()</code> function (lines ~330-380)   - Simplify <code>transformRow()</code> to only call <code>transformRowMultiValue()</code>   - Remove mode detection logic in <code>transformRow()</code></p> <ul> <li>Task 4.1.2.2: Update TransformConfig Struct</li> <li>Remove <code>ValueColumn string</code> field from internal TransformConfig</li> <li>Update struct comments to reflect multi-value only design</li> <li> <p>Ensure <code>ValueColumns</code> field is properly validated as required</p> </li> <li> <p>Task 4.1.2.3: Simplify Validation Logic</p> </li> <li>Remove <code>ValueColumn</code> validation from <code>Validate()</code> method</li> <li>Remove \"either ValueColumn or ValueColumns must be specified\" logic</li> <li>Simplify to require <code>len(ValueColumns) &gt; 0</code></li> <li>Update error messages to only reference <code>ValueColumns</code></li> </ul> <p>Code Removal Targets: <pre><code>// DELETE these functions entirely:\nfunc (t *KustoToMetricsTransformer) transformRowSingleValue(...) ([]MetricData, error)\nfunc (t *KustoToMetricsTransformer) extractValue(row map[string]any) (float64, error)\n\n// SIMPLIFY this function (remove dual-mode logic):\nfunc (t *KustoToMetricsTransformer) transformRow(row map[string]any) ([]MetricData, error) {\n    // Remove: if len(t.config.ValueColumns) &gt; 0 { ... } else { ... }\n    // Keep only: return t.transformRowMultiValue(...)\n}\n</code></pre></p>"},{"location":"designs/metric-values/#task-413-update-service-integration-priority-high-breaking","title":"Task 4.1.3: Update Service Integration \u26a0\ufe0f PRIORITY: HIGH - BREAKING","text":"<ul> <li>File: <code>adxexporter/service.go</code></li> <li>Changes: Remove <code>ValueColumn</code> field from TransformConfig construction</li> <li>Impact: Service no longer passes legacy field to transformer</li> </ul> <p>Code Changes: <pre><code>// BEFORE (current):\ntransformer := transform.NewKustoToMetricsTransformer(\n    transform.TransformConfig{\n        MetricNameColumn:  me.Spec.Transform.MetricNameColumn,\n        MetricNamePrefix:  me.Spec.Transform.MetricNamePrefix,\n        ValueColumn:       me.Spec.Transform.ValueColumn,        // REMOVE THIS LINE\n        ValueColumns:      me.Spec.Transform.ValueColumns,\n        TimestampColumn:   me.Spec.Transform.TimestampColumn,\n        LabelColumns:      me.Spec.Transform.LabelColumns,\n        DefaultMetricName: me.Spec.Transform.DefaultMetricName,\n    },\n    r.Meter,\n)\n\n// AFTER (target):\ntransformer := transform.NewKustoToMetricsTransformer(\n    transform.TransformConfig{\n        MetricNameColumn:  me.Spec.Transform.MetricNameColumn,\n        MetricNamePrefix:  me.Spec.Transform.MetricNamePrefix,\n        ValueColumns:      me.Spec.Transform.ValueColumns,       // Only field needed\n        TimestampColumn:   me.Spec.Transform.TimestampColumn,\n        LabelColumns:      me.Spec.Transform.LabelColumns,\n        DefaultMetricName: me.Spec.Transform.DefaultMetricName,\n    },\n    r.Meter,\n)\n</code></pre></p>"},{"location":"designs/metric-values/#task-414-update-unit-tests-priority-high-breaking","title":"Task 4.1.4: Update Unit Tests \u26a0\ufe0f PRIORITY: HIGH - BREAKING","text":"<ul> <li>Files: <code>transform/kusto_to_metrics_test.go</code>, <code>adxexporter/service_test.go</code></li> <li>Changes: Remove all single-value mode tests and update remaining tests</li> <li>Breaking Change: Tests validating backward compatibility will be removed</li> </ul> <p>Tests to Remove/Update: <pre><code>// REMOVE these test functions entirely:\nfunc TestTransformAndRegisterMetrics_SingleValueColumn(t *testing.T)\nfunc TestTransformAndRegisterMetrics_EmptyValueColumns(t *testing.T)  \nfunc TestTransformAndRegisterMetrics_NilValueColumns(t *testing.T)\n\n// UPDATE existing tests to remove ValueColumn field:\n- All test cases in transform/kusto_to_metrics_test.go using ValueColumn\n- Update validation tests to expect errors when ValueColumns is empty\n- Remove backward compatibility test scenarios\n</code></pre></p> <p>Test Updates Required: - Remove ~50+ test cases using <code>ValueColumn</code> field - Update validation tests to only check <code>ValueColumns</code> requirements - Remove tests verifying \"either ValueColumn or ValueColumns\" logic - Update error message assertions to match new validation messages</p>"},{"location":"designs/metric-values/#task-415-update-crd-manifests-priority-high-complete","title":"Task 4.1.5: Update CRD Manifests \u2705 PRIORITY: HIGH - COMPLETE","text":"<ul> <li>Command: <code>make generate-crd CMD=update</code></li> <li>Files: <code>kustomize/bases/metricsexporters_crd.yaml</code>, <code>operator/manifests/crds/metricsexporters_crd.yaml</code></li> <li>Changes: \u2705 Regenerated CRD manifests without <code>valueColumn</code> field</li> <li>Breaking Change: Kubernetes will reject MetricsExporter resources using <code>valueColumn</code></li> </ul> <p>Manifest Changes: <pre><code># REMOVE these lines from CRD schema:\nvalueColumn:\n  description: ValueColumn specifies which column contains the metric value\n  type: string\n\n# UPDATE required fields (remove valueColumn, keep valueColumns):\nrequired:\n- valueColumns  # Remove valueColumn from required list\n</code></pre></p>"},{"location":"designs/metric-values/#task-416-update-documentation-priority-medium-breaking","title":"Task 4.1.6: Update Documentation \u26a0\ufe0f PRIORITY: MEDIUM - BREAKING","text":"<ul> <li>Files: </li> <li><code>docs/designs/metric-values.md</code> (this file)</li> <li><code>docs/designs/kusto-to-metrics.md</code></li> <li><code>docs/crds.md</code></li> <li><code>README.md</code> examples</li> <li>Changes: Remove all references to <code>valueColumn</code> and update examples</li> <li>Migration Guide: Add breaking change documentation with migration instructions</li> </ul> <p>Documentation Updates: - Remove all <code>valueColumn</code> examples from documentation - Update all YAML examples to use <code>valueColumns</code> instead - Add migration guide section explaining how to convert existing configs - Update API reference documentation - Update troubleshooting guides to reflect new validation messages</p>"},{"location":"designs/metric-values/#task-417-update-example-files-priority-medium-breaking","title":"Task 4.1.7: Update Example Files \u26a0\ufe0f PRIORITY: MEDIUM - BREAKING","text":"<ul> <li>Files: <code>transform/example_usage.go</code>, example manifests, tutorials</li> <li>Changes: Update all examples to use multi-value syntax</li> <li>Migration Examples: Provide before/after examples for common use cases</li> </ul> <p>Example Migration: <pre><code># BEFORE (legacy single-value):\ntransform:\n  valueColumn: \"cpu_percent\"\n\n# AFTER (multi-value):\ntransform:\n  valueColumns: [\"cpu_percent\"]\n</code></pre></p>"},{"location":"designs/metric-values/#migration-guide-for-task-41","title":"Migration Guide for Task 4.1","text":""},{"location":"designs/metric-values/#breaking-change-impact-assessment","title":"Breaking Change Impact Assessment","text":"<ul> <li>Scope: All existing MetricsExporter resources using <code>valueColumn</code> field</li> <li>Timeline: Immediate upon deployment of updated CRDs</li> <li>Mitigation: Automated migration scripts and clear documentation</li> </ul>"},{"location":"designs/metric-values/#required-user-actions","title":"Required User Actions","text":"<ol> <li> <p>Update MetricsExporter Manifests:     <pre><code># Replace this pattern:\nsed -i 's/valueColumn: \"\\([^\"]*\\)\"/valueColumns: [\"\\1\"]/' your-manifest.yaml\n</code></pre></p> </li> <li> <p>Validate Updated Manifests:    <pre><code>kubectl apply --dry-run=client -f updated-manifest.yaml\n</code></pre></p> </li> <li> <p>Test Multi-Value Behavior: Verify metrics are generated correctly with new configuration</p> </li> </ol>"},{"location":"designs/metric-values/#rollback-strategy","title":"Rollback Strategy","text":"<ul> <li>Keep old CRD version available for emergency rollback</li> <li>Maintain ability to downgrade to previous version if issues arise</li> <li>Document rollback procedure for critical production environments</li> </ul>"},{"location":"designs/metric-values/#benefits-of-legacy-removal","title":"Benefits of Legacy Removal","text":"<ol> <li>Simplified API: Single configuration pattern reduces user confusion</li> <li>Reduced Maintenance: Less code to maintain, test, and document  </li> <li>Cleaner Architecture: Eliminates dual-mode complexity in transform engine</li> <li>Better Performance: Single code path is more efficient than mode detection</li> <li>Future-Proof: Positions codebase for easier future enhancements</li> </ol>"},{"location":"designs/metric-values/#risk-mitigation","title":"Risk Mitigation","text":"<ol> <li>Comprehensive Testing: Ensure all multi-value scenarios work before removal</li> <li>Documentation: Clear migration guide with automated conversion tools</li> <li>Gradual Rollout: Deploy to test environments first, monitor for issues</li> <li>Rollback Plan: Maintain ability to revert if critical issues discovered</li> </ol> <p>Task 4.1 Status: \u274c PENDING - Breaking change requiring careful planning and migration strategy </p>"},{"location":"designs/operator/","title":"adx-mon Operator Design","text":""},{"location":"designs/operator/#overview","title":"Overview","text":"<p>The <code>adx-mon operator</code> is a Kubernetes operator responsible for managing the lifecycle of adx-mon components (collector, ingestor, alerter) and potentially related Azure infrastructure like ADX (Azure Data Explorer) clusters. The operator uses the Azure SDK for Go to manage Azure resources when necessary.</p> <p>The operator aims to provide a simple, production-ready bootstrap experience for adx-mon deployments, while supporting advanced customization for complex and federated deployments.</p>"},{"location":"designs/operator/#responsibilities","title":"Responsibilities","text":"<ul> <li>Azure Infrastructure Management (via ADXCluster CRD):</li> <li>Declaratively create and manage Azure ADX clusters and databases based on the <code>ADXCluster</code> CRD.</li> <li>Support using existing ADX clusters via the <code>endpoint</code> field.</li> <li>Handle Azure resource provider registration and validation.</li> <li>Automatic resource group creation and management (optional).</li> <li> <p>Wait for Azure resources to reach a ready state.</p> </li> <li> <p>adx-mon Component Management:</p> </li> <li>Generate and apply manifests for adx-mon components (collector, ingestor, alerter) based on their respective CRDs (<code>Collector</code>, <code>Ingestor</code>, <code>Alerter</code>).</li> <li>Manage Kubernetes resources like StatefulSets, Deployments, and Services for these components.</li> <li>Support default container images for each component, with overrides via the <code>image</code> field in each CRD spec.</li> <li>Allow configuration of the number of ingestor instances via <code>spec.replicas</code> in the <code>Ingestor</code> CRD.</li> <li> <p>Support deployment of only a subset of components (e.g., only collectors) to enable federated/multi-cluster topologies.</p> </li> <li> <p>Reconciliation and Drift Detection:</p> </li> <li>Watch the CRDs and the managed Kubernetes resources (e.g., StatefulSets for the ingestor).</li> <li>If a managed resource is manually changed in a way that conflicts with the CRD spec (e.g., <code>replicas</code> count), detect the change and revert it to match the CRD's desired state.</li> <li>Ensure the operator's desired state is always enforced in the actual cluster state.</li> <li> <p>Note: The operator primarily reconciles Kubernetes resources based on CRD changes. It does not currently reconcile drifted Azure resources (e.g., ADX clusters modified outside Kubernetes).</p> </li> <li> <p>Incremental, Granular Reconciliation:</p> </li> <li>The operator proceeds in steps, often using conditions in the CRD status to track progress (e.g., <code>ADXCluster</code> ready, <code>Ingestor</code> ready).</li> <li> <p>Status and conditions are updated at each step to reflect progress and issues.</p> </li> <li> <p>Resource Cleanup on Deletion:</p> </li> <li>When a CRD for an adx-mon component (e.g., <code>Ingestor</code>) is deleted, the operator uses Kubernetes owner references to ensure the corresponding managed Kubernetes resources (like StatefulSets) are garbage collected.</li> <li>Deletion of an <code>ADXCluster</code> CRD does not automatically delete the underlying Azure resources by default.</li> </ul>"},{"location":"designs/operator/#multi-cluster-and-federation-support","title":"Multi-Cluster and Federation Support","text":"<ul> <li>The operator supports deploying components separately for federated scenarios:</li> <li>Central Ingestor Cluster: Deploys <code>Ingestor</code> components (potentially linked to <code>ADXCluster</code> resources). Receives data from remote collectors.</li> <li>Collector Clusters: Deploys only <code>Collector</code> components. Requires configuration of the <code>ingestorEndpoint</code> in the <code>CollectorSpec</code> to point to the central ingestor.</li> </ul>"},{"location":"designs/operator/#crd-design","title":"CRD Design","text":"<p>See also: CRD Reference for a summary table and links to all CRDs, including advanced types (SummaryRule, Function, ManagementCommand).</p> <p>The adx-mon operator manages the following Custom Resource Definitions (CRDs): - ADXCluster - Ingestor - Collector - Alerter</p> <p>Each CRD is described below with its current schema and example YAML, strictly reflecting the Go source definitions.</p>"},{"location":"designs/operator/#adxcluster-crd","title":"ADXCluster CRD","text":"<p>Spec fields: - <code>clusterName</code> (string, required): Unique, valid name for the ADX cluster. Must match ^[a-zA-Z0-9-]+$ and be at most 100 characters. Used for resource identification and naming in Azure. - <code>endpoint</code> (string, optional): URI of an existing ADX cluster. If set, the operator will use this cluster instead of provisioning a new one. Example: \"https://mycluster.kusto.windows.net\" - <code>databases</code> (array of objects, optional): List of databases to create in the ADX cluster. Each object has:   - <code>databaseName</code> (string, required): ADX valid database name. ^[a-zA-Z0-9_]+$, 1-64 chars.   - <code>retentionInDays</code> (int, optional): Retention period in days. Default: 30.   - <code>retentionPolicy</code> (string, optional): ADX retention policy.   - <code>telemetryType</code> (string, required): One of <code>Metrics</code>, <code>Logs</code>, or <code>Traces</code>. - <code>provision</code> (object, optional): Azure provisioning details:   - <code>subscriptionId</code> (string, optional): Azure subscription ID. If omitted, will be auto-detected in zero-config mode.   - <code>resourceGroup</code> (string, optional): Azure resource group. If omitted, will be auto-created or detected.   - <code>location</code> (string, optional): Azure region (e.g., \"eastus2\"). If omitted, will be auto-detected.   - <code>skuName</code> (string, optional): Azure SKU (e.g., \"Standard_L8as_v3\").   - <code>tier</code> (string, optional): Azure ADX tier (e.g., \"Standard\"). Defaults to \"Standard\" if not specified.   - <code>userAssignedIdentities</code> (array of strings, optional): List of MSIs to attach to the cluster (resource-ids).   - <code>autoScale</code> (bool, optional): Enable auto-scaling for the ADX cluster. Defaults to false.   - <code>autoScaleMax</code> (int, optional): Maximum number of nodes for auto-scaling. Defaults to 10.   - <code>autoScaleMin</code> (int, optional): Minimum number of nodes for auto-scaling. Defaults to 2.   - <code>appliedProvisionState</code> (string, optional, read-only): JSON-serialized snapshot of the SkuName, Tier, and UserAssignedIdentities as last applied by the operator.</p> <p>Status fields: - <code>conditions</code> (array, optional): Standard Kubernetes conditions.</p> <p>Minimal Example: <pre><code>apiVersion: adx-mon.azure.com/v1\nkind: ADXCluster\nmetadata:\n  name: minimal-adx-cluster\nspec:\n  clusterName: minimal-adx-cluster\n</code></pre></p> <p>Full Example: <pre><code>apiVersion: adx-mon.azure.com/v1\nkind: ADXCluster\nmetadata:\n  name: prod-adx-cluster\nspec:\n  clusterName: prod-metrics\n  endpoint: \"https://prod-metrics.kusto.windows.net\"\n  databases:\n    - databaseName: metricsdb\n      retentionInDays: 30\n      telemetryType: Metrics\n    - databaseName: logsdb\n      retentionInDays: 30\n      telemetryType: Logs\n  provision:\n    subscriptionId: \"00000000-0000-0000-0000-000000000000\"\n    resourceGroup: \"adx-monitor-prod\"\n    location: \"eastus2\"\n    skuName: \"Standard_L8as_v3\"\n    tier: \"Standard\"\n    userAssignedIdentities:\n      - \"/subscriptions/00000000-0000-0000-0000-000000000000/resourceGroups/adx-monitor-prod/providers/Microsoft.ManagedIdentity/userAssignedIdentities/identity1\"\n    autoScale: true\n    autoScaleMax: 20\n    autoScaleMin: 4\n</code></pre></p>"},{"location":"designs/operator/#ingestor-crd","title":"Ingestor CRD","text":"<p>Spec fields: - <code>image</code> (string, optional): Container image for the ingestor component. - <code>replicas</code> (int32, optional): Number of ingestor replicas. Default: 1. - <code>endpoint</code> (string, optional): Endpoint for the ingestor. If running in a cluster, this should be the service name; otherwise, the operator will generate an endpoint. - <code>exposeExternally</code> (bool, optional): Whether to expose the ingestor externally. Default: false. - <code>adxClusterSelector</code> (LabelSelector, required): Label selector to target ADXCluster CRDs.</p> <p>Status fields: - <code>conditions</code> (array, optional): Standard Kubernetes conditions.</p> <p>Example: <pre><code>apiVersion: adx-mon.azure.com/v1\nkind: Ingestor\nmetadata:\n  name: prod-ingestor\nspec:\n  image: \"ghcr.io/azure/adx-mon/ingestor:v1.0.0\"\n  replicas: 3\n  endpoint: \"http://prod-ingestor.monitoring.svc.cluster.local:8080\"\n  exposeExternally: false\n  adxClusterSelector:\n    matchLabels:\n      app: adx-mon\n</code></pre></p>"},{"location":"designs/operator/#collector-crd","title":"Collector CRD","text":"<p>Spec fields: - <code>image</code> (string, optional): Container image for the collector component. - <code>ingestorEndpoint</code> (string, optional): URI endpoint for the ingestor service to send data to.</p> <p>Status fields: - <code>conditions</code> (array, optional): Standard Kubernetes conditions.</p> <p>Example: <pre><code>apiVersion: adx-mon.azure.com/v1\nkind: Collector\nmetadata:\n  name: prod-collector\nspec:\n  image: \"ghcr.io/azure/adx-mon/collector:v1.0.0\"\n  ingestorEndpoint: \"http://prod-ingestor.monitoring.svc.cluster.local:8080\"\n</code></pre></p>"},{"location":"designs/operator/#alerter-crd","title":"Alerter CRD","text":"<p>Spec fields: - <code>image</code> (string, optional): Container image for the alerter component. - <code>notificationEndpoint</code> (string, required): URI where alert notifications will be sent. - <code>adxClusterSelector</code> (LabelSelector, required): Label selector to target ADXCluster CRDs.</p> <p>Status fields: - <code>conditions</code> (array, optional): Standard Kubernetes conditions.</p> <p>Example: <pre><code>apiVersion: adx-mon.azure.com/v1\nkind: Alerter\nmetadata:\n  name: prod-alerter\nspec:\n  image: \"ghcr.io/azure/adx-mon/alerter:v1.0.0\"\n  notificationEndpoint: \"http://alerter-endpoint\"\n  adxClusterSelector:\n    matchLabels:\n      app: adx-mon\n</code></pre></p>"},{"location":"designs/operator/#adx-cluster-management-details","title":"ADX Cluster Management (Details)","text":"<p>An important option for ADX clusters is to \"bring your own\" or utilize an existing cluster. This is accomplished by specifying the cluster's endpoint in the <code>ADXCluster</code> CRD (<code>spec.endpoint</code>). When an endpoint is provided, the operator will use the referenced existing ADX cluster rather than provisioning a new one.</p>"},{"location":"designs/operator/#auto-provisioning-and-zero-config-support","title":"Auto-Provisioning and Zero-Config Support","text":"<p>The operator supports both fully specified and zero-config deployments for new clusters (when <code>spec.endpoint</code> is not provided):</p> <ol> <li> <p>Zero-Config Mode (via Azure SDK features, not explicitly shown in operator code provided):</p> <ul> <li>Uses Azure IMDS or environment variables to automatically detect:<ul> <li>Region</li> <li>Subscription ID</li> <li>Resource Group</li> </ul> </li> <li>May automatically select an optimal SKU.</li> <li>Creates default databases if <code>spec.databases</code> is empty.</li> </ul> </li> <li> <p>Infrastructure Preparation (via Azure SDK):</p> <ul> <li>Automatically registers the <code>Microsoft.Kusto</code> resource provider if needed.</li> <li>Creates resource group if specified and doesn't exist.</li> <li>Validates SKU availability in the target region.</li> </ul> </li> </ol>"},{"location":"designs/operator/#adx-deployment-strategy-via-azure-sdk","title":"ADX Deployment Strategy (via Azure SDK)","text":"<p>The operator uses the Azure SDK for Go to manage the lifecycle of ADX clusters and databases when provisioning:</p> <ol> <li>Azure SDK-Based Deployment: Creates, updates, and potentially deletes ADX clusters and databases.</li> <li>Database Configuration: Databases created based on <code>spec.databases</code>.</li> </ol>"},{"location":"designs/operator/#managed-identity-integration-via-azure-sdk","title":"Managed Identity Integration (via Azure SDK)","text":"<p>The operator supports managed identity configuration for provisioned clusters:</p> <ol> <li>Identity Assignment: System-assigned identity by default, or user-assigned if <code>spec.provision.managedIdentityClientId</code> is provided.</li> <li>Role Management: May handle role assignments (details depend on specific Azure SDK implementation).</li> </ol>"},{"location":"designs/operator/#sku-selection-strategy-via-azure-sdk","title":"SKU Selection Strategy (via Azure SDK)","text":"<p>The operator likely implements an SKU selection strategy when provisioning:</p> <ol> <li>Preferred SKUs: May prioritize certain SKUs (e.g., <code>Standard_L8as_v3</code>).</li> <li>Fallback Behavior: Validates SKU availability and falls back if necessary.</li> </ol>"},{"location":"designs/operator/#resource-group-management-via-azure-sdk","title":"Resource Group Management (via Azure SDK)","text":"<p>The operator handles resource group lifecycle when provisioning:</p> <ol> <li>Resource Group Creation: Checks for existence, creates if specified and not found.</li> </ol>"},{"location":"designs/operator/#example-crds","title":"Example CRDs","text":""},{"location":"designs/operator/#minimal-zero-config-ingestorcollectoralerter-with-existing-adx-cluster","title":"Minimal (Zero-Config Ingestor/Collector/Alerter with Existing ADX Cluster)","text":"<p>This example assumes an <code>ADXCluster</code> named <code>existing-adx-cluster</code> (managed separately or by another CRD) exists and has the label <code>app: my-adx</code>. It deploys the components using default images and settings, connecting them to the specified cluster.</p> <pre><code>apiVersion: adx-mon.azure.com/v1\nkind: ADXCluster\nmetadata:\n  name: existing-adx-cluster # This CRD might just reference an existing cluster\n  labels:\n    app: my-adx\nspec:\n  clusterName: my-existing-cluster-name # Name in Azure\n  endpoint: \"https://my-existing-cluster.eastus2.kusto.windows.net\" # IMPORTANT: Tells operator to use existing\n  databases:\n    - databaseName: \"MetricsDB\"\n      telemetryType: Metrics\n    - databaseName: \"LogsDB\"\n      telemetryType: Logs\n# No 'provision' section needed if using existing endpoint\n\n---\n\napiVersion: adx-mon.azure.com/v1\nkind: Ingestor\nmetadata:\n  name: minimal-ingestor\nspec:\n  adxClusterSelector:\n    matchLabels:\n      app: my-adx # Selects the ADXCluster above\n\n---\n\napiVersion: adx-mon.azure.com/v1\nkind: Collector\nmetadata:\n  name: minimal-collector\nspec: {} # Ingestor endpoint will be auto-discovered if Ingestor CRD is in the same namespace\n\n---\n\napiVersion: adx-mon.azure.com/v1\nkind: Alerter\nmetadata:\n  name: minimal-alerter\nspec:\n  notificationEndpoint: \"http://alertmanager.example.com:9093/api/v1/alerts\"\n  adxClusterSelector:\n    matchLabels:\n      app: my-adx # Selects the ADXCluster above\n</code></pre>"},{"location":"designs/operator/#complete-fully-customized-with-provisioning-example","title":"Complete (Fully Customized with Provisioning) Example","text":"<p>This example demonstrates provisioning a new ADX cluster and configuring all components with custom settings.</p> <pre><code>apiVersion: adx-mon.azure.com/v1\nkind: ADXCluster\nmetadata:\n  name: prod-adx-cluster\n  labels:\n    env: production\n    team: monitoring\nspec:\n  clusterName: adxmonprod01 # Desired name in Azure\n  # No 'endpoint' means provision a new cluster\n  provision:\n    subscriptionId: \"00000000-0000-0000-0000-000000000000\"\n    resourceGroup: \"adx-monitor-prod-rg\"\n    location: \"eastus2\"\n    skuName: \"Standard_L8as_v3\"\n    tier: \"Standard\"\n    # managedIdentityClientId: \"11111111-1111-1111-1111-111111111111\" # Optional: User-assigned MSI\n  databases:\n    - databaseName: \"ProdMetrics\"\n      telemetryType: Metrics\n      retentionInDays: 90\n    - databaseName: \"ProdLogs\"\n      telemetryType: Logs\n      retentionInDays: 30\n\n---\n\napiVersion: adx-mon.azure.com/v1\nkind: Ingestor\nmetadata:\n  name: prod-ingestor\nspec:\n  image: \"myacr.azurecr.io/adx-mon/ingestor:v1.2.3\"\n  replicas: 3\n  # endpoint: \"ingestor-service.prod.svc.cluster.local\" # Optional: Override auto-generated endpoint\n  exposeExternally: false\n  adxClusterSelector:\n    matchLabels:\n      env: production # Selects the ADXCluster above\n\n---\n\napiVersion: adx-mon.azure.com/v1\nkind: Collector\nmetadata:\n  name: prod-collector\nspec:\n  image: \"myacr.azurecr.io/adx-mon/collector:v1.2.3\"\n  # ingestorEndpoint: \"http://ingestor-service.prod.svc.cluster.local:8080\" # Optional: Override auto-discovery\n\n---\n\napiVersion: adx-mon.azure.com/v1\nkind: Alerter\nmetadata:\n  name: prod-alerter\nspec:\n  image: \"myacr.azurecr.io/adx-mon/alerter:v1.2.3\"\n  notificationEndpoint: \"http://prod-alertmanager.example.com:9093/api/v1/alerts\"\n  adxClusterSelector:\n    matchLabels:\n      app: adx-mon\n</code></pre> <p>This configuration demonstrates: - Full ADX cluster configuration with provisioning details - Ingestor configuration with custom replica count, endpoint, and ADXCluster reference - Collector configuration with explicit ingestor endpoint and custom image - Alerter configuration with custom image, notification endpoint, and ADXCluster selector</p>"},{"location":"designs/operator/#crd-enhancements-for-federated-cluster-support","title":"CRD Enhancements for Federated Cluster Support","text":"<p>To enable federated cluster functionality, the ADXCluster CRD is extended with new fields and sections as follows:</p>"},{"location":"designs/operator/#role-field","title":"Role Field","text":"<p>Specify the cluster's role: - <code>role: Partition</code> (default, for data-holding clusters) - <code>role: Federated</code> (for the central federating cluster)</p>"},{"location":"designs/operator/#federation-section","title":"Federation Section","text":"<p>A new <code>federation</code> section encapsulates all federation-related configuration.</p>"},{"location":"designs/operator/#for-partition-clusters","title":"For Partition Clusters","text":"<p><pre><code>spec:\n  role: Partition\n  federation:\n    federatedClusters:\n      - endpoint: \"https://federated.kusto.windows.net\"\n        heartbeatDatabase: \"FleetDiscovery\"\n        heartbeatTable: \"Heartbeats\"\n        managedIdentityClientId: \"xxxx-xxxx-xxxx\"\n    partitioning:\n      geo: \"EU\"\n      location: \"westeurope\"\n</code></pre> - <code>federatedClusters</code>: List of federated cluster endpoints and heartbeat config. - <code>partitioning</code>: Open-ended map/object for partitioning metadata (geo, location, tenant, etc.).</p>"},{"location":"designs/operator/#for-federated-clusters","title":"For Federated Clusters","text":"<p><pre><code>spec:\n  clusterName: hub\n  endpoint: \"https://hub.kusto.windows.net\"\n  provision:\n    subscriptionId: \"00000000-0000-0000-0000-000000000000\"\n    resourceGroup: \"adx-monitor-prod\"\n    location: \"eastus2\"\n    skuName: \"Standard_L8as_v3\"\n    tier: \"Standard\"\n    managedIdentityClientId: \"11111111-1111-1111-1111-111111111111\"\n  role: Federated\n  federation:\n    heartbeatDatabase: \"FleetDiscovery\"\n    heartbeatTable: \"Heartbeats\"\n    heartbeatTTL: \"1h\"\n    macroExpand:\n      functionPrefix: \"federated_\"\n      bestEffort: true\n      folder: \"federation_facades\"\n</code></pre> - <code>heartbeatDatabase</code>/<code>heartbeatTable</code>: Where to read heartbeats from. - <code>heartbeatTTL</code>: How recent a heartbeat must be to consider a partition cluster live. - <code>macroExpand</code>: Options for macro-expand KQL function generation.   - <code>functionPrefix</code>: Prefix for generated KQL functions.   - <code>bestEffort</code>: Use best effort mode for macro-expand.   - <code>folder</code>: Folder in the federated cluster where macro-expand facades/functions are stored.</p>"},{"location":"designs/operator/#heartbeat-table-schema","title":"Heartbeat Table Schema","text":"<p>The federated feature relies on a <code>heartbeat</code> table in the federated cluster to track the state and topology of partition clusters. The schema for this table is as follows:</p> Column Name Type Description Timestamp datetime When the heartbeat was sent ClusterEndpoint string The endpoint of the partition cluster Schema dynamic Databases and tables present in the partition PartitionMetadata dynamic Partitioning attributes (geo, location, etc.)"},{"location":"designs/operator/#field-explanations","title":"Field Explanations","text":"<ul> <li>Timestamp: The UTC time when the partition cluster emitted the heartbeat.</li> <li>ClusterEndpoint: The fully qualified endpoint URL of the partition cluster.</li> <li>Schema: A dynamic object describing the databases and tables present in the partition cluster. For example:   <pre><code>[\n  {\n    \"database\": \"Logs\",\n    \"tables\": [\"A\", \"B\", \"C\"]\n  }\n]\n</code></pre></li> <li>PartitionMetadata: A dynamic object containing the partitioning strategy and attributes for the cluster, such as geo or location. For example:   <pre><code>{\n  \"geo\": \"EU\",\n  \"location\": \"westeurope\"\n}\n</code></pre></li> </ul>"},{"location":"designs/operator/#example-log-row","title":"Example Log Row","text":"<pre><code>{\n    \"Timestamp\": \"2025-05-03T12:34:56Z\",\n    \"ClusterEndpoint\": \"https://eu-partition.kusto.windows.net\",\n    \"Schema\": [\n        {\n            \"database\": \"Logs\",\n            \"tables\": [\"A\", \"B\", \"C\"]\n        }\n    ],\n    \"PartitionMetadata\": {\n        \"geo\": \"EU\",\n        \"location\": \"westeurope\"\n    }\n}\n</code></pre>"},{"location":"designs/operator/#example-crd-snippets","title":"Example CRD Snippets","text":"<p>Partition Cluster Example: <pre><code>apiVersion: adx-mon.azure.com/v1\nkind: ADXCluster\nmetadata:\n  name: eu-partition\nspec:\n  role: Partition\n  federation:\n    federatedClusters:\n      - endpoint: \"https://federated.kusto.windows.net\"\n        heartbeatDatabase: \"FleetDiscovery\"\n        heartbeatTable: \"Heartbeats\"\n        managedIdentityClientId: \"xxxx-xxxx\"\n    partitioning:\n      geo: \"EU\"\n      location: \"westeurope\"\n  # ...existing cluster config...\n</code></pre></p> <p>Federated Cluster Example: <pre><code>apiVersion: adx-mon.azure.com/v1\nkind: ADXCluster\nmetadata:\n  name: federated\nspec:\n  role: Federated\n  federation:\n    heartbeatDatabase: \"FleetDiscovery\"\n    heartbeatTable: \"Heartbeats\"\n    heartbeatTTL: \"1h\"\n    macroExpand:\n      functionPrefix: \"federated_\"\n      bestEffort: true\n      folder: \"federation_facades\"\n  # ...existing cluster config...\n</code></pre></p> <p>These enhancements provide a clear, extensible way to configure both partition and federated clusters for the federation feature, including macro-expand facade management.</p>"},{"location":"designs/operator/#operator-workflow","title":"Operator Workflow","text":"<ol> <li>Granular Reconciliation: </li> <li>The operator proceeds in incremental steps, updating subresource conditions for each phase:<ul> <li>Upsert Azure-managed ADX clusters.</li> <li>Wait for ADX clusters to be ready.</li> <li>Upsert ADX databases.</li> <li>Wait for databases to be ready.</li> <li>Create ingestor StatefulSet (parameterized with ADX connection strings).</li> <li>Wait for ingestor to be ready.</li> <li>Create collector DaemonSet (parameterized with ingestor endpoints).</li> <li>Wait for collector to be ready.</li> <li>Create alerter if specified.</li> </ul> </li> <li> <p>Each step is only attempted after all dependencies are ready.</p> </li> <li> <p>Watch Managed Resources: </p> </li> <li>Monitor StatefulSets/Deployments for adx-mon components.</li> <li> <p>If a managed resource is changed outside the operator (e.g., replica count updated), the operator will reconcile the resource back to the value specified in the CRD. The CRD spec is always the source of truth for fields like replica count; the operator does not perform two-way sync. Status fields in the CRD are updated to reflect the current state and any reconciliation actions taken.</p> </li> <li> <p>Sync Desired and Actual State: </p> </li> <li>If the Operator CRD is updated, reconcile the manifests and Azure resources as needed.</li> <li>If the actual state drifts from the desired state, update the actual state to match the desired state.</li> </ol>"},{"location":"designs/operator/#federated-cluster-support","title":"Federated Cluster Support","text":""},{"location":"designs/operator/#overview_1","title":"Overview","text":"<p>The operator supports a federated ADX cluster model to enable organizations to partition telemetry data across multiple Azure Data Explorer (ADX) clusters, often to meet geographic or regulatory data residency requirements. The federated model provides a \"single pane of glass\" experience for querying and analytics, allowing users to query a central federated cluster that transparently federates queries across all partition clusters.</p>"},{"location":"designs/operator/#architecture","title":"Architecture","text":"<ul> <li> <p>Partition Clusters:   Each partition cluster is managed by its own ADX operator instance and contains a subset of the data, typically partitioned by geography or other business criteria.</p> </li> <li> <p>Federated Cluster:   A central federated cluster is managed by a federated operator. This cluster does not manage the lifecycle of partition clusters, but provides a unified query interface.</p> </li> </ul>"},{"location":"designs/operator/#discovery-mechanism","title":"Discovery Mechanism","text":"<ul> <li>Heartbeat-Based Discovery: </li> <li>Partition cluster operators are configured with the endpoint(s) of the federated cluster via the ADX CRD.</li> <li>Each partition cluster operator periodically sends a heartbeat log row to a specified database and table in the federated cluster.</li> <li>The heartbeat includes:<ul> <li>Partition cluster endpoint</li> <li>List of databases and tables (as a dynamic object)</li> <li>Partitioning schema (e.g., location, geo, tenant) as a dynamic object</li> <li>Timestamp</li> </ul> </li> <li>The federated operator periodically queries the heartbeat table (e.g., <code>Heartbeats | where Timestamp &gt; ago(1h) | distinct Endpoint</code>) to discover live partition clusters and their topologies.</li> <li>Liveness is inferred from heartbeat freshness; clusters that have not heartbeated recently are excluded from federated queries.</li> <li> <p>Retention policy on the heartbeat table ensures cleanup of old/stale entries.</p> </li> <li> <p>Authentication: </p> </li> <li>Partition clusters authenticate to the federated cluster using Microsoft-supported mechanisms (e.g., Managed Service Identity), with the identity specified in the CRD.</li> </ul>"},{"location":"designs/operator/#federated-querying","title":"Federated Querying","text":"<ul> <li>Macro-Expand Operator: </li> <li>The federated cluster uses the Kusto macro-expand operator to define KQL functions or queries that union data from all live partition clusters.</li> <li>Entity groups for macro-expand are dynamically constructed based on the current fleet topology, as discovered via heartbeats.</li> <li>This enables the federated cluster to present a single logical view for each table, while queries are transparently federated across all relevant clusters and databases.</li> <li> <p>The federated operator updates KQL functions and entity groups as the fleet topology changes.</p> </li> <li> <p>Schema Flexibility: </p> </li> <li>The dynamic object for database/table topology allows the federated operator to only federate queries to clusters that actually contain the relevant table, improving efficiency and flexibility.</li> </ul>"},{"location":"designs/operator/#operational-considerations","title":"Operational Considerations","text":"<ul> <li>Heartbeat interval and retention policy are configurable.</li> <li>MSI permissions must be correctly configured for secure heartbeat writes.</li> <li>The federated operator must handle schema differences and errors gracefully.</li> <li>Metrics and alerts should be implemented for missed heartbeats or authentication failures.</li> </ul>"},{"location":"designs/operator/#implementation-notes","title":"Implementation Notes","text":"<ul> <li>The operator should use controller-runtime's watches and informers to efficiently detect changes to both its own CRD and the managed resources.</li> <li>Azure resources are the source of truth for infrastructure; the operator should not attempt to reconcile drifted Azure resources directly.</li> <li>The operator should be idempotent and safe to reapply.</li> <li>Subresource conditions should be used to provide granular status and progress reporting.</li> <li>Future versions may support more advanced customization, validation, and upgrade strategies.</li> </ul>"},{"location":"designs/operator/#future-enhancements","title":"Future Enhancements","text":""},{"location":"designs/operator/#ingestor-auto-scaler","title":"Ingestor Auto-Scaler","text":"<ul> <li>Implement an auto-scaler for the Ingestor component that dynamically adjusts the number of replicas based on workload or custom metrics.</li> <li>Add an option in the Ingestor CRD to enable or configure auto-scaling behavior (e.g., min/max replicas, scaling thresholds).</li> </ul>"},{"location":"designs/operator/#adx-cluster-federation","title":"ADX Cluster Federation","text":"<ul> <li>Support for ADX cluster federation, enabling partitioning of data across multiple ADX clusters.</li> <li>Introduce a central ADX cluster that can federate queries across all partitioned clusters, providing a single pane of glass for querying and analytics.</li> <li>Enhance CRDs and operator logic to manage federated cluster relationships and query routing.</li> </ul>"},{"location":"designs/schema-etl/","title":"Custom Schema ETL","text":""},{"location":"designs/schema-etl/#summary","title":"Summary","text":"<p>Logs are ingested into Tables using OTLP schema. There are several limitations in querying OTLP directly that can be solved by supporting custom schema definitions.</p>"},{"location":"designs/schema-etl/#solutions","title":"Solutions","text":"<p>Below we present two ways of defining a schema and two ways of implementing those schemas.</p>"},{"location":"designs/schema-etl/#schema-definition","title":"Schema Definition","text":"<ul> <li>CRD that specifies a Table's schema</li> <li>CRD that generically specifies KQL functions</li> </ul>"},{"location":"designs/schema-etl/#etl","title":"ETL","text":"<ul> <li> <p>Update Policies Leveraging the Medallion architecture logs are ingested into preliminary Tables using the OTLP schema definition and update-policies are leveraged to perform ETL operations against the preliminary Table that then emit the logs with custom schemas applied before being stored in their final destinations. We have a great deal of experience with update-policies and have found several limitations, such as failed ingestions due to schema missalignment and increased runtime resource expenses. We have therefore decided to try another route.</p> </li> <li> <p>Views A View with the same name as a Table is defined, where a custom schema is defined and realized at query time. This means the content is stored as OTLP but able to be queried with a user defined schema. There will be a query-time performance penalty with this approach that we need to measure and determine if it's acceptable. </p> </li> </ul>"},{"location":"designs/schema-etl/#proposed-solution","title":"Proposed Solution","text":"<p>We will define a CRD that enables a user to define a KQL Function, with an optional parameter to specify the Function as being a View, whereby ETL operations within the View will present the user with their desired schema at the time of query.</p>"},{"location":"designs/schema-etl/#crd","title":"CRD","text":"<p>Our CRD could simply enable a user to specify any arbitrary KQL; however, to prevent admin commands from being executed, we'll instead specify all the possible fields for a Function and construct the KQL scaffolding ourselves.</p> <p>The CRD definition is as follows:</p> <pre><code>---\napiVersion: apiextensions.k8s.io/v1\nkind: CustomResourceDefinition\nmetadata:\n  annotations:\n    controller-gen.kubebuilder.io/version: v0.16.1\n  name: functions.adx-mon.azure.com\nspec:\n  group: adx-mon.azure.com\n  names:\n    kind: Function\n    listKind: FunctionList\n    plural: functions\n    singular: function\n  scope: Namespaced\n  versions:\n  - name: v1\n    schema:\n      openAPIV3Schema:\n        description: Function defines a KQL function to be maintained in the Kusto\n          cluster\n        properties:\n          apiVersion:\n            description: |-\n              APIVersion defines the versioned schema of this representation of an object.\n              Servers should convert recognized schemas to the latest internal value, and\n              may reject unrecognized values.\n              More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n            type: string\n          kind:\n            description: |-\n              Kind is a string value representing the REST resource this object represents.\n              Servers may infer this from the endpoint the client submits requests to.\n              Cannot be updated.\n              In CamelCase.\n              More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n            type: string\n          metadata:\n            type: object\n          spec:\n            description: FunctionSpec defines the desired state of Function\n            properties:\n              body:\n                description: Body is the body of the function\n                type: string\n              database:\n                description: Database is the name of the database in which the function\n                  will be created\n                type: string\n              docString:\n                description: DocString is the documentation string for the function\n                type: string\n              folder:\n                description: Folder is the folder in which the function will be created\n                type: string\n              isView:\n                description: IsView is a flag indicating whether the function is a\n                  view\n                type: boolean\n              name:\n                description: Name is the name of the function\n                type: string\n              parameters:\n                description: Parameters is a list of parameters for the function\n                items:\n                  properties:\n                    name:\n                      type: string\n                    type:\n                      type: string\n                  required:\n                  - name\n                  - type\n                  type: object\n                type: array\n              table:\n                description: |-\n                  Table is the name of the table in which the function will be created. We must\n                  specify a table if the function is a view, otherwise the Table name is optional.\n                type: string\n            required:\n            - body\n            - database\n            - name\n            type: object\n          status:\n            description: FunctionStatus defines the observed state of Function\n            properties:\n              lastTimeReconciled:\n                description: LastTimeReconciled is the last time the Function was\n                  reconciled\n                format: date-time\n                type: string\n              message:\n                description: Message is a human-readable message indicating details\n                  about the Function\n                type: string\n            required:\n            - lastTimeReconciled\n            - message\n            type: object\n        type: object\n    served: true\n    storage: true\n    subresources:\n      status: {}\n</code></pre> <p>A sample use is:</p> <pre><code>apiVersion: adx-mon.azure.com/v1\nkind: Function\nmetadata:\n  name: samplefn\nspec:\n  name: SampleFn\n  body: |\n    SampleFn\n    | extend Timestamp = todatetime(Body['ts']),\n            Message = tostring(Body['msg']),\n            Labels = Attributes['labels'],\n            Host = tostring(Resource['Host'])\n  database: SomeDatabase\n  table: SampleFn\n  isView: true\n</code></pre> <p>Ingestor would then execute the following </p> <pre><code>.create-or-alter function with ( view=true ) SampleFn () {\n    SampleFn\n    | extend Timestamp = todatetime(Body['ts']),\n            Message = tostring(Body['msg']),\n            Labels = Attributes['labels'],\n            Host = tostring(Resource['Host'])\n}\n</code></pre>"},{"location":"designs/schema-etl/#implementation-details","title":"Implementation Details","text":"<ul> <li>Ingestor already manages the creation of Tables as logs flow through the system, so we'll have Ingestor also manage creation of these Functions because the Table must exist prior to the Function that it references.</li> <li>Alerter already handles CRDs, as much as possible we'll share code between these components so that we're not duplicating a bunch of code.</li> <li>When attempting to reconcile each Function's state, we'll make sure the process is idempotent such that if a Function definition already exists that matches all the parameters found in its CRD, we do not attempt to update its state.</li> </ul>"},{"location":"designs/schema-etl/#questions","title":"Questions","text":"<ul> <li>If Ingestor has a reference to a CRD / Function that is later deleted, do we want to delete the associated Function in Kusto?</li> </ul> <p>See also: CRD Reference for a summary of all CRDs and links to advanced usage.</p>"},{"location":"designs/static-pods/","title":"Static Pods","text":""},{"location":"designs/static-pods/#background","title":"Background","text":"<p>In some scenarios, Pods cannot be annotated for log collection, particularly in cases involving static Pod manifests (such as kube-apiserver). This document describes a mechanism to configure log scraping for these Pods without requiring changes to their manifests.</p>"},{"location":"designs/static-pods/#proposed-solution","title":"Proposed Solution","text":"<p>Introduce a configuration block that specifies which Pod log targets should be collected. When a target configuration matches a static Pod, the appropriate logs will be included in the overall log collection pipeline.</p>"},{"location":"designs/static-pods/#configuration","title":"Configuration","text":"<p>A sample configuration that aims to scrape kube-apiserver logs is provided below. The configuration itself remains unchanged:</p> <pre><code>[[host-log.static-pod-target]]\nnamespace = \"kube-system\"\nlabel-targets = { \"component\" = \"kube-apiserver\" }\ncontainer = \"apiserver\"\nlog-type = \"kubernetes\"\ndatabase = \"Logs\"\ntable = \"KubeAPIServer\"\nparsers = [\"klog\"]\n</code></pre>"},{"location":"designs/summary-rules/","title":"Summary Rules","text":""},{"location":"designs/summary-rules/#background","title":"Background","text":"<p>ADX-Mon ingests telemetry into ADX without restrictions on cardinality or dimensionality.  Storing this raw data for long periods of time is expensive and inefficient.  There are times where it would be useful to aggregate this raw data for longer retention, query efficiency or to reduce costs.</p> <p>Similarly, sometimes there is data in other ADX clusters that would be useful to join with the local telemetry collected by ADX-Mon.  While this data can be queried using <code>cluster()</code> functions, sometimes the cluster is geographically far from the local cluster and this approach is not as performant as having the data locally.  In addition, the remote cluster may not have the same retention policies as the local cluster which can lead to query issues.</p>"},{"location":"designs/summary-rules/#proposed-solution","title":"Proposed Solution","text":"<p>We will define a CRD, <code>SummaryRule</code>, that enables a user to define a KQL query, a interval and a destination Table for the results of the query.  The query will be executed on a schedule and the results will be stored in the destination Table. ADX-Mon will maintain the last execution time and the start and end time of the query.  The start and end time will be passed to the query, similar to <code>AlertRules</code>, to ensure consistent results.</p> <p>Best Practice: Always use <code>between(_startTime .. _endTime)</code> for time filtering in your summary rule KQL queries. This ensures correct, non-overlapping, and gap-free time windows. The system guarantees that <code>_endTime</code> is exclusive by subtracting 1 tick (100ns) from the window, so you can safely use inclusive <code>between</code> logic.</p>"},{"location":"designs/summary-rules/#crd","title":"CRD","text":"<p>Our CRD could simply enable a user to specify any arbitrary KQL; however, to prevent admin commands from being executed, we'll instead specify all the possible fields for a Function and construct the KQL scaffolding ourselves.</p> <p>The CRD definition is as follows:</p> <pre><code>...\n</code></pre> <p>A sample use is:</p> <pre><code>apiVersion: adx-mon.azure.com/v1\nkind: SummaryRule\nmetadata:\n  name: samplefn\nspec:\n  database: SomeDatabase\n  name: HourlyAvg\n  body: |\n    SomeMetric\n    | where Timestamp between (_startTime .. _endTime)\n    | summarize avg(Value) by bin(Timestamp, 1h)\n  table: SomeMetricHourlyAvg\n  interval: 1h\n</code></pre> <p>Ingestor would then execute the following</p> <pre><code>let _startTime = datetime(...);\nlet _endTime = datetime(...);\n.set-or-append async SomeMetricHourlyAvg &lt;|\n    SomeMetric\n    | where Timestamp between (_startTime .. _endTime)\n    | summarize avg(Value) by bin(Timestamp, 1h)\n</code></pre> <p>Note: The use of <code>between(_startTime .. _endTime)</code> is required for correct time windowing. Do not use <code>&gt;= _startTime and &lt; _endTime</code> or other variants; the system already ensures no overlap or gap by adjusting <code>_endTime</code>.</p> <p>When the query is executed successfully, the <code>SummaryRule</code> CRD will be updated with the last execution time and start and end time used in the query.  These fields will be used to determine the next execution time and interval.</p>"},{"location":"designs/summary-rules/#conditional-execution-with-criteria","title":"Conditional Execution with Criteria","text":"<p>SummaryRules support criteria to enable conditional execution based on cluster labels. This allows the same rule to be deployed across multiple environments while only executing where appropriate.</p> <p>For example, if an ingestor is started with <code>--cluster-labels=region=eastus,environment=production</code>, then a SummaryRule with:</p> <pre><code>apiVersion: adx-mon.azure.com/v1\nkind: SummaryRule\nmetadata:\n  name: regional-summary\nspec:\n  database: SomeDatabase\n  name: HourlyAvg\n  body: |\n    SomeMetric\n    | where Timestamp between (_startTime .. _endTime)\n    | summarize avg(Value)\n  table: SomeMetricHourlyAvg\n  interval: 1h\n  criteria:\n    region:\n      - eastus\n      - westus\n    environment:\n      - production\n</code></pre> <p>Would execute because the cluster has <code>region=eastus</code> (which matches one of the allowed regions) OR <code>environment=production</code> (which matches the required environment). The matching logic uses case-insensitive comparison and OR semantics - any single criteria match allows execution.</p> <p>If no criteria are specified, the rule executes on all ingestor instances regardless of cluster labels.</p>"},{"location":"designs/summary-rules/#recent-changes","title":"Recent Changes","text":"<p>To simplify querying summarized data with recent data, a view can be used to union the summarized data with the most recent raw data.</p> <pre><code>apiVersion: adx-mon.azure.com/v1\nkind: Function\nmetadata:\n  name: samplefn\nspec:\n  body: |\n    .create-or-alter function with (view=true, folder='views') SomeMetricHourlyAvg () {\n      let _interval = 1h;\n      let _startTime = toscalar(table('SomeMetricHourlyAvg') | summarize max(Timestamp)); \n      SomeMetric\n      | where Timestamp &gt;= _startTime\n      | summarize avg(Value) by bin(Timestamp, _interval)\n      | union table('SomeMetricHourlyAvg')\n    }\n  database: SomeDatabase\n</code></pre> <p>Variations of this view pattern can always return the most recent hour of raw data and summarized data thereafter.  The query performance will remain consistent as data grows.</p> <p>In some cases, it may be useful to further layer the summarized data with additional summarized data to support daily or weekly summaraizations.  This data can be incorporated using another <code>SummaryRule</code> and amending the view.</p>"},{"location":"designs/summary-rules/#data-importing","title":"Data Importing","text":"<p>To import data from another cluster, a <code>SummaryRule</code> can be defined to import data using the <code>cluster()</code> function.</p> <pre><code>apiVersion: adx-mon.azure.com/v1\nkind: SummaryRule\nmetadata:\n  name: importfn\nspec:\n    database: SomeDatabase\n    name: ImportData\n    body: |\n        cluster('https://remotecluster.kusto.windows.net').SomeDatabase.SomeTable\n    table: SomeTable\n    interval: 1d\n</code></pre> <p>This is useful when the remote cluster has different retention policies, data is queried frequently or there is data collected in other system that is useful for reference.  For example, it might be useful to import data from a remote cluster that has a global view of all telemetry data but you only need a subset of that data.</p> <p>See also: CRD Reference for a summary of all CRDs and links to advanced usage.</p>"}]}